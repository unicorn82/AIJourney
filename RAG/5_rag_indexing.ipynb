{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Routing](../resources/RAG/5_rag_indexing_1.png)\n",
    "Many RAG methods focus on dividing documents into multiple chunks and returning a certain number of chunks to the LLM during retrieval. However, block size and number are parameters prone to error. Many users find these parameters challenging to set, as inadequate context can significantly impact the results if they do not contain enough information to answer questions effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-representation\n",
    "\n",
    "![Routing](../resources/RAG/5_rag_indexing_2.png)\n",
    "By combining multiple representations such as document summaries, blocks, or complete content, optimal effectiveness is achieved in both retrieval and generation processes. \n",
    "It compensates for potential deficiencies in a single representation by using different levels and granularities of representation, ranging from concise summaries to detailed documents. \n",
    "\n",
    "## Proposition Indexing is one implementation method of Multi-representation.\n",
    "### We use LLM to summarize the original text and generate a summary.\n",
    "### The summary undergoes embedding processing and is saved into a Vector Store. Simultaneously, the original text is stored in a Doc Store.\n",
    "### During retrieval, based on the embedded representation of the user's question, we search the Vector Store for the corresponding summary and then return the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-qdrant in /Users/easonyin/miniconda3/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: qdrant-client in /Users/easonyin/miniconda3/lib/python3.12/site-packages (1.13.3)\n",
      "Requirement already satisfied: langchain-community in /Users/easonyin/miniconda3/lib/python3.12/site-packages (0.3.14)\n",
      "Requirement already satisfied: langchain in /Users/easonyin/miniconda3/lib/python3.12/site-packages (0.3.14)\n",
      "Requirement already satisfied: openai in /Users/easonyin/miniconda3/lib/python3.12/site-packages (1.59.3)\n",
      "Requirement already satisfied: pypdf in /Users/easonyin/miniconda3/lib/python3.12/site-packages (5.1.0)\n",
      "Requirement already satisfied: langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain-qdrant) (0.3.40)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain-qdrant) (2.10.4)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from qdrant-client) (1.71.0)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from qdrant-client) (1.71.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.27.2)\n",
      "Requirement already satisfied: numpy>=1.26 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from qdrant-client) (1.26.4)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from qdrant-client) (2.2.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain-community) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain-community) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain-community) (0.2.10)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain-community) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain) (0.3.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (5.29.2)\n",
      "Requirement already satisfied: setuptools in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (75.1.0)\n",
      "Requirement already satisfied: certifi in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.13)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-qdrant) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-qdrant) (2.27.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.0.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43->langchain-qdrant) (2.1)\n",
      "Requirement already satisfied: mypy_extensions>=0.3.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain-qdrant qdrant-client langchain-community langchain openai pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qj/xprrbhtn049_4mqqdmkglrqh0000gp/T/ipykernel_27684/48864621.py:6: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from dbUtils import DBUtils\n",
    "\n",
    "dbUtils = DBUtils()\n",
    "# Initialize Ollama LLM with qwen model\n",
    "llm = Ollama(\n",
    "    model=\"qwen2.5:latest\",\n",
    "    temperature=0.7,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.1,\n",
    "    num_ctx=4096\n",
    ")\n",
    "\n",
    "embedding_model = dbUtils.get_embedding_function(\"ollama\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"../data/BeigeBook_20250115.pdf\")\n",
    "    \n",
    "    # Load all pages\n",
    "ages = loader.load()\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(ages, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"summaries\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vectorstore = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"summaries\",\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
    "\n",
    "\n",
    "# Docs linked to summaries\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, summary_docs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'doc_id': '08e70822-3020-4baf-a2b0-a114a80ccfc0', '_id': 'cc6228a36a104817b2e86b4f82ae83a1', '_collection_name': 'summaries'}, page_content=\"The document provides an overview of economic conditions across various sectors in the First District, as reported by contacts, with a focus on 2025 expectations.\\n\\n### Key Points:\\n\\n1. **Consumer Goods:**\\n   - Expected revenues to increase strongly in Q1 2025 due to organic growth and recent acquisitions.\\n   - Long-term optimism about continued demand despite political and macro-economic uncertainties.\\n   - Concerns over increased competition from China, which could boost or harm business depending on client responses.\\n\\n2. **Commercial Real Estate:**\\n   - Activity was mostly flat in the First District with elevated long-term interest rates limiting transactions.\\n   - Borrowers favor extensions for maturing loans, hoping that long-term rates will decline.\\n   - Office leasing activity remained slow, but prime Boston properties showed healthy activity.\\n   - Industrial and retail leasing were described as stable.\\n   - Half of contacts are cautiously optimistic about more robust commercial real estate activity in Q1 2025.\\n\\n3. **Residential Real Estate:**\\n   - Home sales rose modestly year-over-year in November 2024.\\n   - Home inventories increased, though days on the market were longer due to buyers' wait for post-election decisions.\\n   - Single-family home prices increased briskly, while condo prices increased moderately or decreased slightly in northern New England states.\\n   - Optimism exists that more sellers will list their homes early 2025, potentially leading to higher sales activity.\\n\\nFor detailed regional economic conditions, visit: [https://www.bostonfed.org/in-the-region.aspx](https://www.bostonfed.org/in-the-region.aspx)\")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is 2024 economic outlook?\"\n",
    "sub_docs = vectorstore.similarity_search(query,k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qj/xprrbhtn049_4mqqdmkglrqh0000gp/T/ipykernel_27684/3791815623.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The document provides an overview of economic conditions across various sectors in the First District, as reported by contacts, with a focus on 2025 expectations.\\n\\n### Key Points:\\n\\n1. **Consumer Goods:**\\n   - Expected revenues to increase strongly in Q1 2025 due to organic growth and recent acquisitions.\\n   - Long-term optimism about continued demand despite political and macro-economic uncertainties.\\n   - Concerns over increased competition from China, which could boost or harm business dependi'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n",
    "retrieved_docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPTOR\n",
    "RAPTOR (Recursive Abstractive Processing for Tree-Organized Retrieval) is a indexing technique that organizes documents into a hierarchical structure, enabling effective recursive abstraction and retrieval of information. \n",
    "This method organizes document data in the form of a \"tree,\" where leaf nodes represent original documents or document blocks, and higher-level nodes represent abstracted summaries of these documents. \n",
    "The goal of RAPTOR is to enhance information retrieval efficiency through hierarchical organization, accommodating queries of different granularities.\n",
    "\n",
    "![Routing](../resources/RAG/5_rag_indexing_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf Node Initialization The input at the lowest level (leaf nodes) can be:\n",
    "\n",
    "### Text blocks (small paragraphs or sections within a document)\n",
    "### Entire documents\n",
    "### Even finer-grained data units, such as sentences or phrases.\n",
    "\n",
    "## Embedding and Clustering \n",
    "### Generate embedding vectors for the content of each leaf node.\n",
    "### Cluster the embeddings based on the similarity of document content, using methods like K-means or hierarchical clustering algorithms.\n",
    "\n",
    "## Cluster Summarization \n",
    "### Integrate and summarize information from documents or text blocks within each cluster to generate a higher-level summary. \n",
    "### This step can utilize LLM (Large Language Model) for summarization.\n",
    "\n",
    "## Recursive Tree \n",
    "### Construction Use the generated higher-level summaries as new inputs and repeat the steps of embedding, clustering, and summarization. \n",
    "### Continue this process recursively until a global summary at the top level is generated.\n",
    "\n",
    "# Plotting the histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, edgecolor=\"black\", alpha=0.7)  # Removed color parameter\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()  # Fixed the show() call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/easonyin/miniconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: dashscope in /Users/easonyin/miniconda3/lib/python3.12/site-packages (1.22.2)\n",
      "Requirement already satisfied: umap in /Users/easonyin/miniconda3/lib/python3.12/site-packages (0.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: aiohttp in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from dashscope) (3.11.11)\n",
      "Requirement already satisfied: websocket-client in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from dashscope) (1.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp->dashscope) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp->dashscope) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp->dashscope) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp->dashscope) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp->dashscope) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp->dashscope) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/easonyin/miniconda3/lib/python3.12/site-packages (from aiohttp->dashscope) (1.18.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install tiktoken dashscope umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dashscope import get_tokenizer  # dashscope version >= 1.14.0\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "\n",
    "# Get the tokenizer object, currently only supports the Qwen series models\n",
    "tokenizer = get_tokenizer('qwen-max')\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    # Split the string into tokens and convert to token ids\n",
    "    num_tokens = tokenizer.encode(string)\n",
    "    return num_tokens\n",
    "    len(num_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/concepts/lcel/', 'content_type': 'text/html; charset=utf-8', 'title': 'LangChain Expression Language (LCEL) | ü¶úÔ∏èüîó LangChain', 'description': '* Runnable Interface', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideLangChain Expression Language (LCEL)On this pageLangChain Expression Language (LCEL)\\nPrerequisites\\nRunnable Interface\\n\\nThe LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables.\\nThis means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains.\\nWe often refer to a Runnable created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is Runnable and it implements the full Runnable Interface.\\nnote\\nThe LCEL cheatsheet shows common patterns that involve the Runnable interface and LCEL expressions.\\nPlease see the following list of how-to guides that cover common tasks with LCEL.\\nA list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n\\nBenefits of LCEL\\u200b\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\nOptimized parallel execution: Run Runnables in parallel using RunnableParallel or run multiple inputs through a given chain in parallel using the Runnable Batch API. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\nGuaranteed Async support: Any chain built with LCEL can be run asynchronously using the Runnable Async API. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\nSimplify streaming: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out).\\n\\nOther benefits include:\\n\\nSeamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\\nStandard API: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\nDeployable with LangServe: Chains built with LCEL can be deployed using for production use.\\n\\nShould I use LCEL?\\u200b\\nLCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way.\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of LangGraph.\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\nHere are some guidelines:\\n\\nIf you are making a single LLM call, you don\\'t need LCEL; instead call the underlying chat model directly.\\nIf you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\nIf you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\nComposition Primitives\\u200b\\nLCEL chains are built by composing existing Runnables together. The two main composition primitives are RunnableSequence and RunnableParallel.\\nMany other composition primitives (e.g., RunnableAssign) can be thought of as variations of these two primitives.\\nnoteYou can find a list of all composition primitives in the LangChain Core API Reference.\\nRunnableSequence\\u200b\\nRunnableSequence is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\nfrom langchain_core.runnables import RunnableSequencechain = RunnableSequence([runnable1, runnable2])API Reference:RunnableSequence\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\ncorresponds to the following:\\noutput1 = runnable1.invoke(some_input)final_output = runnable2.invoke(output1)\\nnoterunnable1 and runnable2 are placeholders for any Runnable that you want to chain together.\\nRunnableParallel\\u200b\\nRunnableParallel is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\nfrom langchain_core.runnables import RunnableParallelchain = RunnableParallel({    \"key1\": runnable1,    \"key2\": runnable2,})API Reference:RunnableParallel\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\nWill yield a final_output dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n{    \"key1\": runnable1.invoke(some_input),    \"key2\": runnable2.invoke(some_input),}\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\nnoteRunnableParallelsupports both synchronous and asynchronous execution (as all Runnables do).\\nFor synchronous execution, RunnableParallel uses a ThreadPoolExecutor to run the runnables concurrently.\\nFor asynchronous execution, RunnableParallel uses asyncio.gather to run the runnables concurrently.\\n\\nComposition Syntax\\u200b\\nThe usage of RunnableSequence and RunnableParallel is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\nThe | operator\\u200b\\nWe have overloaded the | operator to create a RunnableSequence from two Runnables.\\nchain = runnable1 | runnable2\\nis Equivalent to:\\nchain = RunnableSequence([runnable1, runnable2])\\nThe .pipe method\\u200b\\nIf you have moral qualms with operator overloading, you can use the .pipe method instead. This is equivalent to the | operator.\\nchain = runnable1.pipe(runnable2)\\nCoercion\\u200b\\nLCEL applies automatic type coercion to make it easier to compose chains.\\nIf you do not understand the type coercion, you can always use the RunnableSequence and RunnableParallel classes directly.\\nThis will make the code more verbose, but it will also make it more explicit.\\nDictionary to RunnableParallel\\u200b\\nInside an LCEL expression, a dictionary is automatically converted to a RunnableParallel.\\nFor example, the following code:\\nmapping = {    \"key1\": runnable1,    \"key2\": runnable2,}chain = mapping | runnable3\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\ncautionYou have to be careful because the mapping dictionary is not a RunnableParallel object, it is just a dictionary. This means that the following code will raise an AttributeError:mapping.invoke(some_input)\\nFunction to RunnableLambda\\u200b\\nInside an LCEL expression, a function is automatically converted to a RunnableLambda.\\ndef some_func(x):    return xchain = some_func | runnable1\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\ncautionYou have to be careful because the lambda function is not a RunnableLambda object, it is just a function. This means that the following code will raise an AttributeError:lambda x: x + 1.invoke(some_input)\\nLegacy chains\\u200b\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\\nConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\nIf you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.\\nFor guides on how to do specific tasks with LCEL, check out the relevant how-to guides.Edit this pageWas this page helpful?PreviousKey-value storesNextMessagesBenefits of LCELShould I use LCEL?Composition PrimitivesRunnableSequenceRunnableParallelComposition SyntaxThe | operatorThe .pipe methodCoercionLegacy chainsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/concepts/lcel/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "docs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/how_to/output_parser_structured/', 'content_type': 'text/html; charset=utf-8', 'title': 'How to use output parsers to parse an LLM response into structured format | ü¶úÔ∏èüîó LangChain', 'description': 'Language models output text. But there are times where you want to get more structured information than just text back. While some model providers support built-in ways to return structured output, not all do.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nHow to use output parsers to parse an LLM response into structured format | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to use output parsers to parse an LLM response into structured formatOn this pageHow to use output parsers to parse an LLM response into structured format\\nLanguage models output text. But there are times where you want to get more structured information than just text back. While some model providers support built-in ways to return structured output, not all do.\\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\\n\\n\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\\n\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\\n\\nAnd then one optional one:\\n\\n\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\\n\\nGet started\\u200b\\nBelow we go over the main type of output parser, the PydanticOutputParser.\\nfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import OpenAIfrom pydantic import BaseModel, Field, model_validatormodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @model_validator(mode=\"before\")    @classmethod    def question_ends_with_question_mark(cls, values: dict) -> dict:        setup = values.get(\"setup\")        if setup and setup[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return values# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)API Reference:PydanticOutputParser | PromptTemplate | OpenAI\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nLCEL\\u200b\\nOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\\nOutput parsers accept a string or BaseMessage as input and can return an arbitrary type.\\nparser.invoke(output)\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nInstead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:\\nchain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nWhile all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.\\nThe SimpleJsonOutputParser for example can stream through partial outputs:\\nfrom langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parserAPI Reference:SimpleJsonOutputParser\\nlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))\\n[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]\\nSimilarly,for PydanticOutputParser:\\nlist(chain.stream({\"query\": \"Tell me a joke.\"}))\\n[Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')]Edit this pageWas this page helpful?PreviousHow to run custom functionsNextHow to handle cases where no queries are generatedGet startedLCELCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LCEL w/ PydanticOutputParser (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/how_to/output_parser_structured/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_pydantic = loader.load()\n",
    "\n",
    "docs_pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/how_to/self_query/', 'content_type': 'text/html; charset=utf-8', 'title': 'How to do \"self-querying\" retrieval | ü¶úÔ∏èüîó LangChain', 'description': 'Head to Integrations for documentation on vector stores with built-in support for self-querying.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nHow to do \"self-querying\" retrieval | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to do \"self-querying\" retrievalOn this pageHow to do \"self-querying\" retrieval\\ninfoHead to Integrations for documentation on vector stores with built-in support for self-querying.\\nA self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying vector store. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\\n\\nGet started\\u200b\\nFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.\\nNote: The self-query retriever requires you to have lark package installed.\\n%pip install --upgrade --quiet  lark langchain-chroma\\nfrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())API Reference:Document | OpenAIEmbeddings\\nCreating our self-querying retriever\\u200b\\nNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\\nfrom langchain.chains.query_constructor.schema import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)API Reference:AttributeInfo | SelfQueryRetriever | ChatOpenAI\\nTesting it out\\u200b\\nAnd now we can actually try using our retriever!\\n# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")\\n[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]\\n# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\\n[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]\\n# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")\\n[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]\\n# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")\\n[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]\\nFilter k\\u200b\\nWe can also use the self query retriever to specify k: the number of documents to fetch.\\nWe can do this by passing enable_limit=True to the constructor.\\nretriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")\\n[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]\\nConstructing from scratch with LCEL\\u200b\\nTo see what\\'s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.\\nFirst, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.\\nfrom langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserAPI Reference:StructuredQueryOutputParser | get_query_constructor_prompt\\nLet\\'s look at our prompt:\\nprint(prompt.format(query=\"dummy question\"))\\nYour goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:\\\\`\\\\`\\\\`json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}\\\\`\\\\`\\\\`The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: `comp(attr, val)`:- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator- `attr` (string):  name of attribute to apply the comparison to- `val` (string): is the comparison valueA logical operation statement takes the form `op(statement1, statement2, ...)`:- `op` (and | or | not): logical operator- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.Make sure that filters only refer to attributes that exist in the data source.Make sure that filters only use the attributed names with its function names if there are functions applied on them.Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.Make sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}\\\\`\\\\`\\\\`User Query:What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:\\\\`\\\\`\\\\`json{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}\\\\`\\\\`\\\\`<< Example 2. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}\\\\`\\\\`\\\\`User Query:What are songs that were not published on SpotifyStructured Request:\\\\`\\\\`\\\\`json{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}\\\\`\\\\`\\\\`<< Example 3. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}\\\\`\\\\`\\\\`User Query:dummy questionStructured Request:\\nAnd what our full chain produces:\\nquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })\\nStructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)\\nThe query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.\\nThe next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.\\nfrom langchain_community.query_constructors.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)API Reference:ChromaTranslator\\nretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")\\n[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Edit this pageWas this page helpful?PreviousHow to pass runtime secrets to runnablesNextHow to split text based on semantic similarityGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LCEL w/ Self Query (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/how_to/self_query/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_sq = loader.load()\n",
    "\n",
    "docs_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideLangChain Expression Language (LCEL)On this pageLangChain Expression Language (LCEL)\\nPrerequisites\\nRunnable Interface\\n\\nThe LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables.\\nThis means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains.\\nWe often refer to a Runnable created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is Runnable and it implements the full Runnable Interface.\\nnote\\nThe LCEL cheatsheet shows common patterns that involve the Runnable interface and LCEL expressions.\\nPlease see the following list of how-to guides that cover common tasks with LCEL.\\nA list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n\\nBenefits of LCEL\\u200b\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\nOptimized parallel execution: Run Runnables in parallel using RunnableParallel or run multiple inputs through a given chain in parallel using the Runnable Batch API. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\nGuaranteed Async support: Any chain built with LCEL can be run asynchronously using the Runnable Async API. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\nSimplify streaming: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out).\\n\\nOther benefits include:\\n\\nSeamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\\nStandard API: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\nDeployable with LangServe: Chains built with LCEL can be deployed using for production use.\\n\\nShould I use LCEL?\\u200b\\nLCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way.\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of LangGraph.\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\nHere are some guidelines:\\n\\nIf you are making a single LLM call, you don\\'t need LCEL; instead call the underlying chat model directly.\\nIf you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\nIf you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\nComposition Primitives\\u200b\\nLCEL chains are built by composing existing Runnables together. The two main composition primitives are RunnableSequence and RunnableParallel.\\nMany other composition primitives (e.g., RunnableAssign) can be thought of as variations of these two primitives.\\nnoteYou can find a list of all composition primitives in the LangChain Core API Reference.\\nRunnableSequence\\u200b\\nRunnableSequence is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\nfrom langchain_core.runnables import RunnableSequencechain = RunnableSequence([runnable1, runnable2])API Reference:RunnableSequence\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\ncorresponds to the following:\\noutput1 = runnable1.invoke(some_input)final_output = runnable2.invoke(output1)\\nnoterunnable1 and runnable2 are placeholders for any Runnable that you want to chain together.\\nRunnableParallel\\u200b\\nRunnableParallel is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\nfrom langchain_core.runnables import RunnableParallelchain = RunnableParallel({    \"key1\": runnable1,    \"key2\": runnable2,})API Reference:RunnableParallel\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\nWill yield a final_output dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n{    \"key1\": runnable1.invoke(some_input),    \"key2\": runnable2.invoke(some_input),}\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\nnoteRunnableParallelsupports both synchronous and asynchronous execution (as all Runnables do).\\nFor synchronous execution, RunnableParallel uses a ThreadPoolExecutor to run the runnables concurrently.\\nFor asynchronous execution, RunnableParallel uses asyncio.gather to run the runnables concurrently.\\n\\nComposition Syntax\\u200b\\nThe usage of RunnableSequence and RunnableParallel is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\nThe | operator\\u200b\\nWe have overloaded the | operator to create a RunnableSequence from two Runnables.\\nchain = runnable1 | runnable2\\nis Equivalent to:\\nchain = RunnableSequence([runnable1, runnable2])\\nThe .pipe method\\u200b\\nIf you have moral qualms with operator overloading, you can use the .pipe method instead. This is equivalent to the | operator.\\nchain = runnable1.pipe(runnable2)\\nCoercion\\u200b\\nLCEL applies automatic type coercion to make it easier to compose chains.\\nIf you do not understand the type coercion, you can always use the RunnableSequence and RunnableParallel classes directly.\\nThis will make the code more verbose, but it will also make it more explicit.\\nDictionary to RunnableParallel\\u200b\\nInside an LCEL expression, a dictionary is automatically converted to a RunnableParallel.\\nFor example, the following code:\\nmapping = {    \"key1\": runnable1,    \"key2\": runnable2,}chain = mapping | runnable3\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\ncautionYou have to be careful because the mapping dictionary is not a RunnableParallel object, it is just a dictionary. This means that the following code will raise an AttributeError:mapping.invoke(some_input)\\nFunction to RunnableLambda\\u200b\\nInside an LCEL expression, a function is automatically converted to a RunnableLambda.\\ndef some_func(x):    return xchain = some_func | runnable1\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\ncautionYou have to be careful because the lambda function is not a RunnableLambda object, it is just a function. This means that the following code will raise an AttributeError:lambda x: x + 1.invoke(some_input)\\nLegacy chains\\u200b\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\\nConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\nIf you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.\\nFor guides on how to do specific tasks with LCEL, check out the relevant how-to guides.Edit this pageWas this page helpful?PreviousKey-value storesNextMessagesBenefits of LCELShould I use LCEL?Composition PrimitivesRunnableSequenceRunnableParallelComposition SyntaxThe | operatorThe .pipe methodCoercionLegacy chainsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n', '\\n\\n\\n\\n\\nHow to use output parsers to parse an LLM response into structured format | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to use output parsers to parse an LLM response into structured formatOn this pageHow to use output parsers to parse an LLM response into structured format\\nLanguage models output text. But there are times where you want to get more structured information than just text back. While some model providers support built-in ways to return structured output, not all do.\\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\\n\\n\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\\n\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\\n\\nAnd then one optional one:\\n\\n\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\\n\\nGet started\\u200b\\nBelow we go over the main type of output parser, the PydanticOutputParser.\\nfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import OpenAIfrom pydantic import BaseModel, Field, model_validatormodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @model_validator(mode=\"before\")    @classmethod    def question_ends_with_question_mark(cls, values: dict) -> dict:        setup = values.get(\"setup\")        if setup and setup[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return values# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)API Reference:PydanticOutputParser | PromptTemplate | OpenAI\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nLCEL\\u200b\\nOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\\nOutput parsers accept a string or BaseMessage as input and can return an arbitrary type.\\nparser.invoke(output)\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nInstead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:\\nchain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nWhile all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.\\nThe SimpleJsonOutputParser for example can stream through partial outputs:\\nfrom langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parserAPI Reference:SimpleJsonOutputParser\\nlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))\\n[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]\\nSimilarly,for PydanticOutputParser:\\nlist(chain.stream({\"query\": \"Tell me a joke.\"}))\\n[Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')]Edit this pageWas this page helpful?PreviousHow to run custom functionsNextHow to handle cases where no queries are generatedGet startedLCELCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n', '\\n\\n\\n\\n\\nHow to do \"self-querying\" retrieval | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to do \"self-querying\" retrievalOn this pageHow to do \"self-querying\" retrieval\\ninfoHead to Integrations for documentation on vector stores with built-in support for self-querying.\\nA self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying vector store. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\\n\\nGet started\\u200b\\nFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.\\nNote: The self-query retriever requires you to have lark package installed.\\n%pip install --upgrade --quiet  lark langchain-chroma\\nfrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())API Reference:Document | OpenAIEmbeddings\\nCreating our self-querying retriever\\u200b\\nNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\\nfrom langchain.chains.query_constructor.schema import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)API Reference:AttributeInfo | SelfQueryRetriever | ChatOpenAI\\nTesting it out\\u200b\\nAnd now we can actually try using our retriever!\\n# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")\\n[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]\\n# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\\n[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]\\n# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")\\n[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]\\n# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")\\n[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]\\nFilter k\\u200b\\nWe can also use the self query retriever to specify k: the number of documents to fetch.\\nWe can do this by passing enable_limit=True to the constructor.\\nretriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")\\n[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]\\nConstructing from scratch with LCEL\\u200b\\nTo see what\\'s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.\\nFirst, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.\\nfrom langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserAPI Reference:StructuredQueryOutputParser | get_query_constructor_prompt\\nLet\\'s look at our prompt:\\nprint(prompt.format(query=\"dummy question\"))\\nYour goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:\\\\`\\\\`\\\\`json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}\\\\`\\\\`\\\\`The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: `comp(attr, val)`:- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator- `attr` (string):  name of attribute to apply the comparison to- `val` (string): is the comparison valueA logical operation statement takes the form `op(statement1, statement2, ...)`:- `op` (and | or | not): logical operator- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.Make sure that filters only refer to attributes that exist in the data source.Make sure that filters only use the attributed names with its function names if there are functions applied on them.Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.Make sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}\\\\`\\\\`\\\\`User Query:What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:\\\\`\\\\`\\\\`json{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}\\\\`\\\\`\\\\`<< Example 2. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}\\\\`\\\\`\\\\`User Query:What are songs that were not published on SpotifyStructured Request:\\\\`\\\\`\\\\`json{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}\\\\`\\\\`\\\\`<< Example 3. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}\\\\`\\\\`\\\\`User Query:dummy questionStructured Request:\\nAnd what our full chain produces:\\nquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })\\nStructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)\\nThe query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.\\nThe next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.\\nfrom langchain_community.query_constructors.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)API Reference:ChromaTranslator\\nretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")\\n[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Edit this pageWas this page helpful?PreviousHow to pass runtime secrets to runnablesNextHow to split text based on semantic similarityGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[14621,\n",
       "  26223,\n",
       "  18837,\n",
       "  16378,\n",
       "  11434,\n",
       "  320,\n",
       "  43,\n",
       "  41664,\n",
       "  8,\n",
       "  760,\n",
       "  11162,\n",
       "  99,\n",
       "  250,\n",
       "  30543,\n",
       "  146450,\n",
       "  22463,\n",
       "  18837,\n",
       "  34583,\n",
       "  35134,\n",
       "  311,\n",
       "  1887,\n",
       "  2213,\n",
       "  12292,\n",
       "  601,\n",
       "  518,\n",
       "  220,\n",
       "  22145,\n",
       "  25,\n",
       "  576,\n",
       "  20713,\n",
       "  15235,\n",
       "  14872,\n",
       "  553,\n",
       "  22463,\n",
       "  18837,\n",
       "  389,\n",
       "  3217,\n",
       "  220,\n",
       "  16,\n",
       "  18,\n",
       "  609,\n",
       "  220,\n",
       "  16,\n",
       "  19,\n",
       "  304,\n",
       "  5836,\n",
       "  12879,\n",
       "  0,\n",
       "  1072,\n",
       "  14412,\n",
       "  804,\n",
       "  7082,\n",
       "  17207,\n",
       "  7661,\n",
       "  52984,\n",
       "  10607,\n",
       "  15919,\n",
       "  1454,\n",
       "  5785,\n",
       "  26223,\n",
       "  41885,\n",
       "  26223,\n",
       "  11212,\n",
       "  26223,\n",
       "  18837,\n",
       "  26538,\n",
       "  26223,\n",
       "  18837,\n",
       "  12162,\n",
       "  14,\n",
       "  9951,\n",
       "  85,\n",
       "  15,\n",
       "  13,\n",
       "  18,\n",
       "  85,\n",
       "  15,\n",
       "  13,\n",
       "  18,\n",
       "  85,\n",
       "  15,\n",
       "  13,\n",
       "  17,\n",
       "  85,\n",
       "  15,\n",
       "  13,\n",
       "  16,\n",
       "  145653,\n",
       "  5890,\n",
       "  37155,\n",
       "  51,\n",
       "  54927,\n",
       "  11066,\n",
       "  264,\n",
       "  15846,\n",
       "  21806,\n",
       "  287,\n",
       "  3766,\n",
       "  916,\n",
       "  264,\n",
       "  12165,\n",
       "  9994,\n",
       "  51,\n",
       "  54927,\n",
       "  11066,\n",
       "  264,\n",
       "  4285,\n",
       "  444,\n",
       "  10994,\n",
       "  3766,\n",
       "  448,\n",
       "  6236,\n",
       "  4119,\n",
       "  323,\n",
       "  9934,\n",
       "  19911,\n",
       "  11066,\n",
       "  264,\n",
       "  12853,\n",
       "  6331,\n",
       "  11066,\n",
       "  264,\n",
       "  19470,\n",
       "  831,\n",
       "  4928,\n",
       "  26980,\n",
       "  23470,\n",
       "  320,\n",
       "  49,\n",
       "  1890,\n",
       "  8,\n",
       "  1845,\n",
       "  25,\n",
       "  3660,\n",
       "  220,\n",
       "  17,\n",
       "  11066,\n",
       "  458,\n",
       "  94506,\n",
       "  28525,\n",
       "  11066,\n",
       "  458,\n",
       "  20713,\n",
       "  5668,\n",
       "  3173,\n",
       "  11066,\n",
       "  264,\n",
       "  19470,\n",
       "  831,\n",
       "  4928,\n",
       "  26980,\n",
       "  23470,\n",
       "  320,\n",
       "  49,\n",
       "  1890,\n",
       "  8,\n",
       "  1845,\n",
       "  25,\n",
       "  3660,\n",
       "  220,\n",
       "  16,\n",
       "  11066,\n",
       "  264,\n",
       "  41733,\n",
       "  2711,\n",
       "  4712,\n",
       "  11066,\n",
       "  264,\n",
       "  15846,\n",
       "  14,\n",
       "  16141,\n",
       "  287,\n",
       "  1849,\n",
       "  916,\n",
       "  7870,\n",
       "  821,\n",
       "  9190,\n",
       "  5612,\n",
       "  551,\n",
       "  2918,\n",
       "  4340,\n",
       "  4686,\n",
       "  27193,\n",
       "  4340,\n",
       "  4686,\n",
       "  27193,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  7375,\n",
       "  304,\n",
       "  264,\n",
       "  8781,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  264,\n",
       "  4621,\n",
       "  4314,\n",
       "  438,\n",
       "  264,\n",
       "  10759,\n",
       "  423,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  4938,\n",
       "  311,\n",
       "  6236,\n",
       "  61905,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  3110,\n",
       "  56037,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  264,\n",
       "  41733,\n",
       "  6193,\n",
       "  916,\n",
       "  4771,\n",
       "  4625,\n",
       "  4340,\n",
       "  311,\n",
       "  19873,\n",
       "  1598,\n",
       "  77,\n",
       "  4788,\n",
       "  304,\n",
       "  15279,\n",
       "  4340,\n",
       "  311,\n",
       "  4269,\n",
       "  6236,\n",
       "  1614,\n",
       "  14507,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  1638,\n",
       "  28696,\n",
       "  2827,\n",
       "  311,\n",
       "  264,\n",
       "  22109,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  56370,\n",
       "  311,\n",
       "  6236,\n",
       "  61905,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  2421,\n",
       "  6552,\n",
       "  10295,\n",
       "  304,\n",
       "  6236,\n",
       "  4119,\n",
       "  4340,\n",
       "  311,\n",
       "  653,\n",
       "  5392,\n",
       "  70643,\n",
       "  8098,\n",
       "  4340,\n",
       "  311,\n",
       "  4582,\n",
       "  22463,\n",
       "  18837,\n",
       "  14185,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  10295,\n",
       "  311,\n",
       "  279,\n",
       "  9934,\n",
       "  369,\n",
       "  3239,\n",
       "  6358,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  2421,\n",
       "  6552,\n",
       "  10295,\n",
       "  4340,\n",
       "  311,\n",
       "  1598,\n",
       "  2526,\n",
       "  5746,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  2550,\n",
       "  87073,\n",
       "  311,\n",
       "  4715,\n",
       "  458,\n",
       "  444,\n",
       "  10994,\n",
       "  2033,\n",
       "  1119,\n",
       "  32930,\n",
       "  3561,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  5048,\n",
       "  1380,\n",
       "  902,\n",
       "  19556,\n",
       "  525,\n",
       "  7907,\n",
       "  4340,\n",
       "  311,\n",
       "  6021,\n",
       "  1948,\n",
       "  1186,\n",
       "  11582,\n",
       "  1735,\n",
       "  4340,\n",
       "  311,\n",
       "  470,\n",
       "  32930,\n",
       "  821,\n",
       "  504,\n",
       "  264,\n",
       "  1614,\n",
       "  4340,\n",
       "  311,\n",
       "  62079,\n",
       "  1467,\n",
       "  1526,\n",
       "  15279,\n",
       "  2022,\n",
       "  4340,\n",
       "  311,\n",
       "  62079,\n",
       "  1467,\n",
       "  1526,\n",
       "  86875,\n",
       "  72913,\n",
       "  4340,\n",
       "  311,\n",
       "  62079,\n",
       "  1467,\n",
       "  304,\n",
       "  264,\n",
       "  3175,\n",
       "  444,\n",
       "  10994,\n",
       "  1618,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  5392,\n",
       "  89417,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  993,\n",
       "  2832,\n",
       "  509,\n",
       "  5392,\n",
       "  8098,\n",
       "  22302,\n",
       "  311,\n",
       "  444,\n",
       "  10994,\n",
       "  82,\n",
       "  323,\n",
       "  12853,\n",
       "  26874,\n",
       "  11066,\n",
       "  458,\n",
       "  20713,\n",
       "  448,\n",
       "  20713,\n",
       "  25255,\n",
       "  320,\n",
       "  77415,\n",
       "  8,\n",
       "  4340,\n",
       "  311,\n",
       "  9245,\n",
       "  6540,\n",
       "  38999,\n",
       "  4340,\n",
       "  311,\n",
       "  25244,\n",
       "  3561,\n",
       "  9934,\n",
       "  19911,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  5248,\n",
       "  19556,\n",
       "  979,\n",
       "  3730,\n",
       "  3239,\n",
       "  6358,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  5798,\n",
       "  3419,\n",
       "  7375,\n",
       "  323,\n",
       "  5392,\n",
       "  89417,\n",
       "  4340,\n",
       "  311,\n",
       "  1494,\n",
       "  1526,\n",
       "  5977,\n",
       "  504,\n",
       "  825,\n",
       "  3019,\n",
       "  311,\n",
       "  279,\n",
       "  1790,\n",
       "  4340,\n",
       "  311,\n",
       "  30335,\n",
       "  50932,\n",
       "  3786,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  5248,\n",
       "  10759,\n",
       "  3004,\n",
       "  979,\n",
       "  3730,\n",
       "  3239,\n",
       "  6358,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  2750,\n",
       "  311,\n",
       "  264,\n",
       "  8781,\n",
       "  594,\n",
       "  1584,\n",
       "  4340,\n",
       "  311,\n",
       "  9245,\n",
       "  13406,\n",
       "  369,\n",
       "  3239,\n",
       "  6358,\n",
       "  4340,\n",
       "  311,\n",
       "  14411,\n",
       "  15592,\n",
       "  8781,\n",
       "  91025,\n",
       "  4340,\n",
       "  3484,\n",
       "  448,\n",
       "  1550,\n",
       "  55880,\n",
       "  487,\n",
       "  22049,\n",
       "  52603,\n",
       "  979,\n",
       "  3730,\n",
       "  3239,\n",
       "  6358,\n",
       "  10268,\n",
       "  11789,\n",
       "  27811,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  279,\n",
       "  17439,\n",
       "  2859,\n",
       "  12020,\n",
       "  461,\n",
       "  2054,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  12205,\n",
       "  311,\n",
       "  10759,\n",
       "  423,\n",
       "  3059,\n",
       "  34,\n",
       "  11829,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  26679,\n",
       "  304,\n",
       "  3312,\n",
       "  21737,\n",
       "  4340,\n",
       "  311,\n",
       "  15498,\n",
       "  26679,\n",
       "  311,\n",
       "  264,\n",
       "  78679,\n",
       "  4340,\n",
       "  311,\n",
       "  57414,\n",
       "  26679,\n",
       "  220,\n",
       "  4692,\n",
       "  4340,\n",
       "  311,\n",
       "  6845,\n",
       "  2526,\n",
       "  4822,\n",
       "  4357,\n",
       "  4340,\n",
       "  311,\n",
       "  1494,\n",
       "  26679,\n",
       "  304,\n",
       "  518,\n",
       "  15592,\n",
       "  4340,\n",
       "  311,\n",
       "  6718,\n",
       "  553,\n",
       "  3668,\n",
       "  4340,\n",
       "  311,\n",
       "  6500,\n",
       "  6236,\n",
       "  1614,\n",
       "  14507,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  4379,\n",
       "  13388,\n",
       "  4340,\n",
       "  311,\n",
       "  2930,\n",
       "  894,\n",
       "  1614,\n",
       "  304,\n",
       "  825,\n",
       "  1555,\n",
       "  4340,\n",
       "  311,\n",
       "  3754,\n",
       "  3950,\n",
       "  10431,\n",
       "  304,\n",
       "  12853,\n",
       "  16969,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  7375,\n",
       "  311,\n",
       "  6236,\n",
       "  61905,\n",
       "  4340,\n",
       "  311,\n",
       "  6718,\n",
       "  2038,\n",
       "  4340,\n",
       "  311,\n",
       "  653,\n",
       "  56370,\n",
       "  448,\n",
       "  65151,\n",
       "  25111,\n",
       "  4340,\n",
       "  311,\n",
       "  5508,\n",
       "  6452,\n",
       "  77,\n",
       "  4788,\n",
       "  311,\n",
       "  13852,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  2526,\n",
       "  4822,\n",
       "  24083,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  264,\n",
       "  2526,\n",
       "  6236,\n",
       "  1614,\n",
       "  536,\n",
       "  10268,\n",
       "  37068,\n",
       "  24602,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  264,\n",
       "  2526,\n",
       "  444,\n",
       "  10994,\n",
       "  536,\n",
       "  10268,\n",
       "  10392,\n",
       "  461,\n",
       "  2054,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  7375,\n",
       "  4340,\n",
       "  311,\n",
       "  7390,\n",
       "  697,\n",
       "  444,\n",
       "  10994,\n",
       "  10500,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  27445,\n",
       "  82,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  9293,\n",
       "  504,\n",
       "  264,\n",
       "  6220,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  9308,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  4718,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  73192,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  5100,\n",
       "  8246,\n",
       "  3542,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  11358,\n",
       "  82,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  3482,\n",
       "  6816,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  264,\n",
       "  8741,\n",
       "  320,\n",
       "  721,\n",
       "  12,\n",
       "  7596,\n",
       "  287,\n",
       "  8,\n",
       "  8781,\n",
       "  1178,\n",
       "  39088,\n",
       "  4119,\n",
       "  4340,\n",
       "  311,\n",
       "  15963,\n",
       "  3059,\n",
       "  504,\n",
       "  5248,\n",
       "  10759,\n",
       "  3004,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  504,\n",
       "  264,\n",
       "  22463,\n",
       "  41885,\n",
       "  10337,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  553,\n",
       "  3084,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  553,\n",
       "  53129,\n",
       "  31773,\n",
       "  40861,\n",
       "  320,\n",
       "  8035,\n",
       "  49,\n",
       "  8,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  553,\n",
       "  308,\n",
       "  12,\n",
       "  1520,\n",
       "  27248,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  553,\n",
       "  37623,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  5785,\n",
       "  10295,\n",
       "  979,\n",
       "  3730,\n",
       "  32189,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  1293,\n",
       "  1467,\n",
       "  979,\n",
       "  3730,\n",
       "  32189,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  49645,\n",
       "  7484,\n",
       "  320,\n",
       "  2152,\n",
       "  5392,\n",
       "  8098,\n",
       "  8,\n",
       "  311,\n",
       "  653,\n",
       "  32189,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  32772,\n",
       "  82,\n",
       "  311,\n",
       "  264,\n",
       "  78679,\n",
       "  4340,\n",
       "  311,\n",
       "  4051,\n",
       "  6605,\n",
       "  30816,\n",
       "  16223,\n",
       "  7542,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  279,\n",
       "  22463,\n",
       "  18837,\n",
       "  51980,\n",
       "  5333,\n",
       "  4340,\n",
       "  311,\n",
       "  24085,\n",
       "  1598,\n",
       "  77,\n",
       "  4788,\n",
       "  26223,\n",
       "  18837,\n",
       "  16378,\n",
       "  11434,\n",
       "  8436,\n",
       "  1862,\n",
       "  3674,\n",
       "  4340,\n",
       "  311,\n",
       "  6500,\n",
       "  444,\n",
       "  10994,\n",
       "  14507,\n",
       "  4340,\n",
       "  311,\n",
       "  3754,\n",
       "  3950,\n",
       "  10431,\n",
       "  369,\n",
       "  444,\n",
       "  10994,\n",
       "  82,\n",
       "  6727,\n",
       "  4119,\n",
       "  23490,\n",
       "  4340,\n",
       "  311,\n",
       "  633,\n",
       "  1487,\n",
       "  48216,\n",
       "  4340,\n",
       "  311,\n",
       "  83184,\n",
       "  30403,\n",
       "  3059,\n",
       "  311,\n",
       "  49360,\n",
       "  279,\n",
       "  330,\n",
       "  54337,\n",
       "  304,\n",
       "  279,\n",
       "  6149,\n",
       "  1,\n",
       "  2456,\n",
       "  4340,\n",
       "  311,\n",
       "  6718,\n",
       "  73192,\n",
       "  553,\n",
       "  21426,\n",
       "  4340,\n",
       "  311,\n",
       "  10880,\n",
       "  23921,\n",
       "  6605,\n",
       "  315,\n",
       "  279,\n",
       "  1852,\n",
       "  943,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  1943,\n",
       "  3840,\n",
       "  4340,\n",
       "  311,\n",
       "  44566,\n",
       "  504,\n",
       "  19588,\n",
       "  22463,\n",
       "  18837,\n",
       "  13009,\n",
       "  311,\n",
       "  22463,\n",
       "  11212,\n",
       "  4340,\n",
       "  311,\n",
       "  17179,\n",
       "  1667,\n",
       "  5248,\n",
       "  22879,\n",
       "  817,\n",
       "  2197,\n",
       "  4340,\n",
       "  311,\n",
       "  1494,\n",
       "  79049,\n",
       "  57597,\n",
       "  821,\n",
       "  5961,\n",
       "  311,\n",
       "  4119,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  79049,\n",
       "  57597,\n",
       "  50932,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  264,\n",
       "  2526,\n",
       "  9258,\n",
       "  21102,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  279,\n",
       "  2550,\n",
       "  70913,\n",
       "  287,\n",
       "  6729,\n",
       "  4340,\n",
       "  311,\n",
       "  4715,\n",
       "  4718,\n",
       "  2550,\n",
       "  4340,\n",
       "  311,\n",
       "  22683,\n",
       "  979,\n",
       "  264,\n",
       "  22314,\n",
       "  1465,\n",
       "  13666,\n",
       "  4340,\n",
       "  311,\n",
       "  4715,\n",
       "  1467,\n",
       "  504,\n",
       "  1943,\n",
       "  6171,\n",
       "  4340,\n",
       "  311,\n",
       "  4715,\n",
       "  11874,\n",
       "  2550,\n",
       "  4340,\n",
       "  311,\n",
       "  4715,\n",
       "  53127,\n",
       "  2550,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  279,\n",
       "  17022,\n",
       "  11789,\n",
       "  10392,\n",
       "  461,\n",
       "  2054,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  22463,\n",
       "  18837,\n",
       "  448,\n",
       "  2155,\n",
       "  5355,\n",
       "  67,\n",
       "  8159,\n",
       "  10795,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  6236,\n",
       "  3840,\n",
       "  4340,\n",
       "  311,\n",
       "  633,\n",
       "  264,\n",
       "  431,\n",
       "  1890,\n",
       "  3766,\n",
       "  311,\n",
       "  912,\n",
       "  51846,\n",
       "  4340,\n",
       "  311,\n",
       "  653,\n",
       "  817,\n",
       "  8694,\n",
       "  56370,\n",
       "  4340,\n",
       "  311,\n",
       "  633,\n",
       "  697,\n",
       "  431,\n",
       "  1890,\n",
       "  3766,\n",
       "  311,\n",
       "  470,\n",
       "  8173,\n",
       "  4340,\n",
       "  311,\n",
       "  4269,\n",
       "  3059,\n",
       "  504,\n",
       "  697,\n",
       "  431,\n",
       "  1890,\n",
       "  3766,\n",
       "  4340,\n",
       "  311,\n",
       "  6718,\n",
       "  ...],\n",
       " [14621,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  2550,\n",
       "  87073,\n",
       "  311,\n",
       "  4715,\n",
       "  458,\n",
       "  444,\n",
       "  10994,\n",
       "  2033,\n",
       "  1119,\n",
       "  32930,\n",
       "  3561,\n",
       "  760,\n",
       "  11162,\n",
       "  99,\n",
       "  250,\n",
       "  30543,\n",
       "  146450,\n",
       "  22463,\n",
       "  18837,\n",
       "  34583,\n",
       "  35134,\n",
       "  311,\n",
       "  1887,\n",
       "  2213,\n",
       "  12292,\n",
       "  601,\n",
       "  518,\n",
       "  220,\n",
       "  22145,\n",
       "  25,\n",
       "  576,\n",
       "  20713,\n",
       "  15235,\n",
       "  14872,\n",
       "  553,\n",
       "  22463,\n",
       "  18837,\n",
       "  389,\n",
       "  3217,\n",
       "  220,\n",
       "  16,\n",
       "  18,\n",
       "  609,\n",
       "  220,\n",
       "  16,\n",
       "  19,\n",
       "  304,\n",
       "  5836,\n",
       "  12879,\n",
       "  0,\n",
       "  1072,\n",
       "  14412,\n",
       "  804,\n",
       "  7082,\n",
       "  17207,\n",
       "  7661,\n",
       "  52984,\n",
       "  10607,\n",
       "  15919,\n",
       "  1454,\n",
       "  5785,\n",
       "  26223,\n",
       "  41885,\n",
       "  26223,\n",
       "  11212,\n",
       "  26223,\n",
       "  18837,\n",
       "  26538,\n",
       "  26223,\n",
       "  18837,\n",
       "  12162,\n",
       "  14,\n",
       "  9951,\n",
       "  85,\n",
       "  15,\n",
       "  13,\n",
       "  18,\n",
       "  85,\n",
       "  15,\n",
       "  13,\n",
       "  18,\n",
       "  85,\n",
       "  15,\n",
       "  13,\n",
       "  17,\n",
       "  85,\n",
       "  15,\n",
       "  13,\n",
       "  16,\n",
       "  145653,\n",
       "  5890,\n",
       "  37155,\n",
       "  51,\n",
       "  54927,\n",
       "  11066,\n",
       "  264,\n",
       "  15846,\n",
       "  21806,\n",
       "  287,\n",
       "  3766,\n",
       "  916,\n",
       "  264,\n",
       "  12165,\n",
       "  9994,\n",
       "  51,\n",
       "  54927,\n",
       "  11066,\n",
       "  264,\n",
       "  4285,\n",
       "  444,\n",
       "  10994,\n",
       "  3766,\n",
       "  448,\n",
       "  6236,\n",
       "  4119,\n",
       "  323,\n",
       "  9934,\n",
       "  19911,\n",
       "  11066,\n",
       "  264,\n",
       "  12853,\n",
       "  6331,\n",
       "  11066,\n",
       "  264,\n",
       "  19470,\n",
       "  831,\n",
       "  4928,\n",
       "  26980,\n",
       "  23470,\n",
       "  320,\n",
       "  49,\n",
       "  1890,\n",
       "  8,\n",
       "  1845,\n",
       "  25,\n",
       "  3660,\n",
       "  220,\n",
       "  17,\n",
       "  11066,\n",
       "  458,\n",
       "  94506,\n",
       "  28525,\n",
       "  11066,\n",
       "  458,\n",
       "  20713,\n",
       "  5668,\n",
       "  3173,\n",
       "  11066,\n",
       "  264,\n",
       "  19470,\n",
       "  831,\n",
       "  4928,\n",
       "  26980,\n",
       "  23470,\n",
       "  320,\n",
       "  49,\n",
       "  1890,\n",
       "  8,\n",
       "  1845,\n",
       "  25,\n",
       "  3660,\n",
       "  220,\n",
       "  16,\n",
       "  11066,\n",
       "  264,\n",
       "  41733,\n",
       "  2711,\n",
       "  4712,\n",
       "  11066,\n",
       "  264,\n",
       "  15846,\n",
       "  14,\n",
       "  16141,\n",
       "  287,\n",
       "  1849,\n",
       "  916,\n",
       "  7870,\n",
       "  821,\n",
       "  9190,\n",
       "  5612,\n",
       "  551,\n",
       "  2918,\n",
       "  4340,\n",
       "  4686,\n",
       "  27193,\n",
       "  4340,\n",
       "  4686,\n",
       "  27193,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  7375,\n",
       "  304,\n",
       "  264,\n",
       "  8781,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  264,\n",
       "  4621,\n",
       "  4314,\n",
       "  438,\n",
       "  264,\n",
       "  10759,\n",
       "  423,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  4938,\n",
       "  311,\n",
       "  6236,\n",
       "  61905,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  3110,\n",
       "  56037,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  264,\n",
       "  41733,\n",
       "  6193,\n",
       "  916,\n",
       "  4771,\n",
       "  4625,\n",
       "  4340,\n",
       "  311,\n",
       "  19873,\n",
       "  1598,\n",
       "  77,\n",
       "  4788,\n",
       "  304,\n",
       "  15279,\n",
       "  4340,\n",
       "  311,\n",
       "  4269,\n",
       "  6236,\n",
       "  1614,\n",
       "  14507,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  1638,\n",
       "  28696,\n",
       "  2827,\n",
       "  311,\n",
       "  264,\n",
       "  22109,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  56370,\n",
       "  311,\n",
       "  6236,\n",
       "  61905,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  2421,\n",
       "  6552,\n",
       "  10295,\n",
       "  304,\n",
       "  6236,\n",
       "  4119,\n",
       "  4340,\n",
       "  311,\n",
       "  653,\n",
       "  5392,\n",
       "  70643,\n",
       "  8098,\n",
       "  4340,\n",
       "  311,\n",
       "  4582,\n",
       "  22463,\n",
       "  18837,\n",
       "  14185,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  10295,\n",
       "  311,\n",
       "  279,\n",
       "  9934,\n",
       "  369,\n",
       "  3239,\n",
       "  6358,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  2421,\n",
       "  6552,\n",
       "  10295,\n",
       "  4340,\n",
       "  311,\n",
       "  1598,\n",
       "  2526,\n",
       "  5746,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  2550,\n",
       "  87073,\n",
       "  311,\n",
       "  4715,\n",
       "  458,\n",
       "  444,\n",
       "  10994,\n",
       "  2033,\n",
       "  1119,\n",
       "  32930,\n",
       "  3561,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  5048,\n",
       "  1380,\n",
       "  902,\n",
       "  19556,\n",
       "  525,\n",
       "  7907,\n",
       "  4340,\n",
       "  311,\n",
       "  6021,\n",
       "  1948,\n",
       "  1186,\n",
       "  11582,\n",
       "  1735,\n",
       "  4340,\n",
       "  311,\n",
       "  470,\n",
       "  32930,\n",
       "  821,\n",
       "  504,\n",
       "  264,\n",
       "  1614,\n",
       "  4340,\n",
       "  311,\n",
       "  62079,\n",
       "  1467,\n",
       "  1526,\n",
       "  15279,\n",
       "  2022,\n",
       "  4340,\n",
       "  311,\n",
       "  62079,\n",
       "  1467,\n",
       "  1526,\n",
       "  86875,\n",
       "  72913,\n",
       "  4340,\n",
       "  311,\n",
       "  62079,\n",
       "  1467,\n",
       "  304,\n",
       "  264,\n",
       "  3175,\n",
       "  444,\n",
       "  10994,\n",
       "  1618,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  5392,\n",
       "  89417,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  993,\n",
       "  2832,\n",
       "  509,\n",
       "  5392,\n",
       "  8098,\n",
       "  22302,\n",
       "  311,\n",
       "  444,\n",
       "  10994,\n",
       "  82,\n",
       "  323,\n",
       "  12853,\n",
       "  26874,\n",
       "  11066,\n",
       "  458,\n",
       "  20713,\n",
       "  448,\n",
       "  20713,\n",
       "  25255,\n",
       "  320,\n",
       "  77415,\n",
       "  8,\n",
       "  4340,\n",
       "  311,\n",
       "  9245,\n",
       "  6540,\n",
       "  38999,\n",
       "  4340,\n",
       "  311,\n",
       "  25244,\n",
       "  3561,\n",
       "  9934,\n",
       "  19911,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  5248,\n",
       "  19556,\n",
       "  979,\n",
       "  3730,\n",
       "  3239,\n",
       "  6358,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  5798,\n",
       "  3419,\n",
       "  7375,\n",
       "  323,\n",
       "  5392,\n",
       "  89417,\n",
       "  4340,\n",
       "  311,\n",
       "  1494,\n",
       "  1526,\n",
       "  5977,\n",
       "  504,\n",
       "  825,\n",
       "  3019,\n",
       "  311,\n",
       "  279,\n",
       "  1790,\n",
       "  4340,\n",
       "  311,\n",
       "  30335,\n",
       "  50932,\n",
       "  3786,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  5248,\n",
       "  10759,\n",
       "  3004,\n",
       "  979,\n",
       "  3730,\n",
       "  3239,\n",
       "  6358,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  2750,\n",
       "  311,\n",
       "  264,\n",
       "  8781,\n",
       "  594,\n",
       "  1584,\n",
       "  4340,\n",
       "  311,\n",
       "  9245,\n",
       "  13406,\n",
       "  369,\n",
       "  3239,\n",
       "  6358,\n",
       "  4340,\n",
       "  311,\n",
       "  14411,\n",
       "  15592,\n",
       "  8781,\n",
       "  91025,\n",
       "  4340,\n",
       "  3484,\n",
       "  448,\n",
       "  1550,\n",
       "  55880,\n",
       "  487,\n",
       "  22049,\n",
       "  52603,\n",
       "  979,\n",
       "  3730,\n",
       "  3239,\n",
       "  6358,\n",
       "  10268,\n",
       "  11789,\n",
       "  27811,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  279,\n",
       "  17439,\n",
       "  2859,\n",
       "  12020,\n",
       "  461,\n",
       "  2054,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  12205,\n",
       "  311,\n",
       "  10759,\n",
       "  423,\n",
       "  3059,\n",
       "  34,\n",
       "  11829,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  26679,\n",
       "  304,\n",
       "  3312,\n",
       "  21737,\n",
       "  4340,\n",
       "  311,\n",
       "  15498,\n",
       "  26679,\n",
       "  311,\n",
       "  264,\n",
       "  78679,\n",
       "  4340,\n",
       "  311,\n",
       "  57414,\n",
       "  26679,\n",
       "  220,\n",
       "  4692,\n",
       "  4340,\n",
       "  311,\n",
       "  6845,\n",
       "  2526,\n",
       "  4822,\n",
       "  4357,\n",
       "  4340,\n",
       "  311,\n",
       "  1494,\n",
       "  26679,\n",
       "  304,\n",
       "  518,\n",
       "  15592,\n",
       "  4340,\n",
       "  311,\n",
       "  6718,\n",
       "  553,\n",
       "  3668,\n",
       "  4340,\n",
       "  311,\n",
       "  6500,\n",
       "  6236,\n",
       "  1614,\n",
       "  14507,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  4379,\n",
       "  13388,\n",
       "  4340,\n",
       "  311,\n",
       "  2930,\n",
       "  894,\n",
       "  1614,\n",
       "  304,\n",
       "  825,\n",
       "  1555,\n",
       "  4340,\n",
       "  311,\n",
       "  3754,\n",
       "  3950,\n",
       "  10431,\n",
       "  304,\n",
       "  12853,\n",
       "  16969,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  7375,\n",
       "  311,\n",
       "  6236,\n",
       "  61905,\n",
       "  4340,\n",
       "  311,\n",
       "  6718,\n",
       "  2038,\n",
       "  4340,\n",
       "  311,\n",
       "  653,\n",
       "  56370,\n",
       "  448,\n",
       "  65151,\n",
       "  25111,\n",
       "  4340,\n",
       "  311,\n",
       "  5508,\n",
       "  6452,\n",
       "  77,\n",
       "  4788,\n",
       "  311,\n",
       "  13852,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  2526,\n",
       "  4822,\n",
       "  24083,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  264,\n",
       "  2526,\n",
       "  6236,\n",
       "  1614,\n",
       "  536,\n",
       "  10268,\n",
       "  37068,\n",
       "  24602,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  264,\n",
       "  2526,\n",
       "  444,\n",
       "  10994,\n",
       "  536,\n",
       "  10268,\n",
       "  10392,\n",
       "  461,\n",
       "  2054,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  7375,\n",
       "  4340,\n",
       "  311,\n",
       "  7390,\n",
       "  697,\n",
       "  444,\n",
       "  10994,\n",
       "  10500,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  27445,\n",
       "  82,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  9293,\n",
       "  504,\n",
       "  264,\n",
       "  6220,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  9308,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  4718,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  73192,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  5100,\n",
       "  8246,\n",
       "  3542,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  11358,\n",
       "  82,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  3482,\n",
       "  6816,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  264,\n",
       "  8741,\n",
       "  320,\n",
       "  721,\n",
       "  12,\n",
       "  7596,\n",
       "  287,\n",
       "  8,\n",
       "  8781,\n",
       "  1178,\n",
       "  39088,\n",
       "  4119,\n",
       "  4340,\n",
       "  311,\n",
       "  15963,\n",
       "  3059,\n",
       "  504,\n",
       "  5248,\n",
       "  10759,\n",
       "  3004,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  504,\n",
       "  264,\n",
       "  22463,\n",
       "  41885,\n",
       "  10337,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  553,\n",
       "  3084,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  553,\n",
       "  53129,\n",
       "  31773,\n",
       "  40861,\n",
       "  320,\n",
       "  8035,\n",
       "  49,\n",
       "  8,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  553,\n",
       "  308,\n",
       "  12,\n",
       "  1520,\n",
       "  27248,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  553,\n",
       "  37623,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  5785,\n",
       "  10295,\n",
       "  979,\n",
       "  3730,\n",
       "  32189,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  1293,\n",
       "  1467,\n",
       "  979,\n",
       "  3730,\n",
       "  32189,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  49645,\n",
       "  7484,\n",
       "  320,\n",
       "  2152,\n",
       "  5392,\n",
       "  8098,\n",
       "  8,\n",
       "  311,\n",
       "  653,\n",
       "  32189,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  32772,\n",
       "  82,\n",
       "  311,\n",
       "  264,\n",
       "  78679,\n",
       "  4340,\n",
       "  311,\n",
       "  4051,\n",
       "  6605,\n",
       "  30816,\n",
       "  16223,\n",
       "  7542,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  279,\n",
       "  22463,\n",
       "  18837,\n",
       "  51980,\n",
       "  5333,\n",
       "  4340,\n",
       "  311,\n",
       "  24085,\n",
       "  1598,\n",
       "  77,\n",
       "  4788,\n",
       "  26223,\n",
       "  18837,\n",
       "  16378,\n",
       "  11434,\n",
       "  8436,\n",
       "  1862,\n",
       "  3674,\n",
       "  4340,\n",
       "  311,\n",
       "  6500,\n",
       "  444,\n",
       "  10994,\n",
       "  14507,\n",
       "  4340,\n",
       "  311,\n",
       "  3754,\n",
       "  3950,\n",
       "  10431,\n",
       "  369,\n",
       "  444,\n",
       "  10994,\n",
       "  82,\n",
       "  6727,\n",
       "  4119,\n",
       "  23490,\n",
       "  4340,\n",
       "  311,\n",
       "  633,\n",
       "  1487,\n",
       "  48216,\n",
       "  4340,\n",
       "  311,\n",
       "  83184,\n",
       "  30403,\n",
       "  3059,\n",
       "  311,\n",
       "  49360,\n",
       "  279,\n",
       "  330,\n",
       "  54337,\n",
       "  304,\n",
       "  279,\n",
       "  6149,\n",
       "  1,\n",
       "  2456,\n",
       "  4340,\n",
       "  311,\n",
       "  6718,\n",
       "  73192,\n",
       "  553,\n",
       "  21426,\n",
       "  4340,\n",
       "  311,\n",
       "  10880,\n",
       "  23921,\n",
       "  6605,\n",
       "  315,\n",
       "  279,\n",
       "  1852,\n",
       "  943,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  1943,\n",
       "  3840,\n",
       "  4340,\n",
       "  311,\n",
       "  44566,\n",
       "  504,\n",
       "  19588,\n",
       "  22463,\n",
       "  18837,\n",
       "  13009,\n",
       "  311,\n",
       "  22463,\n",
       "  11212,\n",
       "  4340,\n",
       "  311,\n",
       "  17179,\n",
       "  1667,\n",
       "  5248,\n",
       "  22879,\n",
       "  817,\n",
       "  2197,\n",
       "  4340,\n",
       "  311,\n",
       "  1494,\n",
       "  79049,\n",
       "  57597,\n",
       "  821,\n",
       "  5961,\n",
       "  311,\n",
       "  4119,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  79049,\n",
       "  57597,\n",
       "  50932,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  264,\n",
       "  2526,\n",
       "  9258,\n",
       "  21102,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  279,\n",
       "  2550,\n",
       "  70913,\n",
       "  287,\n",
       "  6729,\n",
       "  4340,\n",
       "  311,\n",
       "  4715,\n",
       "  4718,\n",
       "  2550,\n",
       "  4340,\n",
       "  311,\n",
       "  22683,\n",
       "  979,\n",
       "  264,\n",
       "  22314,\n",
       "  1465,\n",
       "  13666,\n",
       "  4340,\n",
       "  311,\n",
       "  4715,\n",
       "  1467,\n",
       "  504,\n",
       "  1943,\n",
       "  6171,\n",
       "  4340,\n",
       "  311,\n",
       "  4715,\n",
       "  11874,\n",
       "  2550,\n",
       "  4340,\n",
       "  311,\n",
       "  4715,\n",
       "  53127,\n",
       "  2550,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  279,\n",
       "  17022,\n",
       "  11789,\n",
       "  10392,\n",
       "  461,\n",
       "  2054,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  22463,\n",
       "  18837,\n",
       "  448,\n",
       "  2155,\n",
       "  5355,\n",
       "  67,\n",
       "  8159,\n",
       "  10795,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  6236,\n",
       "  3840,\n",
       "  4340,\n",
       "  311,\n",
       "  633,\n",
       "  264,\n",
       "  431,\n",
       "  1890,\n",
       "  3766,\n",
       "  311,\n",
       "  912,\n",
       "  51846,\n",
       "  4340,\n",
       "  311,\n",
       "  653,\n",
       "  817,\n",
       "  8694,\n",
       "  56370,\n",
       "  4340,\n",
       "  311,\n",
       "  633,\n",
       "  697,\n",
       "  431,\n",
       "  1890,\n",
       "  3766,\n",
       "  311,\n",
       "  470,\n",
       "  8173,\n",
       "  4340,\n",
       "  311,\n",
       "  4269,\n",
       "  3059,\n",
       "  504,\n",
       "  697,\n",
       "  ...],\n",
       " [14621,\n",
       "  4340,\n",
       "  311,\n",
       "  653,\n",
       "  330,\n",
       "  721,\n",
       "  65489,\n",
       "  287,\n",
       "  1,\n",
       "  56370,\n",
       "  760,\n",
       "  11162,\n",
       "  99,\n",
       "  250,\n",
       "  30543,\n",
       "  146450,\n",
       "  22463,\n",
       "  18837,\n",
       "  34583,\n",
       "  35134,\n",
       "  311,\n",
       "  1887,\n",
       "  2213,\n",
       "  12292,\n",
       "  601,\n",
       "  518,\n",
       "  220,\n",
       "  22145,\n",
       "  25,\n",
       "  576,\n",
       "  20713,\n",
       "  15235,\n",
       "  14872,\n",
       "  553,\n",
       "  22463,\n",
       "  18837,\n",
       "  389,\n",
       "  3217,\n",
       "  220,\n",
       "  16,\n",
       "  18,\n",
       "  609,\n",
       "  220,\n",
       "  16,\n",
       "  19,\n",
       "  304,\n",
       "  5836,\n",
       "  12879,\n",
       "  0,\n",
       "  1072,\n",
       "  14412,\n",
       "  804,\n",
       "  7082,\n",
       "  17207,\n",
       "  7661,\n",
       "  52984,\n",
       "  10607,\n",
       "  15919,\n",
       "  1454,\n",
       "  5785,\n",
       "  26223,\n",
       "  41885,\n",
       "  26223,\n",
       "  11212,\n",
       "  26223,\n",
       "  18837,\n",
       "  26538,\n",
       "  26223,\n",
       "  18837,\n",
       "  12162,\n",
       "  14,\n",
       "  9951,\n",
       "  85,\n",
       "  15,\n",
       "  13,\n",
       "  18,\n",
       "  85,\n",
       "  15,\n",
       "  13,\n",
       "  18,\n",
       "  85,\n",
       "  15,\n",
       "  13,\n",
       "  17,\n",
       "  85,\n",
       "  15,\n",
       "  13,\n",
       "  16,\n",
       "  145653,\n",
       "  5890,\n",
       "  37155,\n",
       "  51,\n",
       "  54927,\n",
       "  11066,\n",
       "  264,\n",
       "  15846,\n",
       "  21806,\n",
       "  287,\n",
       "  3766,\n",
       "  916,\n",
       "  264,\n",
       "  12165,\n",
       "  9994,\n",
       "  51,\n",
       "  54927,\n",
       "  11066,\n",
       "  264,\n",
       "  4285,\n",
       "  444,\n",
       "  10994,\n",
       "  3766,\n",
       "  448,\n",
       "  6236,\n",
       "  4119,\n",
       "  323,\n",
       "  9934,\n",
       "  19911,\n",
       "  11066,\n",
       "  264,\n",
       "  12853,\n",
       "  6331,\n",
       "  11066,\n",
       "  264,\n",
       "  19470,\n",
       "  831,\n",
       "  4928,\n",
       "  26980,\n",
       "  23470,\n",
       "  320,\n",
       "  49,\n",
       "  1890,\n",
       "  8,\n",
       "  1845,\n",
       "  25,\n",
       "  3660,\n",
       "  220,\n",
       "  17,\n",
       "  11066,\n",
       "  458,\n",
       "  94506,\n",
       "  28525,\n",
       "  11066,\n",
       "  458,\n",
       "  20713,\n",
       "  5668,\n",
       "  3173,\n",
       "  11066,\n",
       "  264,\n",
       "  19470,\n",
       "  831,\n",
       "  4928,\n",
       "  26980,\n",
       "  23470,\n",
       "  320,\n",
       "  49,\n",
       "  1890,\n",
       "  8,\n",
       "  1845,\n",
       "  25,\n",
       "  3660,\n",
       "  220,\n",
       "  16,\n",
       "  11066,\n",
       "  264,\n",
       "  41733,\n",
       "  2711,\n",
       "  4712,\n",
       "  11066,\n",
       "  264,\n",
       "  15846,\n",
       "  14,\n",
       "  16141,\n",
       "  287,\n",
       "  1849,\n",
       "  916,\n",
       "  7870,\n",
       "  821,\n",
       "  9190,\n",
       "  5612,\n",
       "  551,\n",
       "  2918,\n",
       "  4340,\n",
       "  4686,\n",
       "  27193,\n",
       "  4340,\n",
       "  4686,\n",
       "  27193,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  7375,\n",
       "  304,\n",
       "  264,\n",
       "  8781,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  264,\n",
       "  4621,\n",
       "  4314,\n",
       "  438,\n",
       "  264,\n",
       "  10759,\n",
       "  423,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  4938,\n",
       "  311,\n",
       "  6236,\n",
       "  61905,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  3110,\n",
       "  56037,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  264,\n",
       "  41733,\n",
       "  6193,\n",
       "  916,\n",
       "  4771,\n",
       "  4625,\n",
       "  4340,\n",
       "  311,\n",
       "  19873,\n",
       "  1598,\n",
       "  77,\n",
       "  4788,\n",
       "  304,\n",
       "  15279,\n",
       "  4340,\n",
       "  311,\n",
       "  4269,\n",
       "  6236,\n",
       "  1614,\n",
       "  14507,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  1638,\n",
       "  28696,\n",
       "  2827,\n",
       "  311,\n",
       "  264,\n",
       "  22109,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  56370,\n",
       "  311,\n",
       "  6236,\n",
       "  61905,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  2421,\n",
       "  6552,\n",
       "  10295,\n",
       "  304,\n",
       "  6236,\n",
       "  4119,\n",
       "  4340,\n",
       "  311,\n",
       "  653,\n",
       "  5392,\n",
       "  70643,\n",
       "  8098,\n",
       "  4340,\n",
       "  311,\n",
       "  4582,\n",
       "  22463,\n",
       "  18837,\n",
       "  14185,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  10295,\n",
       "  311,\n",
       "  279,\n",
       "  9934,\n",
       "  369,\n",
       "  3239,\n",
       "  6358,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  2421,\n",
       "  6552,\n",
       "  10295,\n",
       "  4340,\n",
       "  311,\n",
       "  1598,\n",
       "  2526,\n",
       "  5746,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  2550,\n",
       "  87073,\n",
       "  311,\n",
       "  4715,\n",
       "  458,\n",
       "  444,\n",
       "  10994,\n",
       "  2033,\n",
       "  1119,\n",
       "  32930,\n",
       "  3561,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  5048,\n",
       "  1380,\n",
       "  902,\n",
       "  19556,\n",
       "  525,\n",
       "  7907,\n",
       "  4340,\n",
       "  311,\n",
       "  6021,\n",
       "  1948,\n",
       "  1186,\n",
       "  11582,\n",
       "  1735,\n",
       "  4340,\n",
       "  311,\n",
       "  470,\n",
       "  32930,\n",
       "  821,\n",
       "  504,\n",
       "  264,\n",
       "  1614,\n",
       "  4340,\n",
       "  311,\n",
       "  62079,\n",
       "  1467,\n",
       "  1526,\n",
       "  15279,\n",
       "  2022,\n",
       "  4340,\n",
       "  311,\n",
       "  62079,\n",
       "  1467,\n",
       "  1526,\n",
       "  86875,\n",
       "  72913,\n",
       "  4340,\n",
       "  311,\n",
       "  62079,\n",
       "  1467,\n",
       "  304,\n",
       "  264,\n",
       "  3175,\n",
       "  444,\n",
       "  10994,\n",
       "  1618,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  5392,\n",
       "  89417,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  993,\n",
       "  2832,\n",
       "  509,\n",
       "  5392,\n",
       "  8098,\n",
       "  22302,\n",
       "  311,\n",
       "  444,\n",
       "  10994,\n",
       "  82,\n",
       "  323,\n",
       "  12853,\n",
       "  26874,\n",
       "  11066,\n",
       "  458,\n",
       "  20713,\n",
       "  448,\n",
       "  20713,\n",
       "  25255,\n",
       "  320,\n",
       "  77415,\n",
       "  8,\n",
       "  4340,\n",
       "  311,\n",
       "  9245,\n",
       "  6540,\n",
       "  38999,\n",
       "  4340,\n",
       "  311,\n",
       "  25244,\n",
       "  3561,\n",
       "  9934,\n",
       "  19911,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  5248,\n",
       "  19556,\n",
       "  979,\n",
       "  3730,\n",
       "  3239,\n",
       "  6358,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  5798,\n",
       "  3419,\n",
       "  7375,\n",
       "  323,\n",
       "  5392,\n",
       "  89417,\n",
       "  4340,\n",
       "  311,\n",
       "  1494,\n",
       "  1526,\n",
       "  5977,\n",
       "  504,\n",
       "  825,\n",
       "  3019,\n",
       "  311,\n",
       "  279,\n",
       "  1790,\n",
       "  4340,\n",
       "  311,\n",
       "  30335,\n",
       "  50932,\n",
       "  3786,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  5248,\n",
       "  10759,\n",
       "  3004,\n",
       "  979,\n",
       "  3730,\n",
       "  3239,\n",
       "  6358,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  2750,\n",
       "  311,\n",
       "  264,\n",
       "  8781,\n",
       "  594,\n",
       "  1584,\n",
       "  4340,\n",
       "  311,\n",
       "  9245,\n",
       "  13406,\n",
       "  369,\n",
       "  3239,\n",
       "  6358,\n",
       "  4340,\n",
       "  311,\n",
       "  14411,\n",
       "  15592,\n",
       "  8781,\n",
       "  91025,\n",
       "  4340,\n",
       "  3484,\n",
       "  448,\n",
       "  1550,\n",
       "  55880,\n",
       "  487,\n",
       "  22049,\n",
       "  52603,\n",
       "  979,\n",
       "  3730,\n",
       "  3239,\n",
       "  6358,\n",
       "  10268,\n",
       "  11789,\n",
       "  27811,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  279,\n",
       "  17439,\n",
       "  2859,\n",
       "  12020,\n",
       "  461,\n",
       "  2054,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  12205,\n",
       "  311,\n",
       "  10759,\n",
       "  423,\n",
       "  3059,\n",
       "  34,\n",
       "  11829,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  26679,\n",
       "  304,\n",
       "  3312,\n",
       "  21737,\n",
       "  4340,\n",
       "  311,\n",
       "  15498,\n",
       "  26679,\n",
       "  311,\n",
       "  264,\n",
       "  78679,\n",
       "  4340,\n",
       "  311,\n",
       "  57414,\n",
       "  26679,\n",
       "  220,\n",
       "  4692,\n",
       "  4340,\n",
       "  311,\n",
       "  6845,\n",
       "  2526,\n",
       "  4822,\n",
       "  4357,\n",
       "  4340,\n",
       "  311,\n",
       "  1494,\n",
       "  26679,\n",
       "  304,\n",
       "  518,\n",
       "  15592,\n",
       "  4340,\n",
       "  311,\n",
       "  6718,\n",
       "  553,\n",
       "  3668,\n",
       "  4340,\n",
       "  311,\n",
       "  6500,\n",
       "  6236,\n",
       "  1614,\n",
       "  14507,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  4379,\n",
       "  13388,\n",
       "  4340,\n",
       "  311,\n",
       "  2930,\n",
       "  894,\n",
       "  1614,\n",
       "  304,\n",
       "  825,\n",
       "  1555,\n",
       "  4340,\n",
       "  311,\n",
       "  3754,\n",
       "  3950,\n",
       "  10431,\n",
       "  304,\n",
       "  12853,\n",
       "  16969,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  7375,\n",
       "  311,\n",
       "  6236,\n",
       "  61905,\n",
       "  4340,\n",
       "  311,\n",
       "  6718,\n",
       "  2038,\n",
       "  4340,\n",
       "  311,\n",
       "  653,\n",
       "  56370,\n",
       "  448,\n",
       "  65151,\n",
       "  25111,\n",
       "  4340,\n",
       "  311,\n",
       "  5508,\n",
       "  6452,\n",
       "  77,\n",
       "  4788,\n",
       "  311,\n",
       "  13852,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  2526,\n",
       "  4822,\n",
       "  24083,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  264,\n",
       "  2526,\n",
       "  6236,\n",
       "  1614,\n",
       "  536,\n",
       "  10268,\n",
       "  37068,\n",
       "  24602,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  264,\n",
       "  2526,\n",
       "  444,\n",
       "  10994,\n",
       "  536,\n",
       "  10268,\n",
       "  10392,\n",
       "  461,\n",
       "  2054,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  7375,\n",
       "  4340,\n",
       "  311,\n",
       "  7390,\n",
       "  697,\n",
       "  444,\n",
       "  10994,\n",
       "  10500,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  27445,\n",
       "  82,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  9293,\n",
       "  504,\n",
       "  264,\n",
       "  6220,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  9308,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  4718,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  73192,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  5100,\n",
       "  8246,\n",
       "  3542,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  11358,\n",
       "  82,\n",
       "  4340,\n",
       "  311,\n",
       "  2795,\n",
       "  3482,\n",
       "  6816,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  264,\n",
       "  8741,\n",
       "  320,\n",
       "  721,\n",
       "  12,\n",
       "  7596,\n",
       "  287,\n",
       "  8,\n",
       "  8781,\n",
       "  1178,\n",
       "  39088,\n",
       "  4119,\n",
       "  4340,\n",
       "  311,\n",
       "  15963,\n",
       "  3059,\n",
       "  504,\n",
       "  5248,\n",
       "  10759,\n",
       "  3004,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  504,\n",
       "  264,\n",
       "  22463,\n",
       "  41885,\n",
       "  10337,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  553,\n",
       "  3084,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  553,\n",
       "  53129,\n",
       "  31773,\n",
       "  40861,\n",
       "  320,\n",
       "  8035,\n",
       "  49,\n",
       "  8,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  553,\n",
       "  308,\n",
       "  12,\n",
       "  1520,\n",
       "  27248,\n",
       "  4340,\n",
       "  311,\n",
       "  3293,\n",
       "  10295,\n",
       "  553,\n",
       "  37623,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  5785,\n",
       "  10295,\n",
       "  979,\n",
       "  3730,\n",
       "  32189,\n",
       "  4340,\n",
       "  311,\n",
       "  3705,\n",
       "  1293,\n",
       "  1467,\n",
       "  979,\n",
       "  3730,\n",
       "  32189,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  49645,\n",
       "  7484,\n",
       "  320,\n",
       "  2152,\n",
       "  5392,\n",
       "  8098,\n",
       "  8,\n",
       "  311,\n",
       "  653,\n",
       "  32189,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  32772,\n",
       "  82,\n",
       "  311,\n",
       "  264,\n",
       "  78679,\n",
       "  4340,\n",
       "  311,\n",
       "  4051,\n",
       "  6605,\n",
       "  30816,\n",
       "  16223,\n",
       "  7542,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  279,\n",
       "  22463,\n",
       "  18837,\n",
       "  51980,\n",
       "  5333,\n",
       "  4340,\n",
       "  311,\n",
       "  24085,\n",
       "  1598,\n",
       "  77,\n",
       "  4788,\n",
       "  26223,\n",
       "  18837,\n",
       "  16378,\n",
       "  11434,\n",
       "  8436,\n",
       "  1862,\n",
       "  3674,\n",
       "  4340,\n",
       "  311,\n",
       "  6500,\n",
       "  444,\n",
       "  10994,\n",
       "  14507,\n",
       "  4340,\n",
       "  311,\n",
       "  3754,\n",
       "  3950,\n",
       "  10431,\n",
       "  369,\n",
       "  444,\n",
       "  10994,\n",
       "  82,\n",
       "  6727,\n",
       "  4119,\n",
       "  23490,\n",
       "  4340,\n",
       "  311,\n",
       "  633,\n",
       "  1487,\n",
       "  48216,\n",
       "  4340,\n",
       "  311,\n",
       "  83184,\n",
       "  30403,\n",
       "  3059,\n",
       "  311,\n",
       "  49360,\n",
       "  279,\n",
       "  330,\n",
       "  54337,\n",
       "  304,\n",
       "  279,\n",
       "  6149,\n",
       "  1,\n",
       "  2456,\n",
       "  4340,\n",
       "  311,\n",
       "  6718,\n",
       "  73192,\n",
       "  553,\n",
       "  21426,\n",
       "  4340,\n",
       "  311,\n",
       "  10880,\n",
       "  23921,\n",
       "  6605,\n",
       "  315,\n",
       "  279,\n",
       "  1852,\n",
       "  943,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  1943,\n",
       "  3840,\n",
       "  4340,\n",
       "  311,\n",
       "  44566,\n",
       "  504,\n",
       "  19588,\n",
       "  22463,\n",
       "  18837,\n",
       "  13009,\n",
       "  311,\n",
       "  22463,\n",
       "  11212,\n",
       "  4340,\n",
       "  311,\n",
       "  17179,\n",
       "  1667,\n",
       "  5248,\n",
       "  22879,\n",
       "  817,\n",
       "  2197,\n",
       "  4340,\n",
       "  311,\n",
       "  1494,\n",
       "  79049,\n",
       "  57597,\n",
       "  821,\n",
       "  5961,\n",
       "  311,\n",
       "  4119,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  79049,\n",
       "  57597,\n",
       "  50932,\n",
       "  4340,\n",
       "  311,\n",
       "  1855,\n",
       "  264,\n",
       "  2526,\n",
       "  9258,\n",
       "  21102,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  279,\n",
       "  2550,\n",
       "  70913,\n",
       "  287,\n",
       "  6729,\n",
       "  4340,\n",
       "  311,\n",
       "  4715,\n",
       "  4718,\n",
       "  2550,\n",
       "  4340,\n",
       "  311,\n",
       "  22683,\n",
       "  979,\n",
       "  264,\n",
       "  22314,\n",
       "  1465,\n",
       "  13666,\n",
       "  4340,\n",
       "  311,\n",
       "  4715,\n",
       "  1467,\n",
       "  504,\n",
       "  1943,\n",
       "  6171,\n",
       "  4340,\n",
       "  311,\n",
       "  4715,\n",
       "  11874,\n",
       "  2550,\n",
       "  4340,\n",
       "  311,\n",
       "  4715,\n",
       "  53127,\n",
       "  2550,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  279,\n",
       "  17022,\n",
       "  11789,\n",
       "  10392,\n",
       "  461,\n",
       "  2054,\n",
       "  4340,\n",
       "  311,\n",
       "  990,\n",
       "  22463,\n",
       "  18837,\n",
       "  448,\n",
       "  2155,\n",
       "  5355,\n",
       "  67,\n",
       "  8159,\n",
       "  10795,\n",
       "  4340,\n",
       "  311,\n",
       "  912,\n",
       "  6236,\n",
       "  3840,\n",
       "  4340,\n",
       "  311,\n",
       "  633,\n",
       "  264,\n",
       "  431,\n",
       "  1890,\n",
       "  3766,\n",
       "  311,\n",
       "  912,\n",
       "  51846,\n",
       "  4340,\n",
       "  311,\n",
       "  653,\n",
       "  817,\n",
       "  8694,\n",
       "  56370,\n",
       "  4340,\n",
       "  311,\n",
       "  633,\n",
       "  697,\n",
       "  431,\n",
       "  1890,\n",
       "  3766,\n",
       "  311,\n",
       "  470,\n",
       "  8173,\n",
       "  4340,\n",
       "  311,\n",
       "  4269,\n",
       "  3059,\n",
       "  504,\n",
       "  697,\n",
       "  431,\n",
       "  1890,\n",
       "  3766,\n",
       "  4340,\n",
       "  311,\n",
       "  ...]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doc texts\n",
    "docs.extend([*docs_pydantic, *docs_sq])\n",
    "\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "\n",
    "print(docs_texts)\n",
    "\n",
    "# Calculate the number of tokens for each document\n",
    "counts = [num_tokens_from_string(doc_text) for doc_text in docs_texts]\n",
    "\n",
    "counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAIjCAYAAAD1OgEdAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAWxhJREFUeJzt3XlcVHX////nwLCKgKiAuCDu+5KmcmmWaeKSmtlVmrmUbYapaWpelWlWmqZmaVm/Sq20xevTdqmpuKQtlGniWqhlWClIoo4gAsOc3x99mRpZBOQ4oI/77Ta3y3mf97zP67yHK+bJOec9FsMwDAEAAAAAypSHuwsAAAAAgCsRYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAqiLp162rkyJHuLuOKN3fuXNWrV0+enp5q06aNqfv64osvZLFY9N///tfU/QAA3IOwBQBusGzZMlksFu3YsaPA7TfccINatGhxyftZu3atpk+ffsnjXC02bNigyZMnq3Pnzlq6dKmee+65fH3yAlJxHhXR+fPntWDBAnXs2FFBQUHy9fVVo0aNNGbMGB08eNDd5UmSvvnmG02fPl2nT592dykAUCSruwsAABRPYmKiPDxK9jeytWvXavHixQSuYtq8ebM8PDz05ptvytvbu8A+TZs21TvvvOPSNnXqVAUEBOjxxx+/HGWa5s8//1SvXr20c+dO3XzzzbrzzjsVEBCgxMREvf/++3r99deVnZ3t7jL1zTffaMaMGRo5cqSCg4PdXQ4AFIqwBQAVhI+Pj7tLKLGMjAxVqlTJ3WUU24kTJ+Tn51do0JKksLAw3XXXXS5ts2fPVrVq1fK1VzQjR47Url279N///leDBg1y2TZz5swKHyYB4HLjMkIAqCAuvGcrJydHM2bMUMOGDeXr66uqVauqS5cuiouLk/TXB+fFixdLUoGXtmVkZGjixImqXbu2fHx81LhxY73wwgsyDMNlv5mZmRo7dqyqVaumypUrq3///vrjjz9ksVhczphNnz5dFotFBw4c0J133qkqVaqoS5cukqQ9e/Zo5MiRqlevnnx9fRUeHq577rlHJ0+edNlX3hgHDx7UXXfdpaCgIFWvXl1PPvmkDMPQb7/9pgEDBigwMFDh4eGaN29esebObrdr5syZql+/vnx8fFS3bl395z//UVZWlrOPxWLR0qVLlZGR4ZyrZcuWFWv8gvzyyy/697//rZCQEPn7+6tTp05as2bNRV+XlZWlm2++WUFBQfrmm28kSQ6HQy+++KKaN28uX19fhYWF6YEHHtCpU6dcXlu3bl3dfPPN+uqrr9ShQwf5+vqqXr16evvtty+63++++05r1qzRqFGj8gUt6a+w/8ILL7i0bd68Wdddd50qVaqk4OBgDRgwQD/++KNLn5EjR6pu3br5xst7r//JYrFozJgx+uSTT9SiRQv5+PioefPmWrduncvrJk2aJEmKiopyvle//vqrJCkuLk5dunRRcHCwAgIC1LhxY/3nP/+56PEDgBk4swUAbnTmzBn9+eef+dpzcnIu+trp06dr1qxZuvfee9WhQwfZbDbt2LFDP/zwg2666SY98MADOnbsmOLi4vJd9mYYhvr3768tW7Zo1KhRatOmjdavX69Jkybpjz/+0IIFC5x9R44cqQ8//FDDhg1Tp06dtHXrVvXt27fQuv7973+rYcOGeu6555zBLS4uTr/88ovuvvtuhYeHa//+/Xr99de1f/9+ffvtt/k+dN9xxx1q2rSpZs+erTVr1uiZZ55RSEiIXnvtNd144416/vnntWLFCj366KO69tpr1bVr1yLn6t5779Xy5ct12223aeLEifruu+80a9Ys/fjjj/r4448lSe+8845ef/11bd++XW+88YYk6V//+tdF34eCpKSk6F//+pfOnTunsWPHqmrVqlq+fLn69++v//73vxo4cGCBr8vMzNSAAQO0Y8cObdy4Uddee60k6YEHHtCyZct09913a+zYsTpy5IgWLVqkXbt26euvv5aXl5dzjMOHD+u2227TqFGjNGLECL311lsaOXKk2rVrp+bNmxda82effSZJGjZsWLGOcePGjerdu7fq1aun6dOnKzMzUy+//LI6d+6sH374ocCAVRxfffWVPvroIz300EOqXLmyXnrpJQ0aNEhHjx5V1apVdeutt+rgwYN67733tGDBAlWrVk2SVL16de3fv18333yzWrVqpaefflo+Pj46fPiwvv7661LVAgCXzAAAXHZLly41JBX5aN68uctrIiMjjREjRjift27d2ujbt2+R+4mNjTUK+k/9J598YkgynnnmGZf22267zbBYLMbhw4cNwzCMnTt3GpKM8ePHu/QbOXKkIcl46qmnnG1PPfWUIckYMmRIvv2dO3cuX9t7771nSDK2bduWb4z777/f2Wa3241atWoZFovFmD17trP91KlThp+fn8ucFCQhIcGQZNx7770u7Y8++qghydi8ebOzbcSIEUalSpWKHK8gzZs3N66//nrn8/HjxxuSjC+//NLZdvbsWSMqKsqoW7eukZubaxiGYWzZssWQZKxatco4e/ascf311xvVqlUzdu3a5Xzdl19+aUgyVqxY4bLPdevW5WuPjIzMN6cnTpwwfHx8jIkTJxZ5DAMHDjQkGadOnSrWMbdp08YIDQ01Tp486WzbvXu34eHhYQwfPtzZNmLECCMyMjLf6/Pe63+SZHh7ezt//vLGlGS8/PLLzra5c+cakowjR464vH7BggWGJCM1NbVYxwAAZuMyQgBwo8WLFysuLi7fo1WrVhd9bXBwsPbv369Dhw6VeL9r166Vp6enxo4d69I+ceJEGYahzz//XJKcl2899NBDLv0efvjhQsd+8MEH87X5+fk5/33+/Hn9+eef6tSpkyTphx9+yNf/3nvvdf7b09NT7du3l2EYGjVqlLM9ODhYjRs31i+//FJoLdJfxypJEyZMcGmfOHGiJBXr0r6SWrt2rTp06OC8jFKSAgICdP/99+vXX3/VgQMHXPqfOXNGPXv21E8//aQvvvjCZcn5VatWKSgoSDfddJP+/PNP56Ndu3YKCAjQli1bXMZq1qyZrrvuOufz6tWrF2uebDabJKly5coXPb7jx48rISFBI0eOVEhIiLO9VatWuummm5xzXho9evRQ/fr1XcYMDAy8aP2SnItlfPrpp3I4HKWuAQDKCmELANyoQ4cO6tGjR75HlSpVLvrap59+WqdPn1ajRo3UsmVLTZo0SXv27CnWfpOSkhQREZHvg3XTpk2d2/P+18PDQ1FRUS79GjRoUOjYF/aVpLS0NI0bN05hYWHy8/NT9erVnf3OnDmTr3+dOnVcnuctQZ53ydg/2y+8b+lCecdwYc3h4eEKDg52HmtZSkpKUuPGjfO1Xzi/ecaPH6/vv/9eGzduzHep36FDh3TmzBmFhoaqevXqLo/09HSdOHHCpf+FcydJVapUueg8BQYGSpLOnj1brOOTVOgx/vnnn8rIyLjoOAUpbf3SX5efdu7cWffee6/CwsI0ePBgffjhhwQvAG7DPVsAUEF17dpVP//8sz799FNt2LBBb7zxhhYsWKAlS5a4nBm63P55FivP7bffrm+++UaTJk1SmzZtFBAQIIfDoV69ehX4QdjT07NYbZLyLehRmPL8vVcDBgzQ+++/r9mzZ+vtt992WeLf4XAoNDRUK1asKPC11atXd3le2nlq0qSJJGnv3r0uZ8YuVWHznpubW2D7pbzPfn5+2rZtm7Zs2aI1a9Zo3bp1+uCDD3TjjTdqw4YNhY4NAGbhzBYAVGAhISG6++679d577+m3335Tq1atXFYILOyDbmRkpI4dO5bvLMZPP/3k3J73vw6HQ0eOHHHpd/jw4WLXeOrUKW3atEmPPfaYZsyYoYEDB+qmm25SvXr1ij3Gpcg7hgsvt0xJSdHp06edx1rW+0xMTMzXfuH85rnlllv01ltvaeXKlYqNjXXZVr9+fZ08eVKdO3cu8Cxo69aty6Tmfv36SZLefffdi/bNq7+wY6xWrZpzyf8qVaoU+OXDl3JGsajg7OHhoe7du2v+/Pk6cOCAnn32WW3evDnf5ZYAcDkQtgCggrpw2fSAgAA1aNDAZTnzvA+8F37Y7dOnj3Jzc7Vo0SKX9gULFshisah3796SpJiYGEnSK6+84tLv5ZdfLnadeWcTLjwz8eKLLxZ7jEvRp0+fAvc3f/58SSpyZcVL2ef27dsVHx/vbMvIyNDrr7+uunXrqlmzZvleM3z4cL300ktasmSJpkyZ4my//fbblZubq5kzZ+Z7jd1uLzDIlEZ0dLR69eqlN954Q5988km+7dnZ2Xr00UclSTVq1FCbNm20fPlyl/3v27dPGzZscM659FdYPHPmjMslrsePH3euAlkahf1cp6Wl5eubd//bP/9/AQCXC5cRAkAF1axZM91www1q166dQkJCtGPHDv33v//VmDFjnH3atWsnSRo7dqxiYmLk6empwYMHq1+/furWrZsef/xx/frrr2rdurU2bNigTz/9VOPHj3cuUNCuXTsNGjRIL774ok6ePOlc+v3gwYOSindpXmBgoLp27ao5c+YoJydHNWvW1IYNG/KdLTNL69atNWLECL3++us6ffq0rr/+em3fvl3Lly/XLbfcom7dupX5Ph977DG999576t27t8aOHauQkBAtX75cR44c0f/93/+5XCb4T2PGjJHNZtPjjz+uoKAg/ec//9H111+vBx54QLNmzVJCQoJ69uwpLy8vHTp0SKtWrdLChQt12223lUndb7/9tnr27Klbb71V/fr1U/fu3VWpUiUdOnRI77//vo4fP+78rq25c+eqd+/eio6O1qhRo5xLvwcFBbmcXR08eLCmTJmigQMHauzYsTp37pxeffVVNWrUqMDFUYoj7+f68ccf1+DBg+Xl5aV+/frp6aef1rZt29S3b19FRkbqxIkTeuWVV1SrVi2XxUoA4LJx51KIAHC1ylv6/fvvvy9w+/XXX3/Rpd+feeYZo0OHDkZwcLDh5+dnNGnSxHj22WeN7OxsZx+73W48/PDDRvXq1Q2LxeKy1PbZs2eNRx55xIiIiDC8vLyMhg0bGnPnzjUcDofLfjMyMozY2FgjJCTECAgIMG655RYjMTHRkOSyFHveUt4FLbv9+++/GwMHDjSCg4ONoKAg49///rdx7NixQpePv3CMwpZkL2ieCpKTk2PMmDHDiIqKMry8vIzatWsbU6dONc6fP1+s/VzMhUu/G4Zh/Pzzz8Ztt91mBAcHG76+vkaHDh2M1atXu/T559Lv/zR58mRDkrFo0SJn2+uvv260a9fO8PPzMypXrmy0bNnSmDx5snHs2DFnn8jIyAK/DuD666/PV19hzp07Z7zwwgvGtddeawQEBBje3t5Gw4YNjYcffthlSXbDMIyNGzcanTt3Nvz8/IzAwECjX79+xoEDB/KNuWHDBqNFixaGt7e30bhxY+Pdd98tdOn32NjYfK+/8GffMAxj5syZRs2aNQ0PDw/nMvCbNm0yBgwYYERERBje3t5GRESEMWTIEOPgwYPFOnYAKGsWwyjmncUAAPw/CQkJatu2rd59910NHTrU3eUAAFAucc8WAKBImZmZ+dpefPFFeXh4qGvXrm6oCACAioF7tgAARZozZ4527typbt26yWq16vPPP9fnn3+u+++/X7Vr13Z3eQAAlFtcRggAKFJcXJxmzJihAwcOKD09XXXq1NGwYcP0+OOPy2rlb3YAABSGsAUAAAAAJuCeLQAAAAAwAWELAAAAAEzAxfbF4HA4dOzYMVWuXLlYX+AJAAAA4MpkGIbOnj2riIiIQr+kPg9hqxiOHTvGilsAAAAAnH777TfVqlWryD6ErWKoXLmypL8mNDAw0M3VAAAAAHAXm82m2rVrOzNCUQhbxZB36WBgYCBhCwAAAECxbi9igQwAAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATGB1dwG4PFJTU2Wz2QrcFhgYqOrVq1/migAAAIArG2HrKpCamqpho4YpLSOtwO0hlUL0zpvvELgAAACAMkTYugrYbDalZaSpZp+aCggNcNmWfiJdf6z9QzabjbAFAAAAlCHC1lUkIDRAQRFB7i4DAAAAuCqwQAYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJ3Bq2Xn31VbVq1UqBgYEKDAxUdHS0Pv/8c+f28+fPKzY2VlWrVlVAQIAGDRqklJQUlzGOHj2qvn37yt/fX6GhoZo0aZLsdrtLny+++ELXXHONfHx81KBBAy1btuxyHB4AAACAq5hbw1atWrU0e/Zs7dy5Uzt27NCNN96oAQMGaP/+/ZKkRx55RP/73/+0atUqbd26VceOHdOtt97qfH1ubq769u2r7OxsffPNN1q+fLmWLVumadOmOfscOXJEffv2Vbdu3ZSQkKDx48fr3nvv1fr16y/78QIAAAC4elgMwzDcXcQ/hYSEaO7cubrttttUvXp1rVy5Urfddpsk6aefflLTpk0VHx+vTp066fPPP9fNN9+sY8eOKSwsTJK0ZMkSTZkyRampqfL29taUKVO0Zs0a7du3z7mPwYMH6/Tp01q3bl2xarLZbAoKCtKZM2cUGBhY9gdtsp9//llD7h+ixiMbKygiyGXbmWNnlLgsUe+9/p7q16/vpgoBAACAiqEk2cB6mWq6qNzcXK1atUoZGRmKjo7Wzp07lZOTox49ejj7NGnSRHXq1HGGrfj4eLVs2dIZtCQpJiZGo0eP1v79+9W2bVvFx8e7jJHXZ/z48YXWkpWVpaysLOdzm80mSbLb7fkuUawIHA6HrFarPOUpD8P1ZKanPGW1WuVwOCrksQEAAACXU0k+M7s9bO3du1fR0dE6f/68AgIC9PHHH6tZs2ZKSEiQt7e3goODXfqHhYUpOTlZkpScnOwStPK2520rqo/NZlNmZqb8/Pzy1TRr1izNmDEjX/uOHTtUqVKlUh+ru2RmZuqO/nfI399f1vOub7nd3662/dsqKSlJJ06ccFOFAAAAQMWQkZFR7L5uD1uNGzdWQkKCzpw5o//+978aMWKEtm7d6taapk6dqgkTJjif22w21a5dW+3bt6+QlxEeOXJE0+ZOU6O7GikwyLV+2ymbDn52UMsXLVdUVJSbKgQAAAAqhryr3orD7WHL29tbDRo0kCS1a9dO33//vRYuXKg77rhD2dnZOn36tMvZrZSUFIWHh0uSwsPDtX37dpfx8lYr/GefC1cwTElJUWBgYIFntSTJx8dHPj4++dqtVqusVrdPWYl5eHjIbrcrV7lyWBwu23KVK7vdLg8Pjwp5bAAAAMDlVJLPzOXue7YcDoeysrLUrl07eXl5adOmTc5tiYmJOnr0qKKjoyVJ0dHR2rt3r8vlb3FxcQoMDFSzZs2cff45Rl6fvDEAAAAAwAxuPZUxdepU9e7dW3Xq1NHZs2e1cuVKffHFF1q/fr2CgoI0atQoTZgwQSEhIQoMDNTDDz+s6OhoderUSZLUs2dPNWvWTMOGDdOcOXOUnJysJ554QrGxsc4zUw8++KAWLVqkyZMn65577tHmzZv14Ycfas2aNe48dAAAAABXOLeGrRMnTmj48OE6fvy4goKC1KpVK61fv1433XSTJGnBggXy8PDQoEGDlJWVpZiYGL3yyivO13t6emr16tUaPXq0oqOjValSJY0YMUJPP/20s09UVJTWrFmjRx55RAsXLlStWrX0xhtvKCYm5rIfLwAAAICrR7n7nq3yiO/ZAgAAACCVLBuUu3u2AAAAAOBKQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMIFbw9asWbN07bXXqnLlygoNDdUtt9yixMRElz433HCDLBaLy+PBBx906XP06FH17dtX/v7+Cg0N1aRJk2S32136fPHFF7rmmmvk4+OjBg0aaNmyZWYfHgAAAICrmFvD1tatWxUbG6tvv/1WcXFxysnJUc+ePZWRkeHS77777tPx48edjzlz5ji35ebmqm/fvsrOztY333yj5cuXa9myZZo2bZqzz5EjR9S3b19169ZNCQkJGj9+vO69916tX7/+sh0rAAAAgKuL1Z07X7duncvzZcuWKTQ0VDt37lTXrl2d7f7+/goPDy9wjA0bNujAgQPauHGjwsLC1KZNG82cOVNTpkzR9OnT5e3trSVLligqKkrz5s2TJDVt2lRfffWVFixYoJiYGPMOEAAAAMBVy61h60JnzpyRJIWEhLi0r1ixQu+++67Cw8PVr18/Pfnkk/L395ckxcfHq2XLlgoLC3P2j4mJ0ejRo7V//361bdtW8fHx6tGjh8uYMTExGj9+fIF1ZGVlKSsry/ncZrNJkux2e77LEysCh8Mhq9UqT3nKw3A9mekpT1mtVjkcjgp5bAAAAMDlVJLPzOUmbDkcDo0fP16dO3dWixYtnO133nmnIiMjFRERoT179mjKlClKTEzURx99JElKTk52CVqSnM+Tk5OL7GOz2ZSZmSk/Pz+XbbNmzdKMGTPy1bhjxw5VqlTp0g/2MsvMzNQd/e+Qv7+/rOdd33K7v11t+7dVUlKSTpw44aYKAQAAgIrhwlueilJuwlZsbKz27dunr776yqX9/vvvd/67ZcuWqlGjhrp3766ff/5Z9evXN6WWqVOnasKECc7nNptNtWvXVvv27RUYGGjKPs105MgRTZs7TY3uaqTAINf6badsOvjZQS1ftFxRUVFuqhAAAACoGPKueiuOchG2xowZo9WrV2vbtm2qVatWkX07duwoSTp8+LDq16+v8PBwbd++3aVPSkqKJDnv8woPD3e2/bNPYGBgvrNakuTj4yMfH5987VarVVZruZiyEvHw8JDdbleucuWwOFy25SpXdrtdHh4eFfLYAAAAgMupJJ+Z3boaoWEYGjNmjD7++GNt3ry5WGdWEhISJEk1atSQJEVHR2vv3r0ul8DFxcUpMDBQzZo1c/bZtGmTyzhxcXGKjo4uoyMBAAAAAFduDVuxsbF69913tXLlSlWuXFnJyclKTk5WZmamJOnnn3/WzJkztXPnTv3666/67LPPNHz4cHXt2lWtWrWSJPXs2VPNmjXTsGHDtHv3bq1fv15PPPGEYmNjnWenHnzwQf3yyy+aPHmyfvrpJ73yyiv68MMP9cgjj7jt2AEAAABc2dwatl599VWdOXNGN9xwg2rUqOF8fPDBB5Ikb29vbdy4UT179lSTJk00ceJEDRo0SP/73/+cY3h6emr16tXy9PRUdHS07rrrLg0fPlxPP/20s09UVJTWrFmjuLg4tW7dWvPmzdMbb7zBsu8AAAAATOPWm3QMwyhye+3atbV169aLjhMZGam1a9cW2eeGG27Qrl27SlQfAAAAAJSWW89sAQAAAMCVirAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmsLq7AJROamqqbDZbvvbAwEBVr17dDRUBAAAA+CfCVgWUmpqqu+6+V2lnz+XbFlLZX+8ufYPABQAAALgZYasCstlsSjt7TtWjB6lSSJizPSMtRanx/yebzUbYAgAAANyMsFWBVQoJU2BoLZe2VDfVAgAAAMAVC2QAAAAAgAkIWwAAAABgAreGrVmzZunaa69V5cqVFRoaqltuuUWJiYkufc6fP6/Y2FhVrVpVAQEBGjRokFJSUlz6HD16VH379pW/v79CQ0M1adIk2e12lz5ffPGFrrnmGvn4+KhBgwZatmyZ2YcHAAAA4Crm1rC1detWxcbG6ttvv1VcXJxycnLUs2dPZWRkOPs88sgj+t///qdVq1Zp69atOnbsmG699Vbn9tzcXPXt21fZ2dn65ptvtHz5ci1btkzTpk1z9jly5Ij69u2rbt26KSEhQePHj9e9996r9evXX9bjBQAAAHD1cOsCGevWrXN5vmzZMoWGhmrnzp3q2rWrzpw5ozfffFMrV67UjTfeKElaunSpmjZtqm+//VadOnXShg0bdODAAW3cuFFhYWFq06aNZs6cqSlTpmj69Ony9vbWkiVLFBUVpXnz5kmSmjZtqq+++koLFixQTEzMZT9uAAAAAFe+crUa4ZkzZyRJISEhkqSdO3cqJydHPXr0cPZp0qSJ6tSpo/j4eHXq1Enx8fFq2bKlwsL+XgI9JiZGo0eP1v79+9W2bVvFx8e7jJHXZ/z48QXWkZWVpaysLOfzvC8Pttvt+S5PdAeHwyGr1SpPi+Qph7Pd0yJZrVY5HA6XOp395SkPw/Vkpqc8C3wNAAAAgPxK8pm53IQth8Oh8ePHq3PnzmrRooUkKTk5Wd7e3goODnbpGxYWpuTkZGeffwatvO1524rqY7PZlJmZKT8/P5dts2bN0owZM/LVuGPHDlWqVKn0B1lGMjMzNXjgzfKtapHVK83Zbvex6PzAm5WUlKQTJ0649L+j/x3y9/eX9bzrW273t6tt/7b5XgMAAAAgv3/e8nQx5SZsxcbGat++ffrqq6/cXYqmTp2qCRMmOJ/bbDbVrl1b7du3V2BgoBsr+8uRI0f05Kx5iux1vwKrhzjbbanHlLRutd5e0lNRUVEu/afNnaZGdzVSYJBr/bZTNh387KCWL1ru8hoAAAAA+eVd9VYc5SJsjRkzRqtXr9a2bdtUq9bfX9IbHh6u7OxsnT592uXsVkpKisLDw519tm/f7jJe3mqF/+xz4QqGKSkpCgwMzHdWS5J8fHzk4+OTr91qtcpqdf+UeXh4yG63K9eQcv+xxkmu8ddpTQ8PD5c6nf2VK4fF4TJWrnILfA0AAACA/ErymdmtqxEahqExY8bo448/1ubNm/OdWWnXrp28vLy0adMmZ1tiYqKOHj2q6OhoSVJ0dLT27t3rcglcXFycAgMD1axZM2eff46R1ydvDAAAAAAoa249lREbG6uVK1fq008/VeXKlZ33WAUFBcnPz09BQUEaNWqUJkyYoJCQEAUGBurhhx9WdHS0OnXqJEnq2bOnmjVrpmHDhmnOnDlKTk7WE088odjYWOfZqQcffFCLFi3S5MmTdc8992jz5s368MMPtWbNGrcdOwAAAIArm1vPbL366qs6c+aMbrjhBtWoUcP5+OCDD5x9FixYoJtvvlmDBg1S165dFR4ero8++si53dPTU6tXr5anp6eio6N11113afjw4Xr66aedfaKiorRmzRrFxcWpdevWmjdvnt544w2WfQcAAABgGree2TIM46J9fH19tXjxYi1evLjQPpGRkVq7dm2R49xwww3atWtXiWsEAAAAgNJw65ktAAAAALhSEbYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATlCps/fLLL2VdBwAAAABcUUoVtho0aKBu3brp3Xff1fnz58u6JgAAAACo8EoVtn744Qe1atVKEyZMUHh4uB544AFt3769rGsDAAAAgAqrVGGrTZs2WrhwoY4dO6a33npLx48fV5cuXdSiRQvNnz9fqampZV0nAAAAAFQol7RAhtVq1a233qpVq1bp+eef1+HDh/Xoo4+qdu3aGj58uI4fP15WdQIAAABAhXJJYWvHjh166KGHVKNGDc2fP1+PPvqofv75Z8XFxenYsWMaMGBAWdUJAAAAABWKtTQvmj9/vpYuXarExET16dNHb7/9tvr06SMPj7+yW1RUlJYtW6a6deuWZa0AAAAAUGGUKmy9+uqruueeezRy5EjVqFGjwD6hoaF68803L6k4AAAAAKioShW2Dh06dNE+3t7eGjFiRGmGBwAAAIAKr1T3bC1dulSrVq3K175q1SotX778kosCAAAAgIquVGFr1qxZqlatWr720NBQPffcc5dcFAAAAABUdKUKW0ePHlVUVFS+9sjISB09evSSiwIAAACAiq5UYSs0NFR79uzJ1757925VrVr1kosCAAAAgIquVGFryJAhGjt2rLZs2aLc3Fzl5uZq8+bNGjdunAYPHlzWNQIAAABAhVOq1QhnzpypX3/9Vd27d5fV+tcQDodDw4cP554tAAAAAFApw5a3t7c++OADzZw5U7t375afn59atmypyMjIsq4PAAAAACqkUoWtPI0aNVKjRo3KqhYAAAAAuGKUKmzl5uZq2bJl2rRpk06cOCGHw+GyffPmzWVSHAAAAABUVKUKW+PGjdOyZcvUt29ftWjRQhaLpazrAgAAAIAKrVRh6/3339eHH36oPn36lHU9uEQ52dlKSkpyaUtKSlKuPddNFQEAAABXp1IvkNGgQYOyrgWXKCv9jE4e2afZj4+Tj7e3sz0jM0tHTxxTgxzeMwAAAOByKVXYmjhxohYuXKhFixZxCWE5kpOVKV8Pux7pHKBGNUOc7d8ePqlxn+XKnmN3Y3UAAADA1aVUYeurr77Sli1b9Pnnn6t58+by8vJy2f7RRx+VSXEonVpVfFU/LMD5POnPDDdWAwAAAFydShW2goODNXDgwLKuBQAAAACuGKUKW0uXLi3rOgAAAADgiuJR2hfa7XZt3LhRr732ms6ePStJOnbsmNLT08usOAAAAACoqEp1ZispKUm9evXS0aNHlZWVpZtuukmVK1fW888/r6ysLC1ZsqSs6wQAAACACqVUZ7bGjRun9u3b69SpU/Lz83O2Dxw4UJs2bSqz4gAAAACgoirVma0vv/xS33zzjbz/8V1OklS3bl398ccfZVIYAAAAAFRkpTqz5XA4lJubm6/9999/V+XKlS+5KAAAAACo6EoVtnr27KkXX3zR+dxisSg9PV1PPfWU+vTpU1a1AQAAAECFVarLCOfNm6eYmBg1a9ZM58+f15133qlDhw6pWrVqeu+998q6RgAAAACocEoVtmrVqqXdu3fr/fff1549e5Senq5Ro0Zp6NChLgtmAAAAAMDVqlRhS5KsVqvuuuuusqwFAAAAAK4YpQpbb7/9dpHbhw8fXqpiAAAAAOBKUaqwNW7cOJfnOTk5OnfunLy9veXv70/YAgAAAHDVK9VqhKdOnXJ5pKenKzExUV26dGGBDAAAAABQKcNWQRo2bKjZs2fnO+sFAAAAAFejMgtb0l+LZhw7dqwshwQAAACACqlU92x99tlnLs8Nw9Dx48e1aNEide7cuUwKAwAAAICKrFRh65ZbbnF5brFYVL16dd14442aN29eWdQFAAAAABVaqcKWw+Eo6zoAAAAA4IpSpvdsAQAAAAD+UqozWxMmTCh23/nz55dmFwAAAABQoZUqbO3atUu7du1STk6OGjduLEk6ePCgPD09dc011zj7WSyWsqkSAAAAACqYUoWtfv36qXLlylq+fLmqVKki6a8vOr777rt13XXXaeLEiWVaJAAAAABUNKW6Z2vevHmaNWuWM2hJUpUqVfTMM8+wGiEAAAAAqJRhy2azKTU1NV97amqqzp49W+xxtm3bpn79+ikiIkIWi0WffPKJy/aRI0fKYrG4PHr16uXSJy0tTUOHDlVgYKCCg4M1atQopaenu/TZs2ePrrvuOvn6+qp27dqaM2dO8Q8WAAAAAEqhVGFr4MCBuvvuu/XRRx/p999/1++//67/+7//06hRo3TrrbcWe5yMjAy1bt1aixcvLrRPr169dPz4cefjvffec9k+dOhQ7d+/X3FxcVq9erW2bdum+++/37ndZrOpZ8+eioyM1M6dOzV37lxNnz5dr7/+eskPHAAAAACKqVT3bC1ZskSPPvqo7rzzTuXk5Pw1kNWqUaNGae7cucUep3fv3urdu3eRfXx8fBQeHl7gth9//FHr1q3T999/r/bt20uSXn75ZfXp00cvvPCCIiIitGLFCmVnZ+utt96St7e3mjdvroSEBM2fP98llAEAAABAWSpV2PL399crr7yiuXPn6ueff5Yk1a9fX5UqVSrT4iTpiy++UGhoqKpUqaIbb7xRzzzzjKpWrSpJio+PV3BwsDNoSVKPHj3k4eGh7777TgMHDlR8fLy6du0qb29vZ5+YmBg9//zzOnXqlMt9Z3mysrKUlZXlfG6z2SRJdrtddru9zI+xpBwOh6xWqzwtkqf+/oJpq4dFXl7eclisssvT2W54WOXt5S1PecrDcD2Z6SlPWa1WORyOcnFsAAAAQHlWks/MpQpbefIu7evatav8/PxkGEaZLvfeq1cv3XrrrYqKitLPP/+s//znP+rdu7fi4+Pl6emp5ORkhYaGurzGarUqJCREycnJkqTk5GRFRUW59AkLC3NuKyhszZo1SzNmzMjXvmPHDlMCZUllZmZq8MCb5VvVIqtXmrM9q2mIMmMnKLlmZaX7/h0uMxtla9zo7gquEiLv894uY9n97Wrbv62SkpJ04sSJy3YMAAAAQEWUkZFR7L6lClsnT57U7bffri1btshisejQoUOqV6+eRo0apSpVqpTZioSDBw92/rtly5Zq1aqV6tevry+++ELdu3cvk30UZOrUqS5f3Gyz2VS7dm21b99egYGBpu23uI4cOaInZ81TZK/7FVg9xNl+PDFJP6+cr/ceaKPmdas727cePKGFK3ary5RuCg9xvSTTdsqmg58d1PJFy/OFUgAAAACu8q56K45Sha1HHnlEXl5eOnr0qJo2bepsv+OOOzRhwgTTln+vV6+eqlWrpsOHD6t79+4KDw/PdzbGbrcrLS3NeZ9XeHi4UlJSXPrkPS/sXjAfHx/5+Pjka7darbJaL+lkYJnw8PCQ3W5XriHl/mONE7vDUE5OtjwMu6zKdbZbHHZl52QrV7lyWBwuY+UqV3a7XR4eHuXi2AAAAIDyrCSfmUu1GuGGDRv0/PPPq1atWi7tDRs2VFJSUmmGLJbff/9dJ0+eVI0aNSRJ0dHROn36tHbu3Onss3nzZjkcDnXs2NHZZ9u2bc6FPCQpLi5OjRs3LvASQgAAAAAoC6UKWxkZGfL398/XnpaWVuAZocKkp6crISFBCQkJkv66PC4hIUFHjx5Venq6Jk2apG+//Va//vqrNm3apAEDBqhBgwaKiYmRJDVt2lS9evXSfffdp+3bt+vrr7/WmDFjNHjwYEVEREiS7rzzTnl7e2vUqFHav3+/PvjgAy1cuNDlMkEAAAAAKGulClvXXXed3n77bedzi8Uih8OhOXPmqFu3bsUeZ8eOHWrbtq3atm0rSZowYYLatm2radOmydPTU3v27FH//v3VqFEjjRo1Su3atdOXX37pEuhWrFihJk2aqHv37urTp4+6dOni8h1aQUFB2rBhg44cOaJ27dpp4sSJmjZtGsu+AwAAADBVqW7SmTNnjrp3764dO3YoOztbkydP1v79+5WWlqavv/662OPccMMNMgyj0O3r16+/6BghISFauXJlkX1atWqlL7/8sth1AQAAAMClKtWZrRYtWujgwYPq0qWLBgwYoIyMDN16663atWuX6tevX9Y1AgAAAECFU+IzWzk5OerVq5eWLFmixx9/3IyaAAAAAKDCK/GZLS8vL+3Zs8eMWgAAAADgilGqywjvuusuvfnmm2VdCwAAAABcMUq1QIbdbtdbb72ljRs3ql27dqpUqZLL9vnz55dJcQAAAABQUZUobP3yyy+qW7eu9u3bp2uuuUaSdPDgQZc+Foul7KoDAAAAgAqqRGGrYcOGOn78uLZs2SJJuuOOO/TSSy8pLCzMlOIAAAAAoKIq0T1bF34n1ueff66MjIwyLQgAAAAArgSlWiAjT1FfSAwAAAAAV7MShS2LxZLvnizu0QIAAACA/Ep0z5ZhGBo5cqR8fHwkSefPn9eDDz6YbzXCjz76qOwqBAAAAIAKqERha8SIES7P77rrrjItBgAAAACuFCUKW0uXLjWrDgAAAAC4olzSAhkAAAAAgIIRtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMIFbw9a2bdvUr18/RUREyGKx6JNPPnHZbhiGpk2bpho1asjPz089evTQoUOHXPqkpaVp6NChCgwMVHBwsEaNGqX09HSXPnv27NF1110nX19f1a5dW3PmzDH70AAAAABc5dwatjIyMtS6dWstXry4wO1z5szRSy+9pCVLlui7775TpUqVFBMTo/Pnzzv7DB06VPv371dcXJxWr16tbdu26f7773dut9ls6tmzpyIjI7Vz507NnTtX06dP1+uvv2768QEAAAC4elndufPevXurd+/eBW4zDEMvvviinnjiCQ0YMECS9PbbbyssLEyffPKJBg8erB9//FHr1q3T999/r/bt20uSXn75ZfXp00cvvPCCIiIitGLFCmVnZ+utt96St7e3mjdvroSEBM2fP98llAEAAABAWXJr2CrKkSNHlJycrB49ejjbgoKC1LFjR8XHx2vw4MGKj49XcHCwM2hJUo8ePeTh4aHvvvtOAwcOVHx8vLp27Spvb29nn5iYGD3//PM6deqUqlSpkm/fWVlZysrKcj632WySJLvdLrvdbsbhlojD4ZDVapWnRfKUw9lu9bDIy8tbDotVdnk62w0Pq7y9vOUpT3kYriczPeUpq9Uqh8NRLo4NAAAAKM9K8pm53Iat5ORkSVJYWJhLe1hYmHNbcnKyQkNDXbZbrVaFhIS49ImKiso3Rt62gsLWrFmzNGPGjHztO3bsUKVKlUp5RGUnMzNTgwfeLN+qFlm90pztWU1DlBk7Qck1Kyvd9+9wmdkoW+NGd1dwlRB5n/d2Gcvub1fb/m2VlJSkEydOXLZjAAAAACqijIyMYvctt2HLnaZOnaoJEyY4n9tsNtWuXVvt27dXYGCgGyv7y5EjR/TkrHmK7HW/AquHONuPJybp55Xz9d4DbdS8bnVn+9aDJ7RwxW51mdJN4SHhLmPZTtl08LODWr5oeb5QCgAAAMBV3lVvxVFuw1Z4+F+hICUlRTVq1HC2p6SkqE2bNs4+F56NsdvtSktLc74+PDxcKSkpLn3ynuf1uZCPj498fHzytVutVlmt7p8yDw8P2e125RpS7j/WOLE7DOXkZMvDsMuqXGe7xWFXdk62cpUrh8XhMlaucmW32+Xh4VEujg0AAAAoz0rymbncfs9WVFSUwsPDtWnTJmebzWbTd999p+joaElSdHS0Tp8+rZ07dzr7bN68WQ6HQx07dnT22bZtm3Jycpx94uLi1Lhx4wIvIQQAAACAsuDWsJWenq6EhAQlJCRI+uvyuISEBB09elQWi0Xjx4/XM888o88++0x79+7V8OHDFRERoVtuuUWS1LRpU/Xq1Uv33Xeftm/frq+//lpjxozR4MGDFRERIUm688475e3trVGjRmn//v364IMPtHDhQpfLBAEAAACgrLn1urEdO3aoW7duzud5AWjEiBFatmyZJk+erIyMDN1///06ffq0unTponXr1snX19f5mhUrVmjMmDHq3r27PDw8NGjQIL300kvO7UFBQdqwYYNiY2PVrl07VatWTdOmTWPZdwAAAACmcmvYuuGGG2QYRqHbLRaLnn76aT399NOF9gkJCdHKlSuL3E+rVq305ZdflrpOAAAAACipcnvPFgAAAABUZIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAAAAATEDYAgAAAAATELYAAAAAwASELQAAAAAwAWELAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABOU67A1ffp0WSwWl0eTJk2c28+fP6/Y2FhVrVpVAQEBGjRokFJSUlzGOHr0qPr27St/f3+FhoZq0qRJstvtl/tQAAAAAFxlrO4u4GKaN2+ujRs3Op9brX+X/Mgjj2jNmjVatWqVgoKCNGbMGN166636+uuvJUm5ubnq27evwsPD9c033+j48eMaPny4vLy89Nxzz132YwEAAABw9Sj3YctqtSo8PDxf+5kzZ/Tmm29q5cqVuvHGGyVJS5cuVdOmTfXtt9+qU6dO2rBhgw4cOKCNGzcqLCxMbdq00cyZMzVlyhRNnz5d3t7el/twAAAAAFwlyn3YOnTokCIiIuTr66vo6GjNmjVLderU0c6dO5WTk6MePXo4+zZp0kR16tRRfHy8OnXqpPj4eLVs2VJhYWHOPjExMRo9erT279+vtm3bFrjPrKwsZWVlOZ/bbDZJkt1uLxeXIDocDlmtVnlaJE85nO1WD4u8vLzlsFhll6ez3fCwytvLW57ylIfheuWopzxltVrlcDjKxbEBAAAA5VlJPjOX67DVsWNHLVu2TI0bN9bx48c1Y8YMXXfdddq3b5+Sk5Pl7e2t4OBgl9eEhYUpOTlZkpScnOwStPK2520rzKxZszRjxox87Tt27FClSpUu8aguXWZmpgYPvFm+VS2yeqU527OahigzdoKSa1ZWuu/fZ+0yG2Vr3OjuCq4SIu/zrmfz7P52te3fVklJSTpx4sRlOwYAAACgIsrIyCh233Idtnr37u38d6tWrdSxY0dFRkbqww8/lJ+fn2n7nTp1qiZMmOB8brPZVLt2bbVv316BgYGm7be4jhw5oidnzVNkr/sVWD3E2X48MUk/r5yv9x5oo+Z1qzvbtx48oYUrdqvLlG4KD3G9JNN2yqaDnx3U8kXLFRUVVeqa/vzzT509ezZfe+XKlVWtWrVSjwsAAACUJ3lXvRVHuQ5bFwoODlajRo10+PBh3XTTTcrOztbp06ddzm6lpKQ47/EKDw/X9u3bXcbIW62woPvA8vj4+MjHxydfu9VqdVmgw108PDxkt9uVa0i5/1hQ0u4wlJOTLQ/DLqtyne0Wh13ZOdnKVa4cFofLWLnKld1ul4eHR6mPLTU1VSPvH6m0jLR820IqheidN99R9erVC3glAAAAULGU5DNzuV76/ULp6en6+eefVaNGDbVr105eXl7atGmTc3tiYqKOHj2q6OhoSVJ0dLT27t3rcnlcXFycAgMD1axZs8te/5XKZrMpLSNNNfvUVOORjZ2Pmn1qKi0jrUTpHwAAALhSuP80TREeffRR9evXT5GRkTp27JieeuopeXp6asiQIQoKCtKoUaM0YcIEhYSEKDAwUA8//LCio6PVqVMnSVLPnj3VrFkzDRs2THPmzFFycrKeeOIJxcbGFnjmCpcmIDRAQRFB7i4DAAAAKBfKddj6/fffNWTIEJ08eVLVq1dXly5d9O233zovSVuwYIE8PDw0aNAgZWVlKSYmRq+88orz9Z6enlq9erVGjx6t6OhoVapUSSNGjNDTTz/trkMCAAAAcJUo12Hr/fffL3K7r6+vFi9erMWLFxfaJzIyUmvXri3r0gAAAACgSBXqni0AAAAAqCgIWwAAAABgAsIWAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmMDq7gJQ/qSmpspmsxW4LTAwUNWrV7/MFQEAAAAVD2ELLlJTU3XX3fcq7ey5AreHVPbXu0vfIHABAAAAF0HYggubzaa0s+dUPXqQKoWEuWzLSEvRsa3vae/evYqMjHS2JyUlKdeee7lLBQAAAMo1whYKVCkkTIGhtVzastLP6OSRfZr9+Dj5eHs72zMys3T0xDE1yGlwucsEAAAAyi3CFootJytTvh52PdI5QI1qhjjbvz18UuM+y5U9x+7G6gAAAIDyhbCFEqtVxVf1wwKcz5P+zHBjNQAAAED5xNLvAAAAAGACwhYAAAAAmICwBQAAAAAm4J4tVAiFfdEyX7IMAACA8oqwhXIvNTVVw0YNU1pGWr5tIZVC9M6b7xC4AAAAUO4QtlDu2Ww2pWWkqWafmgoI/XsVxPQT6fpj7R+y2WyELQAAAJQ7hC1UGAGhAQqKCHJ3GQAAAECxsEAGAAAAAJiAsAUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmMDq7gJw9UlNTZXNZsvXHhgYqOrVq7uhIgAAAKDsEbZwWaWmpuquu+9V2tlz+baFVPbXu0vfIHABAADgikDYwmVls9mUdvacqkcPUqWQMGd7RlqKUuP/TzabjbAFAACAKwJhC25RKSRMgaG1XNqOZWcrKSkpX9+kpCTl2nMvV2kAAABAmSBsoVzISj+jk0f2afbj4+Tj7e2yLSMzS0dPHFODnAam7Jt7yAAAAGAGwhbKhZysTPl62PVI5wA1qhnisu3bwyc17rNc2XPsZb7f1NRUDRs1TGkZafm2hVQK0TtvvkPgAgAAQKkQtlCu1Kriq/phAS5tSX9mmLY/m82mtIw01exTUwGhf+83/US6/lj7B/eQAQAAoNQIW7jilOaywIDQAAVFBJldGgAAAK4ihC1cUVhaHgAAAOUFYQtXFJaWBwAAQHlB2MIVqaCl5VPdVAsAAACuToQtXDVyCvgeL77DCwAAAGYhbOGqUNj3eJn9HV4AAAC4ehG2cFUo7Hu8zPwOLwAAAFzdCFu4qlz4PV4l/Q6vwpaVl4peWh4AAABXH8IWUExFLSsvFb60fGm+9wsAAAAVH2ELKKbClpWXCl9aPjU1VcNGDVNaRlq+8UIqheidN98hcAEAAFyhCFtACRW0rLxU8NLyNptNaRlpqtmnpgJC/758Mf1Euv5Y+wff+wUAAHAFI2wBl0FAaICCIoLcXQYAAAAuI8IWUEbK4nu8Snp/Fwt2AAAAlF+ELaAMlMX3eBW1AEdBi28UdT+YxD1hAAAA7kbYAspAWXyPV2ELcGSkpejY1ve0d+9eRUZGOtuTkpKUeiZVdfrXcbkfTCq7e8JYSREAAKD0CFtAGbrU7/GS8i/AcdGzZlUamHI/GCspAgAAXBrCFlDOlcVZs9Iobyspcpat4uC9AgDgL4QtoIIoi7NmhSnow3He4h5mrqRY3A/lJb2fraixCxq/PKqogeVynBEtL3NT0X/GAADmI2wBV7nU1FSNvnuoss6edGkvyeIepd1vcT+Ul/R+tpMnT+qxaY8pPSe9wH2Xl8sgC/uwXlT9ZVG7mSHB7DOi5eXy1qL+ACAV/kcAAMDVhbAFXCUK+4CdlJSkjFMpmtItRLWr+jvby+NliiW5n23viWPqMDpaIREhLmO44zLIgub+5MmTeubJyTLOn83Xv7D6C6u9JGd6LtcqlmadEb0cYa44QbSwPwBIf/0RIDX+//jScgDA1RW2Fi9erLlz5yo5OVmtW7fWyy+/rA4dOri7LMB0Rf0V/nzmOdmPHVVoYOglXaZY2IfU7Oxsef8jCDnHL4PLFC92P5tvsG+xxi7NmZ5LvQTyr3k/oIV3NVf9MNcaS1J/Ue9tgLennn/2aVWtWtXZVparWBZ1+WlxlfaSwOL+3JgdRC/8A4BzrItWBgC4Glw1YeuDDz7QhAkTtGTJEnXs2FEvvviiYmJilJiYqNDQUHeXB5iqqL/Cn/h5n479vk+5l3AGq7AP/DnZ2bL99qMaR9aQ1er6n5uyvEzxUu5nu9jlYAUFlpJc5lfY3OfNe43KXi61l7T+wsZP+/2wDn44W0+MG3VJq1gWdaljQWfmyuq75Qqad6lkYc6dQbSomkoS/koaRMvifrbyck9cWeL+OgDuctWErfnz5+u+++7T3XffLUlasmSJ1qxZo7feekuPPfaYm6sDLo+C/gqffjL5ksctKlBkJCVobLS/y5knyfzLFIurqCBaVGApyWV+Uv65L4t5v9j4l7qK5cXPiOY/M1cW3y1X2LxLJQtzZgfRouRkZyspKcmlrSQhvTRBtKT3+pX08lZ5V9aTz87Jt9+SBMXCznQXte1Sx68o93DCfCX9uSzq55WQXnxF/bGjpHNfEef9qghb2dnZ2rlzp6ZOneps8/DwUI8ePRQfH5+vf1ZWlrKyspzPz5w5I0lKS0uT3e7eD4aSnD+w6Sm/ypH19y/ic3/+IQ9PqxJPnJfd8+8f6p//PC+rp1Xpf6QrzeJ6eUzG//sLus1mU1paWqFjl2b8C8cuTe1mj18WY7trbsrj+2rkZLqOb8+Sh6dVGTmSLcvhul+7ytXc5Ktdkj3jjPy9pP6NvRRR9e8zHT/9YejACYvsGXblZv59lsXx/46xPP3MXzj3hc17QeP//vvvSjt7Tv71O8i3crBL31PHf9WZ4z/JlplbrPFL8nNT2LxLf8/96V9Py9PuWabjFzZ2QeMX9f+p08eO6PTRnzTr8Ufk7e3lbM/MzNaPJ5PVuF8zBYT8vd8sW5bSdv01356ef+23sLm3pR7Xka0r9Pi4+1zGLun4J0+eVOy4CTqVcd5ljOzMc7KnHFRs90iFV6nkbP81NUPztuzTgxMelNXH9aNDFb8qWrRgkTOEFTZ2bna2bMcOqUGtMHlaXec3O9uuH5NTFF6npjysHmU6fmHzUtjcnD592vl7/1IFBQUpODjYpa0ij1+Ray/pz01RP5NS/p/Lijw3Zo5f2LxLpZv7C+fdXZy/Xwzjon0tRnF6VXDHjh1TzZo19c033yg6OtrZPnnyZG3dulXfffedS//p06drxowZl7tMAAAAABXEb7/9plq18t+3+09XxZmtkpo6daomTJjgfO5wOJSWlqaqVavKYrG4pSabzabatWvrt99+U2BgoFtquJox/+7F/LsX8+9ezL97Mf/uw9y7F/NfOMMwdPbsWUVERFy071URtqpVqyZPT0+lpKS4tKekpCg8PDxffx8fH/n4+Li0XXi61V0CAwP5gXcj5t+9mH/3Yv7di/l3L+bffZh792L+CxYUVLx7e/NfhHoF8vb2Vrt27bRp0yZnm8Ph0KZNm1wuKwQAAACAsnJVnNmSpAkTJmjEiBFq3769OnTooBdffFEZGRnO1QkBAAAAoCxdNWHrjjvuUGpqqqZNm6bk5GS1adNG69atU1hY2MVfXA74+Pjoqaeeynd5Iy4P5t+9mH/3Yv7di/l3L+bffZh792L+y8ZVsRohAAAAAFxuV8U9WwAAAABwuRG2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNiqIBYvXqy6devK19dXHTt21Pbt291dUrk2a9YsXXvttapcubJCQ0N1yy23KDEx0aXP+fPnFRsbq6pVqyogIECDBg3K98XXR48eVd++feXv76/Q0FBNmjRJdrvdpc8XX3yha665Rj4+PmrQoIGWLVuWr56r/f2bPXu2LBaLxo8f72xj/s31xx9/6K677lLVqlXl5+enli1baseOHc7thmFo2rRpqlGjhvz8/NSjRw8dOnTIZYy0tDQNHTpUgYGBCg4O1qhRo5Senu7SZ8+ePbruuuvk6+ur2rVra86cOflqWbVqlZo0aSJfX1+1bNlSa9euNeegy4nc3Fw9+eSTioqKkp+fn+rXr6+ZM2fqn+tRMf9lZ9u2berXr58iIiJksVj0ySefuGwvT3NdnFoqmqLmPycnR1OmTFHLli1VqVIlRUREaPjw4Tp27JjLGMx/6V3s5/+fHnzwQVksFr344osu7cy/yQyUe++//77h7e1tvPXWW8b+/fuN++67zwgODjZSUlLcXVq5FRMTYyxdutTYt2+fkZCQYPTp08eoU6eOkZ6e7uzz4IMPGrVr1zY2bdpk7Nixw+jUqZPxr3/9y7ndbrcbLVq0MHr06GHs2rXLWLt2rVGtWjVj6tSpzj6//PKL4e/vb0yYMME4cOCA8fLLLxuenp7GunXrnH2u9vdv+/btRt26dY1WrVoZ48aNc7Yz/+ZJS0szIiMjjZEjRxrfffed8csvvxjr1683Dh8+7Owze/ZsIygoyPjkk0+M3bt3G/379zeioqKMzMxMZ59evXoZrVu3Nr799lvjyy+/NBo0aGAMGTLEuf3MmTNGWFiYMXToUGPfvn3Ge++9Z/j5+Rmvvfaas8/XX39teHp6GnPmzDEOHDhgPPHEE4aXl5exd+/eyzMZbvDss88aVatWNVavXm0cOXLEWLVqlREQEGAsXLjQ2Yf5Lztr1641Hn/8ceOjjz4yJBkff/yxy/byNNfFqaWiKWr+T58+bfTo0cP44IMPjJ9++smIj483OnToYLRr185lDOa/9C7285/no48+Mlq3bm1EREQYCxYscNnG/JuLsFUBdOjQwYiNjXU+z83NNSIiIoxZs2a5saqK5cSJE4YkY+vWrYZh/PULwMvLy1i1apWzz48//mhIMuLj4w3D+Os/YB4eHkZycrKzz6uvvmoEBgYaWVlZhmEYxuTJk43mzZu77OuOO+4wYmJinM+v5vfv7NmzRsOGDY24uDjj+uuvd4Yt5t9cU6ZMMbp06VLodofDYYSHhxtz5851tp0+fdrw8fEx3nvvPcMwDOPAgQOGJOP777939vn8888Ni8Vi/PHHH4ZhGMYrr7xiVKlSxfl+5O27cePGzue333670bdvX5f9d+zY0XjggQcu7SDLsb59+xr33HOPS9utt95qDB061DAM5t9MF37YLE9zXZxaKrqiPuzn2b59uyHJSEpKMgyD+S9Lhc3/77//btSsWdPYt2+fERkZ6RK2mH/zcRlhOZedna2dO3eqR48ezjYPDw/16NFD8fHxbqysYjlz5owkKSQkRJK0c+dO5eTkuMxrkyZNVKdOHee8xsfHq2XLli5ffB0TEyObzab9+/c7+/xzjLw+eWNc7e9fbGys+vbtm2+OmH9zffbZZ2rfvr3+/e9/KzQ0VG3bttX/9//9f87tR44cUXJyssu8BAUFqWPHji7zHxwcrPbt2zv79OjRQx4eHvruu++cfbp27Spvb29nn5iYGCUmJurUqVPOPkW9R1eif/3rX9q0aZMOHjwoSdq9e7e++uor9e7dWxLzfzmVp7kuTi1XgzNnzshisSg4OFgS8282h8OhYcOGadKkSWrevHm+7cy/+Qhb5dyff/6p3Nxclw+ckhQWFqbk5GQ3VVWxOBwOjR8/Xp07d1aLFi0kScnJyfL29nb+xz7PP+c1OTm5wHnP21ZUH5vNpszMzKv6/Xv//ff1ww8/aNasWfm2Mf/m+uWXX/Tqq6+qYcOGWr9+vUaPHq2xY8dq+fLlkv6ev6LmJTk5WaGhoS7brVarQkJCyuQ9upLn/7HHHtPgwYPVpEkTeXl5qW3btho/fryGDh0qifm/nMrTXBenlivd+fPnNWXKFA0ZMkSBgYGSmH+zPf/887JarRo7dmyB25l/81ndXQBgttjYWO3bt09fffWVu0u5avz2228aN26c4uLi5Ovr6+5yrjoOh0Pt27fXc889J0lq27at9u3bpyVLlmjEiBFuru7K9+GHH2rFihVauXKlmjdvroSEBI0fP14RERHMP65aOTk5uv3222UYhl599VV3l3NV2LlzpxYuXKgffvhBFovF3eVctTizVc5Vq1ZNnp6e+VZpS0lJUXh4uJuqqjjGjBmj1atXa8uWLapVq5azPTw8XNnZ2Tp9+rRL/3/Oa3h4eIHznretqD6BgYHy8/O7at+/nTt36sSJE7rmmmtktVpltVq1detWvfTSS7JarQoLC2P+TVSjRg01a9bMpa1p06Y6evSopL/nr6h5CQ8P14kTJ1y22+12paWllcl7dCXP/6RJk5xnt1q2bKlhw4bpkUcecZ7lZf4vn/I018Wp5UqVF7SSkpIUFxfnPKslMf9m+vLLL3XixAnVqVPH+bs4KSlJEydOVN26dSUx/5cDYauc8/b2Vrt27bRp0yZnm8Ph0KZNmxQdHe3Gyso3wzA0ZswYffzxx9q8ebOioqJctrdr105eXl4u85qYmKijR4865zU6Olp79+51+Y9Q3i+JvA+y0dHRLmPk9ckb42p9/7p37669e/cqISHB+Wjfvr2GDh3q/Dfzb57OnTvn+6qDgwcPKjIyUpIUFRWl8PBwl3mx2Wz67rvvXOb/9OnT2rlzp7PP5s2b5XA41LFjR2efbdu2KScnx9knLi5OjRs3VpUqVZx9inqPrkTnzp2Th4frr1dPT085HA5JzP/lVJ7muji1XInygtahQ4e0ceNGVa1a1WU782+eYcOGac+ePS6/iyMiIjRp0iStX79eEvN/Wbh7hQ5c3Pvvv2/4+PgYy5YtMw4cOGDcf//9RnBwsMsqbXA1evRoIygoyPjiiy+M48ePOx/nzp1z9nnwwQeNOnXqGJs3bzZ27NhhREdHG9HR0c7teUuP9+zZ00hISDDWrVtnVK9evcClxydNmmT8+OOPxuLFiwtcepz3z3BZjdAwmH8zbd++3bBarcazzz5rHDp0yFixYoXh7+9vvPvuu84+s2fPNoKDg41PP/3U2LNnjzFgwIACl8Nu27at8d133xlfffWV0bBhQ5flgE+fPm2EhYUZw4YNM/bt22e8//77hr+/f77lgK1Wq/HCCy8YP/74o/HUU09dcUuPX2jEiBFGzZo1nUu/f/TRR0a1atWMyZMnO/sw/2Xn7Nmzxq5du4xdu3YZkoz58+cbu3btcq52V57muji1VDRFzX92drbRv39/o1atWkZCQoLL7+N/rmzH/JfexX7+L3ThaoSGwfybjbBVQbz88stGnTp1DG9vb6NDhw7Gt99+6+6SyjVJBT6WLl3q7JOZmWk89NBDRpUqVQx/f39j4MCBxvHjx13G+fXXX43evXsbfn5+RrVq1YyJEycaOTk5Ln22bNlitGnTxvD29jbq1avnso88vH/5wxbzb67//e9/RosWLQwfHx+jSZMmxuuvv+6y3eFwGE8++aQRFhZm+Pj4GN27dzcSExNd+pw8edIYMmSIERAQYAQGBhp33323cfbsWZc+u3fvNrp06WL4+PgYNWvWNGbPnp2vlg8//NBo1KiR4e3tbTRv3txYs2ZN2R9wOWKz2Yxx48YZderUMXx9fY169eoZjz/+uMuHS+a/7GzZsqXA/96PGDHCMIzyNdfFqaWiKWr+jxw5Uujv4y1btjjHYP5L72I//xcqKGwx/+ayGMY/vtIeAAAAAFAmuGcLAAAAAExA2AIAAAAAExC2AAAAAMAEhC0AAAAAMAFhCwAAAABMQNgCAAAAABMQtgAAAADABIQtAAAAADABYQsAUKH9+uuvslgsSkhIcHcpAAC4IGwBANzOYrEU+Zg+fbq7SyzQ4cOHdffdd6tWrVry8fFRVFSUhgwZoh07dlzWOgicAFA+Wd1dAAAAx48fd/77gw8+0LRp05SYmOhsCwgIcEdZRdqxY4e6d++uFi1a6LXXXlOTJk109uxZffrpp5o4caK2bt3q7hIBAG7GmS0AgNuFh4c7H0FBQbJYLM7noaGhmj9/vvPsUZs2bbRu3bpCx8rNzdU999yjJk2a6OjRo5KkTz/9VNdcc418fX1Vr149zZgxQ3a73fkai8WiN954QwMHDpS/v78aNmyozz77rNB9GIahkSNHqmHDhvryyy/Vt29f1a9fX23atNFTTz2lTz/91Nl37969uvHGG+Xn56eqVavq/vvvV3p6unP7DTfcoPHjx7uMf8stt2jkyJHO53Xr1tVzzz2ne+65R5UrV1adOnX0+uuvO7dHRUVJktq2bSuLxaIbbrihyPkGAFwehC0AQLm2cOFCzZs3Ty+88IL27NmjmJgY9e/fX4cOHcrXNysrS//+97+VkJCgL7/8UnXq1NGXX36p4cOHa9y4cTpw4IBee+01LVu2TM8++6zLa2fMmKHbb79de/bsUZ8+fTR06FClpaUVWFNCQoL279+viRMnysMj/6/S4OBgSVJGRoZiYmJUpUoVff/991q1apU2btyoMWPGlHge5s2bp/bt22vXrl166KGHNHr0aOfZv+3bt0uSNm7cqOPHj+ujjz4q8fgAgLJH2AIAlGsvvPCCpkyZosGDB6tx48Z6/vnn1aZNG7344osu/dLT09W3b1+lpqZqy5Ytql69uqS/QtRjjz2mESNGqF69errppps0c+ZMvfbaay6vHzlypIYMGaIGDRroueeeU3p6ujPEXCgv6DVp0qTI2leuXKnz58/r7bffVosWLXTjjTdq0aJFeuedd5SSklKieejTp48eeughNWjQQFOmTFG1atW0ZcsWSXIea9WqVRUeHq6QkJASjQ0AMAf3bAEAyi2bzaZjx46pc+fOLu2dO3fW7t27XdqGDBmiWrVqafPmzfLz83O27969W19//bXLmazc3FydP39e586dk7+/vySpVatWzu2VKlVSYGCgTpw4UWBdhmEUq/4ff/xRrVu3VqVKlVxqdzgcSkxMVFhYWLHGubC+vMssC6sPAFA+cGYLAHBF6NOnj/bs2aP4+HiX9vT0dM2YMUMJCQnOx969e3Xo0CH5+vo6+3l5ebm8zmKxyOFwFLivRo0aSZJ++umnS67bw8MjX3jLycnJ168k9QEAygfCFgCg3AoMDFRERIS+/vprl/avv/5azZo1c2kbPXq0Zs+erf79+7usBHjNNdcoMTFRDRo0yPco6H6r4mjTpo2aNWumefPmFRh4Tp8+LUlq2rSpdu/erYyMDJfaPTw81LhxY0l/XQL4z9UYc3NztW/fvhLV4+3t7XwtAKD8IGwBAMq1SZMm6fnnn9cHH3ygxMREPfbYY0pISNC4cePy9X344Yf1zDPP6Oabb9ZXX30lSZo2bZrefvttzZgxQ/v379ePP/6o999/X0888USpa7JYLFq6dKkOHjyo6667TmvXrtUvv/yiPXv26Nlnn9WAAQMkSUOHDpWvr69GjBihffv2acuWLXr44Yc1bNgw5yWEN954o9asWaM1a9bop59+0ujRo51hrbhCQ0Pl5+endevWKSUlRWfOnCn1sQEAyg5hCwBQro0dO1YTJkzQxIkT1bJlS61bt06fffaZGjZsWGD/8ePHa8aMGerTp4+++eYbxcTEaPXq1dqwYYOuvfZaderUSQsWLFBkZOQl1dWhQwft2LFDDRo00H333aemTZuqf//+2r9/v3PxDn9/f61fv15paWm69tprddttt6l79+5atGiRc5x77rlHI0aM0PDhw3X99derXr166tatW4lqsVqteumll/Taa68pIiLCGfYAAO5lMYp7ly8AAAAAoNg4swUAAAAAJiBsAQAAAIAJCFsAAAAAYALCFgAAAACYgLAFAAAAACYgbAEAAACACQhbAAAAAGACwhYAAAAAmICwBQAAAAAmIGwBAAAAgAkIWwAAAABggv8fWCOF0UtWgdsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# # Display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens in all context: [14621, 4340, 311, 653, 330, 721, 65489, 287, 1, 56370, 760, 11162, 99, 250, 30543, 146450, 22463, 18837, 34583, 35134, 311, 1887, 2213, 12292, 601, 518, 220, 22145, 25, 576, 20713, 15235, 14872, 553, 22463, 18837, 389, 3217, 220, 16, 18, 609, 220, 16, 19, 304, 5836, 12879, 0, 1072, 14412, 804, 7082, 17207, 7661, 52984, 10607, 15919, 1454, 5785, 26223, 41885, 26223, 11212, 26223, 18837, 26538, 26223, 18837, 12162, 14, 9951, 85, 15, 13, 18, 85, 15, 13, 18, 85, 15, 13, 17, 85, 15, 13, 16, 145653, 5890, 37155, 51, 54927, 11066, 264, 15846, 21806, 287, 3766, 916, 264, 12165, 9994, 51, 54927, 11066, 264, 4285, 444, 10994, 3766, 448, 6236, 4119, 323, 9934, 19911, 11066, 264, 12853, 6331, 11066, 264, 19470, 831, 4928, 26980, 23470, 320, 49, 1890, 8, 1845, 25, 3660, 220, 17, 11066, 458, 94506, 28525, 11066, 458, 20713, 5668, 3173, 11066, 264, 19470, 831, 4928, 26980, 23470, 320, 49, 1890, 8, 1845, 25, 3660, 220, 16, 11066, 264, 41733, 2711, 4712, 11066, 264, 15846, 14, 16141, 287, 1849, 916, 7870, 821, 9190, 5612, 551, 2918, 4340, 4686, 27193, 4340, 4686, 27193, 4340, 311, 990, 7375, 304, 264, 8781, 4340, 311, 990, 264, 4621, 4314, 438, 264, 10759, 423, 4340, 311, 912, 4938, 311, 6236, 61905, 4340, 311, 990, 3110, 56037, 4340, 311, 912, 264, 41733, 6193, 916, 4771, 4625, 4340, 311, 19873, 1598, 77, 4788, 304, 15279, 4340, 311, 4269, 6236, 1614, 14507, 4340, 311, 912, 1638, 28696, 2827, 311, 264, 22109, 4340, 311, 912, 56370, 311, 6236, 61905, 4340, 311, 990, 2421, 6552, 10295, 304, 6236, 4119, 4340, 311, 653, 5392, 70643, 8098, 4340, 311, 4582, 22463, 18837, 14185, 4340, 311, 912, 10295, 311, 279, 9934, 369, 3239, 6358, 4340, 311, 990, 2421, 6552, 10295, 4340, 311, 1598, 2526, 5746, 4340, 311, 990, 2550, 87073, 311, 4715, 458, 444, 10994, 2033, 1119, 32930, 3561, 4340, 311, 3705, 5048, 1380, 902, 19556, 525, 7907, 4340, 311, 6021, 1948, 1186, 11582, 1735, 4340, 311, 470, 32930, 821, 504, 264, 1614, 4340, 311, 62079, 1467, 1526, 15279, 2022, 4340, 311, 62079, 1467, 1526, 86875, 72913, 4340, 311, 62079, 1467, 304, 264, 3175, 444, 10994, 1618, 4340, 311, 990, 5392, 89417, 4340, 311, 912, 993, 2832, 509, 5392, 8098, 22302, 311, 444, 10994, 82, 323, 12853, 26874, 11066, 458, 20713, 448, 20713, 25255, 320, 77415, 8, 4340, 311, 9245, 6540, 38999, 4340, 311, 25244, 3561, 9934, 19911, 4340, 311, 3705, 5248, 19556, 979, 3730, 3239, 6358, 4340, 311, 990, 5798, 3419, 7375, 323, 5392, 89417, 4340, 311, 1494, 1526, 5977, 504, 825, 3019, 311, 279, 1790, 4340, 311, 30335, 50932, 3786, 4340, 311, 3705, 5248, 10759, 3004, 979, 3730, 3239, 6358, 4340, 311, 912, 2750, 311, 264, 8781, 594, 1584, 4340, 311, 9245, 13406, 369, 3239, 6358, 4340, 311, 14411, 15592, 8781, 91025, 4340, 3484, 448, 1550, 55880, 487, 22049, 52603, 979, 3730, 3239, 6358, 10268, 11789, 27811, 4340, 311, 990, 279, 17439, 2859, 12020, 461, 2054, 4340, 311, 912, 12205, 311, 10759, 423, 3059, 34, 11829, 4340, 311, 990, 26679, 304, 3312, 21737, 4340, 311, 15498, 26679, 311, 264, 78679, 4340, 311, 57414, 26679, 220, 4692, 4340, 311, 6845, 2526, 4822, 4357, 4340, 311, 1494, 26679, 304, 518, 15592, 4340, 311, 6718, 553, 3668, 4340, 311, 6500, 6236, 1614, 14507, 4340, 311, 3705, 4379, 13388, 4340, 311, 2930, 894, 1614, 304, 825, 1555, 4340, 311, 3754, 3950, 10431, 304, 12853, 16969, 4340, 311, 912, 7375, 311, 6236, 61905, 4340, 311, 6718, 2038, 4340, 311, 653, 56370, 448, 65151, 25111, 4340, 311, 5508, 6452, 77, 4788, 311, 13852, 4340, 311, 1855, 2526, 4822, 24083, 4340, 311, 1855, 264, 2526, 6236, 1614, 536, 10268, 37068, 24602, 4340, 311, 1855, 264, 2526, 444, 10994, 536, 10268, 10392, 461, 2054, 4340, 311, 1855, 7375, 4340, 311, 7390, 697, 444, 10994, 10500, 4340, 311, 2795, 27445, 82, 4340, 311, 2795, 9293, 504, 264, 6220, 4340, 311, 2795, 9308, 4340, 311, 2795, 4718, 4340, 311, 2795, 73192, 4340, 311, 2795, 5100, 8246, 3542, 4340, 311, 2795, 11358, 82, 4340, 311, 2795, 3482, 6816, 4340, 311, 1855, 264, 8741, 320, 721, 12, 7596, 287, 8, 8781, 1178, 39088, 4119, 4340, 311, 15963, 3059, 504, 5248, 10759, 3004, 4340, 311, 3293, 10295, 504, 264, 22463, 41885, 10337, 4340, 311, 3293, 10295, 553, 3084, 4340, 311, 3293, 10295, 553, 53129, 31773, 40861, 320, 8035, 49, 8, 4340, 311, 3293, 10295, 553, 308, 12, 1520, 27248, 4340, 311, 3293, 10295, 553, 37623, 4340, 311, 990, 5785, 10295, 979, 3730, 32189, 4340, 311, 3705, 1293, 1467, 979, 3730, 32189, 4340, 311, 990, 49645, 7484, 320, 2152, 5392, 8098, 8, 311, 653, 32189, 4340, 311, 912, 32772, 82, 311, 264, 78679, 4340, 311, 4051, 6605, 30816, 16223, 7542, 4340, 311, 990, 279, 22463, 18837, 51980, 5333, 4340, 311, 24085, 1598, 77, 4788, 26223, 18837, 16378, 11434, 8436, 1862, 3674, 4340, 311, 6500, 444, 10994, 14507, 4340, 311, 3754, 3950, 10431, 369, 444, 10994, 82, 6727, 4119, 23490, 4340, 311, 633, 1487, 48216, 4340, 311, 83184, 30403, 3059, 311, 49360, 279, 330, 54337, 304, 279, 6149, 1, 2456, 4340, 311, 6718, 73192, 553, 21426, 4340, 311, 10880, 23921, 6605, 315, 279, 1852, 943, 4340, 311, 912, 1943, 3840, 4340, 311, 44566, 504, 19588, 22463, 18837, 13009, 311, 22463, 11212, 4340, 311, 17179, 1667, 5248, 22879, 817, 2197, 4340, 311, 1494, 79049, 57597, 821, 5961, 311, 4119, 4340, 311, 990, 79049, 57597, 50932, 4340, 311, 1855, 264, 2526, 9258, 21102, 4340, 311, 990, 279, 2550, 70913, 287, 6729, 4340, 311, 4715, 4718, 2550, 4340, 311, 22683, 979, 264, 22314, 1465, 13666, 4340, 311, 4715, 1467, 504, 1943, 6171, 4340, 311, 4715, 11874, 2550, 4340, 311, 4715, 53127, 2550, 4340, 311, 990, 279, 17022, 11789, 10392, 461, 2054, 4340, 311, 990, 22463, 18837, 448, 2155, 5355, 67, 8159, 10795, 4340, 311, 912, 6236, 3840, 4340, 311, 633, 264, 431, 1890, 3766, 311, 912, 51846, 4340, 311, 653, 817, 8694, 56370, 4340, 311, 633, 697, 431, 1890, 3766, 311, 470, 8173, 4340, 311, 4269, 3059, 504, 697, 431, 1890, 3766, 4340, 311, 6718, 4718, 821, 4340, 311, 52847, 6718, 1467, 553, 5766, 2582, 11160, 4340, 311, 1494, 15592, 23594, 311, 1598, 77, 4788, 4340, 311, 653, 330, 721, 65489, 287, 1, 56370, 4340, 311, 6718, 1467, 3118, 389, 41733, 37623, 4340, 311, 8781, 1598, 77, 4788, 4340, 311, 3581, 323, 2795, 22463, 18837, 6171, 4340, 311, 6718, 1467, 553, 11211, 4340, 311, 6718, 9308, 4340, 311, 653, 3405, 35764, 916, 27445, 82, 4340, 311, 3484, 448, 3460, 31806, 979, 3730, 7870, 3405, 12, 596, 86, 4671, 4340, 311, 2664, 9934, 979, 3730, 7870, 3405, 12, 596, 86, 4671, 4340, 311, 653, 3239, 10519, 438, 949, 315, 7870, 3405, 12, 596, 86, 4671, 4340, 311, 4269, 1598, 77, 4788, 4340, 311, 4269, 14507, 504, 458, 444, 10994, 4340, 311, 990, 264, 882, 12635, 291, 4621, 3553, 10759, 423, 4340, 311, 470, 35036, 504, 264, 5392, 4340, 311, 990, 6236, 4119, 311, 1618, 7375, 4340, 311, 11156, 15279, 5392, 8098, 4340, 311, 5344, 4119, 311, 1618, 264, 5392, 4340, 311, 2615, 279, 22109, 2648, 504, 264, 5392, 4340, 311, 1494, 5392, 16275, 311, 6236, 4119, 4340, 311, 1494, 1598, 882, 2750, 311, 7375, 4340, 311, 4269, 4357, 504, 264, 5392, 4340, 311, 4269, 5392, 6738, 4340, 311, 5508, 7375, 311, 5264, 15469, 23550, 4340, 311, 3705, 5392, 5975, 4340, 311, 990, 2421, 63530, 49645, 448, 5392, 8098, 4340, 311, 912, 264, 3738, 3419, 10603, 60666, 369, 7375, 4340, 311, 10719, 1614, 18906, 7375, 4340, 311, 11013, 6605, 4340, 311, 1855, 323, 3239, 4621, 10533, 44576, 928, 8474, 91804, 90951, 6525, 15473, 448, 8688, 8819, 44461, 15672, 3840, 15672, 4119, 7524, 68574, 25486, 6968, 4119, 82363, 13314, 56037, 71104, 63530, 49645, 44576, 928, 8474, 1592, 19083, 10533, 26223, 18837, 16378, 11434, 320, 43, 41664, 8, 15820, 40404, 318, 347, 2719, 5097, 87073, 54615, 19264, 12020, 7231, 831, 55988, 9471, 320, 49, 1890, 8, 12020, 7231, 831, 12020, 7231, 3004, 68836, 3749, 76509, 97457, 16275, 16451, 703, 3419, 11, 914, 9794, 9323, 1011, 1178, 12503, 28063, 29300, 7740, 8098, 16583, 1282, 4527, 3781, 10533, 10234, 22463, 18837, 30, 36, 23287, 123918, 250, 147615, 30543, 22463, 41885, 123918, 250, 147963, 30543, 22463, 11212, 69015, 85, 15, 13, 18, 85, 15, 13, 17, 13828, 67, 8159, 24748, 44, 5233, 1095, 504, 348, 15, 13, 15, 26179, 4340, 311, 44566, 504, 348, 15, 13, 15, 26179, 44, 5233, 1095, 504, 62185, 18837, 44, 5233, 1095, 504, 55396, 1663, 18837, 44, 5233, 1095, 504, 55396, 1663, 12020, 7231, 831, 18837, 44, 5233, 1095, 504, 444, 10994, 18837, 44, 5233, 1095, 504, 19504, 8035, 587, 18837, 44, 5233, 1095, 504, 444, 10994, 9523, 18837, 44, 5233, 1095, 504, 5027, 50325, 27143, 18837, 44, 5233, 1095, 504, 5027, 49, 261, 1180, 27143, 18837, 44, 5233, 1095, 504, 17439, 54615, 18837, 44, 5233, 1095, 504, 8550, 482, 27143, 18837, 44, 5233, 1095, 504, 19470, 831, 48, 1402, 5233, 1095, 504, 45486, 27143, 18837, 2324, 32259, 311, 22463, 11212, 4938, 4340, 311, 44566, 311, 22463, 11212, 4938, 4340, 311, 990, 5351, 15672, 2052, 13424, 448, 22463, 11212, 44, 5233, 1095, 1007, 50830, 4095, 10642, 476, 50830, 703, 4095, 10642, 44, 5233, 1095, 1007, 50830, 4095, 4267, 10642, 476, 55396, 17044, 4095, 10642, 44, 5233, 1095, 1007, 50830, 19237, 10642, 476, 50830, 19237, 4095, 10642, 32, 5724, 9285, 4195, 13850, 20713, 16077, 4842, 15352, 10974, 4340, 4686, 27193, 4340, 311, 653, 330, 721, 65489, 287, 1, 56370, 1925, 419, 2150, 4340, 311, 653, 330, 721, 65489, 287, 1, 56370, 198, 2733, 12346, 311, 29001, 804, 369, 9705, 389, 4621, 10533, 448, 5798, 3419, 1824, 369, 656, 65489, 287, 624, 32, 656, 65489, 287, 10759, 423, 374, 825, 429, 11, 438, 279, 829, 13230, 11, 702, 279, 5726, 311, 3239, 5086, 13, 44763, 11, 2661, 894, 5810, 4128, 3239, 11, 279, 10759, 423, 5711, 264, 3239, 12, 7596, 287, 444, 10994, 8781, 311, 3270, 264, 32930, 3239, 323, 1221, 16790, 429, 32930, 3239, 311, 1181, 16533, 4621, 3553, 13, 1096, 6147, 279, 10759, 423, 311, 537, 1172, 990, 279, 1196, 13933, 3239, 369, 41733, 37623, 12313, 448, 279, 8794, 315, 9768, 9293, 714, 311, 1083, 8649, 13406, 504, 279, 1196, 3239, 389, 279, 11160, 315, 9768, 9293, 323, 311, 9026, 1846, 13406, 382, 1949, 3855, 15692, 198, 2461, 29716, 9895, 582, 3278, 990, 264, 34218, 64, 4621, 3553, 13, 1205, 3003, 3465, 264, 2613, 16661, 738, 315, 9293, 429, 6644, 68922, 315, 9508, 624, 9112, 25, 576, 656, 65489, 10759, 423, 7460, 498, 311, 614, 326, 838, 6328, 10275, 624, 4, 51501, 4582, 1177, 44230, 1177, 43650, 220, 326, 838, 8688, 8819, 11582, 56984, 198, 1499, 8688, 8819, 4138, 56984, 1159, 34218, 64, 1499, 8688, 8819, 15467, 79306, 1159, 11789, 1499, 8688, 8819, 11311, 2143, 1159, 5264, 15469, 25486, 24602, 14120, 284, 508, 262, 11789, 7, 286, 2150, 7495, 428, 32, 15493, 315, 13923, 4446, 1182, 64275, 323, 1231, 29032, 18303, 20174, 497, 286, 11160, 15783, 3157, 788, 220, 16, 24, 24, 18, 11, 330, 21931, 788, 220, 22, 13, 22, 11, 330, 33613, 788, 330, 39557, 16989, 14345, 262, 6882, 262, 11789, 7, 286, 2150, 7495, 428, 98667, 7767, 12903, 10383, 5221, 5558, 304, 264, 7904, 2878, 264, 7904, 2878, 264, 7904, 2878, 264, 2503, 497, 286, 11160, 15783, 3157, 788, 220, 17, 15, 16, 15, 11, 330, 69795, 788, 330, 73966, 61632, 497, 330, 21931, 788, 220, 23, 13, 17, 2137, 262, 6882, 262, 11789, 7, 286, 2150, 7495, 428, 32, 53495, 608, 44159, 5221, 5558, 304, 264, 4013, 315, 18707, 2878, 18707, 2878, 18707, 323, 758, 995, 68743, 279, 4522, 497, 286, 11160, 15783, 3157, 788, 220, 17, 15, 15, 21, 11, 330, 69795, 788, 330, 50, 14030, 6023, 23388, 497, 330, 21931, 788, 220, 23, 13, 21, 2137, 262, 6882, 262, 11789, 7, 286, 2150, 7495, 428, 32, 15493, 315, 4622, 27835, 3198, 525, 34125, 974, 87218, 323, 1045, 2953, 33597, 1283, 1105, 497, 286, 11160, 15783, 3157, 788, 220, 17, 15, 16, 24, 11, 330, 69795, 788, 330, 38, 65698, 19929, 36922, 497, 330, 21931, 788, 220, 23, 13, 18, 2137, 262, 6882, 262, 11789, 7, 286, 2150, 7495, 428, 1249, 1047, 2525, 13675, 323, 614, 264, 20671, 3730, 773, 497, 286, 11160, 15783, 3157, 788, 220, 16, 24, 24, 20, 11, 330, 33613, 788, 330, 19700, 14345, 262, 6882, 262, 11789, 7, 286, 2150, 7495, 428, 19641, 2953, 4227, 1119, 279, 22178, 11, 2326, 2953, 4227, 700, 315, 279, 22178, 497, 286, 11160, 1165, 310, 330, 3157, 788, 220, 16, 24, 22, 24, 11, 310, 330, 69795, 788, 330, 3036, 55314, 350, 838, 78578, 497, 310, 330, 33613, 788, 330, 60024, 15252, 497, 310, 330, 21931, 788, 220, 24, 13, 24, 11, 286, 2470, 262, 6882, 60, 3215, 4314, 284, 34218, 64, 6387, 75927, 89419, 11, 5264, 15469, 25486, 24602, 2140, 7082, 17207, 25, 7524, 760, 5264, 15469, 25486, 24602, 198, 24973, 1039, 656, 65489, 287, 10759, 423, 15692, 198, 7039, 582, 646, 40902, 1039, 10759, 423, 13, 2014, 653, 419, 582, 3278, 1184, 311, 3410, 1045, 1995, 64351, 911, 279, 11160, 5043, 429, 1039, 9293, 1824, 323, 264, 2805, 4008, 315, 279, 2197, 8794, 624, 1499, 8688, 8819, 5329, 1735, 4786, 66210, 30892, 1159, 16752, 1731, 1499, 8688, 8819, 1327, 8927, 3004, 27149, 5738, 8928, 1159, 10115, 2859, 12020, 461, 2054, 1499, 8688, 8819, 11311, 2143, 1159, 12853, 5002, 32, 1427, 7603, 5013, 3109, 284, 508, 262, 16752, 1731, 7, 286, 829, 428, 33613, 497, 286, 4008, 428, 785, 17328, 315, 279, 5700, 13, 3776, 315, 2509, 39557, 16989, 516, 364, 874, 9313, 516, 364, 67, 30373, 516, 364, 60024, 15252, 516, 364, 441, 681, 516, 364, 1311, 516, 364, 19700, 74747, 286, 943, 428, 917, 497, 262, 6882, 262, 16752, 1731, 7, 286, 829, 428, 3157, 497, 286, 4008, 428, 785, 1042, 279, 5700, 572, 5880, 497, 286, 943, 428, 11662, 497, 262, 6882, 262, 16752, 1731, 7, 286, 829, 428, 69795, 497, 286, 4008, 428, 785, 829, 315, 279, 5700, 7538, 497, 286, 943, 428, 917, 497, 262, 6882, 262, 16752, 1731, 7, 286, 829, 428, 21931, 497, 4008, 428, 32, 220, 16, 12, 16, 15, 10728, 369, 279, 5700, 497, 943, 428, 3649, 1, 262, 6882, 60, 6062, 7495, 11448, 284, 330, 85984, 12126, 315, 264, 5700, 1, 654, 76, 284, 12853, 5002, 15469, 7, 34558, 28, 15, 8, 265, 8927, 423, 284, 10115, 2859, 12020, 461, 2054, 6387, 42995, 76, 7, 262, 9323, 76, 11, 262, 4621, 4314, 11, 262, 2197, 7495, 11448, 11, 262, 11160, 5013, 3109, 46021, 7082, 17207, 25, 3907, 1731, 760, 10115, 2859, 12020, 461, 2054, 760, 12853, 5002, 15469, 198, 16451, 432, 700, 15692, 198, 3036, 1431, 582, 646, 3520, 1430, 1667, 1039, 10759, 423, 4894, 2, 1096, 3110, 1172, 29102, 264, 4051, 265, 8927, 423, 27110, 445, 40, 1366, 311, 3736, 264, 5700, 21628, 5080, 1091, 220, 23, 13, 20, 1138, 58, 7524, 12024, 7495, 1131, 19641, 2953, 4227, 1119, 279, 22178, 11, 2326, 2953, 4227, 700, 315, 279, 22178, 516, 11160, 12854, 69795, 1210, 364, 3036, 55314, 350, 838, 78578, 516, 364, 33613, 1210, 364, 60024, 15252, 516, 364, 21931, 1210, 220, 24, 13, 24, 11, 364, 3157, 1210, 220, 16, 24, 22, 24, 38842, 11789, 12024, 7495, 1131, 32, 53495, 608, 44159, 5221, 5558, 304, 264, 4013, 315, 18707, 2878, 18707, 2878, 18707, 323, 758, 995, 68743, 279, 4522, 516, 11160, 12854, 69795, 1210, 364, 50, 14030, 6023, 23388, 516, 364, 21931, 1210, 220, 23, 13, 21, 11, 364, 3157, 1210, 220, 17, 15, 15, 21, 5410, 921, 2, 1096, 3110, 29102, 264, 3239, 323, 264, 4051, 265, 8927, 423, 27110, 445, 10281, 479, 65698, 19929, 36922, 15540, 894, 9508, 911, 3198, 1138, 58, 7524, 12024, 7495, 1131, 32, 15493, 315, 4622, 27835, 3198, 525, 34125, 974, 87218, 323, 1045, 2953, 33597, 1283, 1105, 516, 11160, 12854, 69795, 1210, 364, 38, 65698, 19929, 36922, 516, 364, 21931, 1210, 220, 23, 13, 18, 11, 364, 3157, 1210, 220, 17, 15, 16, 24, 5410, 921, 2, 1096, 3110, 29102, 264, 27714, 4051, 265, 8927, 423, 27110, 445, 3838, 594, 264, 7548, 21628, 320, 48432, 220, 23, 13, 20, 8, 8038, 16989, 4531, 70828, 58, 7524, 12024, 7495, 1131, 32, 53495, 608, 44159, 5221, 5558, 304, 264, 4013, 315, 18707, 2878, 18707, 2878, 18707, 323, 758, 995, 68743, 279, 4522, 516, 11160, 12854, 69795, 1210, 364, 50, 14030, 6023, 23388, 516, 364, 21931, 1210, 220, 23, 13, 21, 11, 364, 3157, 1210, 220, 17, 15, 15, 21, 38842, 11789, 12024, 7495, 1131, 19641, 2953, 4227, 1119, 279, 22178, 11, 2326, 2953, 4227, 700, 315, 279, 22178, 516, 11160, 12854, 69795, 1210, 364, 3036, 55314, 350, 838, 78578, 516, 364, 33613, 1210, 364, 60024, 15252, 516, 364, 21931, 1210, 220, 24, 13, 24, 11, 364, 3157, 1210, 220, 16, 24, 22, 24, 5410, 921, 2, 1096, 3110, 29102, 264, 3239, 323, 27714, 4051, 265, 8927, 423, 27110, 7, 262, 330, 3838, 594, 264, 5700, 1283, 220, 16, 24, 24, 15, 714, 1573, 220, 17, 15, 15, 20, 429, 594, 678, 911, 23069, 11, 323, 51654, 374, 11371, 1138, 58, 7524, 12024, 7495, 1131, 1249, 1047, 2525, 13675, 323, 614, 264, 20671, 3730, 773, 516, 11160, 12854, 33613, 1210, 364, 19700, 516, 364, 3157, 1210, 220, 16, 24, 24, 20, 5410, 921, 5632, 595, 15692, 198, 1654, 646, 1083, 990, 279, 656, 3239, 10759, 423, 311, 13837, 595, 25, 279, 1372, 315, 9293, 311, 7807, 624, 1654, 646, 653, 419, 553, 12299, 7283, 14763, 3618, 311, 279, 4692, 624, 265, 8927, 423, 284, 10115, 2859, 12020, 461, 2054, 6387, 42995, 76, 7, 262, 9323, 76, 11, 262, 4621, 4314, 11, 262, 2197, 7495, 11448, 11, 262, 11160, 5013, 3109, 11, 262, 7283, 14763, 3618, 11, 43385, 1096, 3110, 1172, 29102, 264, 9760, 3239, 265, 8927, 423, 27110, 445, 3838, 525, 1378, 9508, 911, 64275, 1138, 58, 7524, 12024, 7495, 1131, 32, 15493, 315, 13923, 4446, 1182, 64275, 323, 1231, 29032, 18303, 20174, 516, 11160, 12854, 33613, 1210, 364, 39557, 16989, 516, 364, 21931, 1210, 220, 22, 13, 22, 11, 364, 3157, 1210, 220, 16, 24, 24, 18, 38842, 11789, 12024, 7495, 1131, 1249, 1047, 2525, 13675, 323, 614, 264, 20671, 3730, 773, 516, 11160, 12854, 33613, 1210, 364, 19700, 516, 364, 3157, 1210, 220, 16, 24, 24, 20, 5410, 921, 28468, 287, 504, 18778, 448, 444, 41664, 15692, 198, 1249, 1490, 1128, 594, 2087, 389, 1212, 279, 27215, 11, 323, 311, 614, 803, 2526, 2524, 11, 582, 646, 43828, 1039, 10759, 423, 504, 18778, 624, 5338, 11, 582, 1184, 311, 1855, 264, 3239, 14859, 3024, 8781, 13, 1096, 8781, 686, 1896, 264, 1196, 3239, 323, 7907, 264, 16139, 3073, 2859, 1633, 892, 40155, 279, 13406, 5189, 553, 279, 1196, 13, 1205, 3410, 1045, 13137, 5746, 369, 6825, 264, 9934, 323, 2550, 6729, 13, 4220, 614, 264, 1372, 315, 11460, 480, 3628, 429, 582, 3278, 10034, 1588, 369, 38975, 624, 1499, 8688, 8819, 5329, 1735, 4786, 66210, 8928, 1159, 320, 262, 16139, 3073, 2859, 5097, 6570, 11, 262, 633, 5738, 66210, 61421, 46021, 40581, 284, 633, 5738, 66210, 61421, 7, 262, 2197, 7495, 11448, 11, 262, 11160, 5013, 3109, 46021, 3006, 18517, 284, 16139, 3073, 2859, 5097, 6570, 6387, 23258, 368, 1631, 66210, 284, 9934, 760, 9323, 76, 760, 2550, 18517, 7082, 17207, 25, 97457, 2859, 5097, 6570, 760, 633, 5738, 66210, 61421, 198, 10061, 594, 1401, 518, 1039, 9934, 510, 1350, 72253, 8021, 10741, 428, 31390, 3405, 5455, 7771, 5795, 374, 311, 5944, 279, 1196, 594, 3239, 311, 2432, 279, 1681, 10802, 3897, 3685, 13, 2442, 16139, 3073, 6145, 12539, 3578, 4498, 29338, 990, 264, 50494, 2038, 43065, 448, 264, 4718, 1633, 23126, 304, 279, 2701, 10802, 7190, 61069, 61069, 63, 2236, 90, 262, 330, 1631, 788, 914, 1124, 1467, 914, 311, 9429, 311, 2197, 8794, 262, 330, 5315, 788, 914, 1124, 19819, 2971, 5114, 369, 29670, 9293, 11035, 61069, 61069, 63, 785, 3239, 914, 1265, 6644, 1172, 1467, 429, 374, 3601, 311, 2432, 279, 8794, 315, 9293, 13, 5765, 4682, 304, 279, 4051, 1265, 537, 387, 9733, 304, 279, 3239, 438, 1632, 875, 19819, 2971, 5114, 374, 23415, 315, 825, 476, 803, 12313, 323, 19819, 5666, 12239, 875, 12313, 5114, 4990, 279, 1352, 25, 1565, 5689, 32431, 11, 1044, 46186, 10944, 1565, 5689, 63, 320, 11006, 760, 834, 760, 25161, 760, 342, 665, 760, 25175, 760, 326, 665, 760, 6644, 760, 1075, 760, 304, 760, 19550, 1648, 52040, 12, 1565, 2991, 63, 320, 917, 1648, 220, 829, 315, 7035, 311, 3796, 279, 12313, 311, 12, 1565, 831, 63, 320, 917, 1648, 374, 279, 12313, 897, 32, 19819, 5666, 5114, 4990, 279, 1352, 1565, 453, 60971, 16, 11, 5114, 17, 11, 2503, 46186, 10944, 1565, 453, 63, 320, 437, 760, 476, 760, 537, 1648, 19819, 5675, 12, 1565, 24184, 16, 7808, 1565, 24184, 17, 7808, 2503, 320, 54705, 12239, 476, 19819, 5666, 12239, 1648, 825, 476, 803, 12239, 311, 3796, 279, 5666, 311, 8078, 2704, 429, 498, 1172, 990, 279, 7654, 2973, 323, 19819, 19624, 10007, 3403, 323, 902, 3800, 50133, 2704, 429, 13406, 1172, 8300, 311, 8201, 429, 3000, 304, 279, 821, 2530, 50133, 2704, 429, 13406, 1172, 990, 279, 29606, 5036, 448, 1181, 729, 5036, 421, 1052, 525, 5746, 9251, 389, 1105, 50133, 2704, 429, 13406, 1172, 990, 3561, 1565, 28189, 18506, 40175, 63, 979, 11589, 2400, 821, 31969, 2750, 50133, 2704, 429, 13406, 1896, 1119, 2692, 279, 27787, 315, 8201, 323, 1172, 1281, 35495, 429, 525, 42203, 2661, 279, 943, 315, 821, 1660, 9768, 50133, 2704, 429, 13406, 525, 1172, 1483, 438, 4362, 13, 1416, 1052, 525, 902, 13406, 429, 1265, 387, 9251, 470, 330, 8996, 23728, 1, 369, 279, 4051, 897, 13, 2442, 13383, 220, 16, 13, 3578, 1043, 8748, 7190, 61069, 61069, 63, 2236, 90, 262, 330, 1796, 788, 330, 47312, 6198, 315, 264, 5492, 497, 262, 330, 12340, 788, 314, 286, 330, 18622, 788, 314, 310, 330, 1313, 788, 330, 917, 497, 310, 330, 4684, 788, 330, 675, 315, 279, 5492, 10049, 1, 286, 2470, 286, 330, 4129, 788, 314, 310, 330, 1313, 788, 330, 11662, 497, 310, 330, 4684, 788, 330, 4373, 315, 279, 5492, 304, 6486, 1, 286, 2470, 286, 330, 33613, 788, 314, 310, 330, 1313, 788, 330, 917, 497, 310, 330, 4684, 788, 330, 785, 5492, 17328, 11, 825, 315, 330, 8374, 497, 330, 20521, 1, 476, 330, 4611, 3014, 286, 335, 262, 3869, 59, 61069, 61069, 63, 1474, 11361, 25, 3838, 525, 11674, 553, 16439, 23670, 476, 72127, 30321, 911, 33168, 29263, 1212, 220, 18, 4420, 1293, 304, 279, 15254, 2420, 17328, 97457, 6145, 7190, 61069, 61069, 63, 2236, 90, 262, 330, 1631, 788, 330, 14901, 1409, 2948, 497, 262, 330, 5315, 788, 330, 437, 79920, 69909, 36014, 18622, 16215, 7245, 67136, 23670, 2105, 701, 8939, 36014, 18622, 16215, 7245, 42, 49971, 30321, 2105, 5731, 25175, 36014, 4129, 16215, 220, 16, 23, 15, 701, 8939, 36014, 33613, 16215, 7245, 8374, 2105, 593, 9207, 59, 61069, 61069, 63, 2442, 13383, 220, 17, 13, 3578, 1043, 8748, 7190, 61069, 61069, 63, 2236, 90, 262, 330, 1796, 788, 330, 47312, 6198, 315, 264, 5492, 497, 262, 330, 12340, 788, 314, 286, 330, 18622, 788, 314, 310, 330, 1313, 788, 330, 917, 497, 310, 330, 4684, 788, 330, 675, 315, 279, 5492, 10049, 1, 286, 2470, 286, 330, 4129, 788, 314, 310, 330, 1313, 788, 330, 11662, 497, 310, 330, 4684, 788, 330, 4373, 315, 279, 5492, 304, 6486, 1, 286, 2470, 286, 330, 33613, 788, 314, 310, 330, 1313, 788, 330, 917, 497, 310, 330, 4684, 788, 330, 785, 5492, 17328, 11, 825, 315, 330, 8374, 497, 330, 20521, 1, 476, 330, 4611, 3014, 286, 335, 262, 3869, 59, 61069, 61069, 63, 1474, 11361, 25, 3838, 525, 11674, 429, 1033, 537, 4652, 389, 40537, 97457, 6145, 7190, 61069, 61069, 63, 2236, 90, 262, 330, 1631, 788, 7342, 262, 330, 5315, 788, 330, 8996, 23728, 9207, 59, 61069, 61069, 63, 2442, 13383, 220, 18, 13, 3578, 1043, 8748, 7190, 61069, 61069, 63, 2236, 90, 262, 330, 1796, 788, 330, 85984, 12126, 315, 264, 5700, 497, 262, 330, 12340, 788, 314, 262, 330, 33613, 788, 314, 286, 330, 4684, 788, 330, 785, 17328, 315, 279, 5700, 13, 3776, 315, 2509, 39557, 16989, 516, 364, 874, 9313, 516, 364, 67, 30373, 516, 364, 60024, 15252, 516, 364, 441, 681, 516, 364, 1311, 516, 364, 19700, 74747, 286, 330, 1313, 788, 330, 917, 1, 262, 2470, 262, 330, 3157, 788, 314, 286, 330, 4684, 788, 330, 785, 1042, 279, 5700, 572, 5880, 497, 286, 330, 1313, 788, 330, 11662, 1, 262, 2470, 262, 330, 69795, 788, 314, 286, 330, 4684, 788, 330, 785, 829, 315, 279, 5700, 7538, 497, 286, 330, 1313, 788, 330, 917, 1, 262, 2470, 262, 330, 21931, 788, 314, 286, 330, 4684, 788, 330, 32, 220, 16, 12, 16, 15, 10728, 369, 279, 5700, 497, 286, 330, 1313, 788, 330, 3649, 1, 262, 97901, 59, 61069, 61069, 63, 1474, 11361, 25, 31390, 3405, 97457, 6145, 510, 3036, 1128, 1039, 2480, 8781, 18644, 510, 1631, 66210, 27110, 7, 262, 314, 286, 330, 1631, 788, 330, 3838, 525, 1045, 37974, 36631, 9508, 504, 279, 220, 24, 15, 594, 15540, 553, 13784, 425, 71405, 911, 32505, 11788, 1, 262, 2751, 97457, 2859, 10741, 1131, 17616, 72, 5579, 516, 4051, 28, 8432, 79091, 38698, 18461, 875, 8065, 25, 364, 437, 6270, 11, 5977, 5818, 33487, 14094, 12356, 38698, 38658, 5142, 48, 25, 364, 11006, 6270, 11, 7035, 1131, 33613, 516, 897, 1131, 39557, 16989, 4567, 16730, 79091, 38698, 18461, 875, 8065, 25, 364, 437, 6270, 11, 5977, 5818, 33487, 14094, 12356, 38698, 38658, 1224, 2446, 25, 364, 55067, 6270, 11, 7035, 1131, 3157, 516, 897, 28, 16, 24, 24, 15, 701, 42451, 14094, 12356, 38698, 38658, 1214, 51, 25, 364, 4832, 6270, 11, 7035, 1131, 3157, 516, 897, 28, 17, 15, 15, 15, 7252, 701, 42451, 14094, 12356, 38698, 38658, 5142, 48, 25, 364, 11006, 6270, 11, 7035, 1131, 69795, 516, 897, 1131, 40645, 425, 71405, 863, 9719, 3930, 5856, 340, 785, 3239, 4692, 374, 279, 1376, 2392, 315, 279, 656, 65489, 10759, 423, 13, 2014, 1281, 264, 2244, 56370, 1849, 498, 3278, 1184, 311, 1281, 2704, 697, 3239, 4692, 4278, 1632, 13, 34916, 419, 7460, 42368, 279, 9934, 11, 279, 10295, 304, 279, 9934, 11, 279, 7035, 27787, 11, 4992, 13, 1752, 458, 3110, 429, 22479, 1526, 73185, 264, 3239, 4692, 389, 1045, 9500, 15444, 821, 11, 1779, 700, 419, 76104, 624, 785, 1790, 1376, 2392, 374, 279, 32930, 3239, 45488, 13, 1096, 374, 279, 1633, 8480, 369, 66271, 279, 13954, 16139, 3073, 2859, 1633, 1119, 264, 11160, 4051, 304, 279, 19482, 315, 279, 4621, 3553, 498, 2299, 1667, 13, 22463, 18837, 4041, 448, 264, 1372, 315, 5798, 3419, 72704, 13, 2014, 1490, 1105, 678, 1968, 311, 279, 29001, 804, 3772, 624, 1499, 8688, 8819, 97508, 4786, 64803, 1087, 5329, 56984, 1159, 34218, 64, 51653, 265, 8927, 423, 284, 10115, 2859, 12020, 461, 2054, 7, 262, 3239, 66210, 79194, 66210, 11, 262, 4621, 4314, 28, 3215, 4314, 11, 262, 32930, 5738, 7965, 21829, 28, 1143, 56984, 51653, 1507, 8, 7082, 17207, 25, 1143, 56984, 51653, 198, 265, 8927, 423, 27110, 7, 262, 330, 3838, 594, 264, 5700, 1283, 220, 16, 24, 24, 15, 714, 1573, 220, 17, 15, 15, 20, 429, 594, 678, 911, 23069, 11, 323, 51654, 374, 11371, 1138, 58, 7524, 12024, 7495, 1131, 1249, 1047, 2525, 13675, 323, 614, 264, 20671, 3730, 773, 516, 11160, 12854, 33613, 1210, 364, 19700, 516, 364, 3157, 1210, 220, 16, 24, 24, 20, 5410, 60, 4036, 419, 2150, 26034, 419, 2150, 10950, 30, 21291, 4340, 311, 1494, 15592, 23594, 311, 1598, 77, 4788, 5847, 4340, 311, 6718, 1467, 3118, 389, 41733, 37623, 1949, 3855, 24973, 1039, 656, 65489, 287, 10759, 423, 16451, 432, 700, 5632, 595, 28468, 287, 504, 18778, 448, 444, 41664, 33768, 24862, 75615, 41574, 30280, 12545, 14, 9951, 7661, 96452, 26700, 53746, 13547, 7240, 220, 17, 15, 17, 20, 22463, 18837, 11, 4848, 74123, 12448, 220, 5959, 4340, 311, 990, 2550, 87073, 311, 4715, 458, 444, 10994, 2033, 1119, 32930, 3561, 760, 11162, 99, 250, 30543, 146450, 22463, 18837, 34583, 35134, 311, 1887, 2213, 12292, 601, 518, 220, 22145, 25, 576, 20713, 15235, 14872, 553, 22463, 18837, 389, 3217, 220, 16, 18, 609, 220, 16, 19, 304, 5836, 12879, 0, 1072, 14412, 804, 7082, 17207, 7661, 52984, 10607, 15919, 1454, 5785, 26223, 41885, 26223, 11212, 26223, 18837, 26538, 26223, 18837, 12162, 14, 9951, 85, 15, 13, 18, 85, 15, 13, 18, 85, 15, 13, 17, 85, 15, 13, 16, 145653, 5890, 37155, 51, 54927, 11066, 264, 15846, 21806, 287, 3766, 916, 264, 12165, 9994, 51, 54927, 11066, 264, 4285, 444, 10994, 3766, 448, 6236, 4119, 323, 9934, 19911, 11066, 264, 12853, 6331, 11066, 264, 19470, 831, 4928, 26980, 23470, 320, 49, 1890, 8, 1845, 25, 3660, 220, 17, 11066, 458, 94506, 28525, 11066, 458, 20713, 5668, 3173, 11066, 264, 19470, 831, 4928, 26980, 23470, 320, 49, 1890, 8, 1845, 25, 3660, 220, 16, 11066, 264, 41733, 2711, 4712, 11066, 264, 15846, 14, 16141, 287, 1849, 916, 7870, 821, 9190, 5612, 551, 2918, 4340, 4686, 27193, 4340, 4686, 27193, 4340, 311, 990, 7375, 304, 264, 8781, 4340, 311, 990, 264, 4621, 4314, 438, 264, 10759, 423, 4340, 311, 912, 4938, 311, 6236, 61905, 4340, 311, 990, 3110, 56037, 4340, 311, 912, 264, 41733, 6193, 916, 4771, 4625, 4340, 311, 19873, 1598, 77, 4788, 304, 15279, 4340, 311, 4269, 6236, 1614, 14507, 4340, 311, 912, 1638, 28696, 2827, 311, 264, 22109, 4340, 311, 912, 56370, 311, 6236, 61905, 4340, 311, 990, 2421, 6552, 10295, 304, 6236, 4119, 4340, 311, 653, 5392, 70643, 8098, 4340, 311, 4582, 22463, 18837, 14185, 4340, 311, 912, 10295, 311, 279, 9934, 369, 3239, 6358, 4340, 311, 990, 2421, 6552, 10295, 4340, 311, 1598, 2526, 5746, 4340, 311, 990, 2550, 87073, 311, 4715, 458, 444, 10994, 2033, 1119, 32930, 3561, 4340, 311, 3705, 5048, 1380, 902, 19556, 525, 7907, 4340, 311, 6021, 1948, 1186, 11582, 1735, 4340, 311, 470, 32930, 821, 504, 264, 1614, 4340, 311, 62079, 1467, 1526, 15279, 2022, 4340, 311, 62079, 1467, 1526, 86875, 72913, 4340, 311, 62079, 1467, 304, 264, 3175, 444, 10994, 1618, 4340, 311, 990, 5392, 89417, 4340, 311, 912, 993, 2832, 509, 5392, 8098, 22302, 311, 444, 10994, 82, 323, 12853, 26874, 11066, 458, 20713, 448, 20713, 25255, 320, 77415, 8, 4340, 311, 9245, 6540, 38999, 4340, 311, 25244, 3561, 9934, 19911, 4340, 311, 3705, 5248, 19556, 979, 3730, 3239, 6358, 4340, 311, 990, 5798, 3419, 7375, 323, 5392, 89417, 4340, 311, 1494, 1526, 5977, 504, 825, 3019, 311, 279, 1790, 4340, 311, 30335, 50932, 3786, 4340, 311, 3705, 5248, 10759, 3004, 979, 3730, 3239, 6358, 4340, 311, 912, 2750, 311, 264, 8781, 594, 1584, 4340, 311, 9245, 13406, 369, 3239, 6358, 4340, 311, 14411, 15592, 8781, 91025, 4340, 3484, 448, 1550, 55880, 487, 22049, 52603, 979, 3730, 3239, 6358, 10268, 11789, 27811, 4340, 311, 990, 279, 17439, 2859, 12020, 461, 2054, 4340, 311, 912, 12205, 311, 10759, 423, 3059, 34, 11829, 4340, 311, 990, 26679, 304, 3312, 21737, 4340, 311, 15498, 26679, 311, 264, 78679, 4340, 311, 57414, 26679, 220, 4692, 4340, 311, 6845, 2526, 4822, 4357, 4340, 311, 1494, 26679, 304, 518, 15592, 4340, 311, 6718, 553, 3668, 4340, 311, 6500, 6236, 1614, 14507, 4340, 311, 3705, 4379, 13388, 4340, 311, 2930, 894, 1614, 304, 825, 1555, 4340, 311, 3754, 3950, 10431, 304, 12853, 16969, 4340, 311, 912, 7375, 311, 6236, 61905, 4340, 311, 6718, 2038, 4340, 311, 653, 56370, 448, 65151, 25111, 4340, 311, 5508, 6452, 77, 4788, 311, 13852, 4340, 311, 1855, 2526, 4822, 24083, 4340, 311, 1855, 264, 2526, 6236, 1614, 536, 10268, 37068, 24602, 4340, 311, 1855, 264, 2526, 444, 10994, 536, 10268, 10392, 461, 2054, 4340, 311, 1855, 7375, 4340, 311, 7390, 697, 444, 10994, 10500, 4340, 311, 2795, 27445, 82, 4340, 311, 2795, 9293, 504, 264, 6220, 4340, 311, 2795, 9308, 4340, 311, 2795, 4718, 4340, 311, 2795, 73192, 4340, 311, 2795, 5100, 8246, 3542, 4340, 311, 2795, 11358, 82, 4340, 311, 2795, 3482, 6816, 4340, 311, 1855, 264, 8741, 320, 721, 12, 7596, 287, 8, 8781, 1178, 39088, 4119, 4340, 311, 15963, 3059, 504, 5248, 10759, 3004, 4340, 311, 3293, 10295, 504, 264, 22463, 41885, 10337, 4340, 311, 3293, 10295, 553, 3084, 4340, 311, 3293, 10295, 553, 53129, 31773, 40861, 320, 8035, 49, 8, 4340, 311, 3293, 10295, 553, 308, 12, 1520, 27248, 4340, 311, 3293, 10295, 553, 37623, 4340, 311, 990, 5785, 10295, 979, 3730, 32189, 4340, 311, 3705, 1293, 1467, 979, 3730, 32189, 4340, 311, 990, 49645, 7484, 320, 2152, 5392, 8098, 8, 311, 653, 32189, 4340, 311, 912, 32772, 82, 311, 264, 78679, 4340, 311, 4051, 6605, 30816, 16223, 7542, 4340, 311, 990, 279, 22463, 18837, 51980, 5333, 4340, 311, 24085, 1598, 77, 4788, 26223, 18837, 16378, 11434, 8436, 1862, 3674, 4340, 311, 6500, 444, 10994, 14507, 4340, 311, 3754, 3950, 10431, 369, 444, 10994, 82, 6727, 4119, 23490, 4340, 311, 633, 1487, 48216, 4340, 311, 83184, 30403, 3059, 311, 49360, 279, 330, 54337, 304, 279, 6149, 1, 2456, 4340, 311, 6718, 73192, 553, 21426, 4340, 311, 10880, 23921, 6605, 315, 279, 1852, 943, 4340, 311, 912, 1943, 3840, 4340, 311, 44566, 504, 19588, 22463, 18837, 13009, 311, 22463, 11212, 4340, 311, 17179, 1667, 5248, 22879, 817, 2197, 4340, 311, 1494, 79049, 57597, 821, 5961, 311, 4119, 4340, 311, 990, 79049, 57597, 50932, 4340, 311, 1855, 264, 2526, 9258, 21102, 4340, 311, 990, 279, 2550, 70913, 287, 6729, 4340, 311, 4715, 4718, 2550, 4340, 311, 22683, 979, 264, 22314, 1465, 13666, 4340, 311, 4715, 1467, 504, 1943, 6171, 4340, 311, 4715, 11874, 2550, 4340, 311, 4715, 53127, 2550, 4340, 311, 990, 279, 17022, 11789, 10392, 461, 2054, 4340, 311, 990, 22463, 18837, 448, 2155, 5355, 67, 8159, 10795, 4340, 311, 912, 6236, 3840, 4340, 311, 633, 264, 431, 1890, 3766, 311, 912, 51846, 4340, 311, 653, 817, 8694, 56370, 4340, 311, 633, 697, 431, 1890, 3766, 311, 470, 8173, 4340, 311, 4269, 3059, 504, 697, 431, 1890, 3766, 4340, 311, 6718, 4718, 821, 4340, 311, 52847, 6718, 1467, 553, 5766, 2582, 11160, 4340, 311, 1494, 15592, 23594, 311, 1598, 77, 4788, 4340, 311, 653, 330, 721, 65489, 287, 1, 56370, 4340, 311, 6718, 1467, 3118, 389, 41733, 37623, 4340, 311, 8781, 1598, 77, 4788, 4340, 311, 3581, 323, 2795, 22463, 18837, 6171, 4340, 311, 6718, 1467, 553, 11211, 4340, 311, 6718, 9308, 4340, 311, 653, 3405, 35764, 916, 27445, 82, 4340, 311, 3484, 448, 3460, 31806, 979, 3730, 7870, 3405, 12, 596, 86, 4671, 4340, 311, 2664, 9934, 979, 3730, 7870, 3405, 12, 596, 86, 4671, 4340, 311, 653, 3239, 10519, 438, 949, 315, 7870, 3405, 12, 596, 86, 4671, 4340, 311, 4269, 1598, 77, 4788, 4340, 311, 4269, 14507, 504, 458, 444, 10994, 4340, 311, 990, 264, 882, 12635, 291, 4621, 3553, 10759, 423, 4340, 311, 470, 35036, 504, 264, 5392, 4340, 311, 990, 6236, 4119, 311, 1618, 7375, 4340, 311, 11156, 15279, 5392, 8098, 4340, 311, 5344, 4119, 311, 1618, 264, 5392, 4340, 311, 2615, 279, 22109, 2648, 504, 264, 5392, 4340, 311, 1494, 5392, 16275, 311, 6236, 4119, 4340, 311, 1494, 1598, 882, 2750, 311, 7375, 4340, 311, 4269, 4357, 504, 264, 5392, 4340, 311, 4269, 5392, 6738, 4340, 311, 5508, 7375, 311, 5264, 15469, 23550, 4340, 311, 3705, 5392, 5975, 4340, 311, 990, 2421, 63530, 49645, 448, 5392, 8098, 4340, 311, 912, 264, 3738, 3419, 10603, 60666, 369, 7375, 4340, 311, 10719, 1614, 18906, 7375, 4340, 311, 11013, 6605, 4340, 311, 1855, 323, 3239, 4621, 10533, 44576, 928, 8474, 91804, 90951, 6525, 15473, 448, 8688, 8819, 44461, 15672, 3840, 15672, 4119, 7524, 68574, 25486, 6968, 4119, 82363, 13314, 56037, 71104, 63530, 49645, 44576, 928, 8474, 1592, 19083, 10533, 26223, 18837, 16378, 11434, 320, 43, 41664, 8, 15820, 40404, 318, 347, 2719, 5097, 87073, 54615, 19264, 12020, 7231, 831, 55988, 9471, 320, 49, 1890, 8, 12020, 7231, 831, 12020, 7231, 3004, 68836, 3749, 76509, 97457, 16275, 16451, 703, 3419, 11, 914, 9794, 9323, 1011, 1178, 12503, 28063, 29300, 7740, 8098, 16583, 1282, 4527, 3781, 10533, 10234, 22463, 18837, 30, 36, 23287, 123918, 250, 147615, 30543, 22463, 41885, 123918, 250, 147963, 30543, 22463, 11212, 69015, 85, 15, 13, 18, 85, 15, 13, 17, 13828, 67, 8159, 24748, 44, 5233, 1095, 504, 348, 15, 13, 15, 26179, 4340, 311, 44566, 504, 348, 15, 13, 15, 26179, 44, 5233, 1095, 504, 62185, 18837, 44, 5233, 1095, 504, 55396, 1663, 18837, 44, 5233, 1095, 504, 55396, 1663, 12020, 7231, 831, 18837, 44, 5233, 1095, 504, 444, 10994, 18837, 44, 5233, 1095, 504, 19504, 8035, 587, 18837, 44, 5233, 1095, 504, 444, 10994, 9523, 18837, 44, 5233, 1095, 504, 5027, 50325, 27143, 18837, 44, 5233, 1095, 504, 5027, 49, 261, 1180, 27143, 18837, 44, 5233, 1095, 504, 17439, 54615, 18837, 44, 5233, 1095, 504, 8550, 482, 27143, 18837, 44, 5233, 1095, 504, 19470, 831, 48, 1402, 5233, 1095, 504, 45486, 27143, 18837, 2324, 32259, 311, 22463, 11212, 4938, 4340, 311, 44566, 311, 22463, 11212, 4938, 4340, 311, 990, 5351, 15672, 2052, 13424, 448, 22463, 11212, 44, 5233, 1095, 1007, 50830, 4095, 10642, 476, 50830, 703, 4095, 10642, 44, 5233, 1095, 1007, 50830, 4095, 4267, 10642, 476, 55396, 17044, 4095, 10642, 44, 5233, 1095, 1007, 50830, 19237, 10642, 476, 50830, 19237, 4095, 10642, 32, 5724, 9285, 4195, 13850, 20713, 16077, 4842, 15352, 10974, 4340, 4686, 27193, 4340, 311, 990, 2550, 87073, 311, 4715, 458, 444, 10994, 2033, 1119, 32930, 3561, 1925, 419, 2150, 4340, 311, 990, 2550, 87073, 311, 4715, 458, 444, 10994, 2033, 1119, 32930, 3561, 198, 13806, 4119, 2550, 1467, 13, 1988, 1052, 525, 3039, 1380, 498, 1366, 311, 633, 803, 32930, 1995, 1091, 1101, 1467, 1182, 13, 5976, 1045, 1614, 12565, 1824, 5798, 3419, 5510, 311, 470, 32930, 2550, 11, 537, 678, 653, 624, 5097, 87073, 525, 6846, 429, 1492, 5944, 4128, 1614, 14507, 13, 2619, 525, 1378, 1887, 5413, 458, 2550, 6729, 1969, 4211, 1447, 1, 1949, 3561, 11221, 788, 362, 1714, 892, 4675, 264, 914, 8482, 11221, 369, 1246, 279, 2550, 315, 264, 4128, 1614, 1265, 387, 23126, 624, 1, 14463, 788, 362, 1714, 892, 4990, 304, 264, 914, 320, 395, 38155, 311, 387, 279, 2033, 504, 264, 4128, 1614, 8, 323, 70835, 432, 1119, 1045, 5944, 382, 3036, 1221, 825, 10101, 825, 1447, 1, 14463, 448, 9934, 788, 362, 1714, 892, 4990, 304, 264, 914, 320, 395, 38155, 311, 387, 279, 2033, 504, 264, 4128, 1614, 8, 323, 264, 9934, 320, 395, 38155, 311, 387, 279, 9934, 429, 7907, 1741, 264, 2033, 8, 323, 70835, 432, 1119, 1045, 5944, 13, 576, 9934, 374, 13771, 3897, 304, 279, 1538, 279, 9258, 6570, 6801, 311, 22683, 476, 5046, 279, 2550, 304, 1045, 1616, 11, 323, 3880, 1995, 504, 279, 9934, 311, 653, 773, 382, 1949, 3855, 15692, 198, 38214, 582, 728, 916, 279, 1887, 943, 315, 2550, 6729, 11, 279, 5355, 67, 8159, 5097, 6570, 624, 1499, 8688, 8819, 15467, 13413, 620, 40488, 1159, 5355, 67, 8159, 5097, 6570, 1499, 8688, 8819, 15467, 60748, 12754, 1159, 59501, 7275, 1499, 8688, 8819, 11311, 2143, 1159, 5264, 15469, 1499, 4510, 67, 8159, 1159, 64605, 11, 8601, 11, 1614, 8337, 266, 493, 720, 284, 5264, 15469, 7635, 1269, 428, 70, 417, 12, 18, 13, 20, 2385, 324, 749, 3419, 1235, 497, 9315, 28, 15, 13, 15, 43385, 18614, 697, 12685, 821, 5944, 3074, 619, 4740, 22225, 1712, 1648, 262, 6505, 25, 607, 284, 8601, 29833, 428, 7841, 311, 738, 705, 264, 21646, 899, 262, 20380, 1056, 25, 607, 284, 8601, 29833, 428, 9217, 311, 8830, 279, 21646, 899, 262, 671, 1446, 646, 912, 2526, 10519, 12218, 6707, 448, 5355, 67, 8159, 13, 262, 569, 2528, 64959, 31356, 428, 14801, 899, 262, 569, 26755, 262, 707, 3405, 90729, 6615, 28915, 18924, 20398, 11, 2750, 25, 6451, 8, 1464, 6451, 25, 286, 6505, 284, 2750, 670, 445, 15188, 899, 286, 421, 6505, 323, 6505, 7609, 16, 60, 961, 27244, 788, 310, 4828, 15402, 445, 17082, 398, 14122, 3405, 88783, 286, 470, 2750, 2, 2573, 705, 264, 6729, 488, 15551, 11221, 1119, 279, 9934, 3811, 25617, 284, 5355, 67, 8159, 5097, 6570, 1295, 40121, 8159, 5314, 28, 41, 4740, 8, 40581, 284, 59501, 7275, 7, 262, 3811, 428, 16141, 279, 1196, 3239, 7110, 77, 90, 2243, 82427, 11035, 77, 90, 1631, 11035, 77, 497, 262, 1946, 28182, 27965, 1631, 7914, 262, 7130, 28182, 15783, 2243, 82427, 788, 6729, 670, 8955, 82427, 76777, 43385, 1597, 264, 3239, 10602, 311, 9934, 264, 4128, 1614, 311, 30446, 279, 821, 5944, 65399, 8378, 5047, 284, 9934, 760, 1614, 3006, 284, 9934, 8378, 5047, 27110, 16864, 1631, 788, 330, 40451, 752, 264, 21646, 1189, 5410, 9657, 27110, 11057, 8, 7082, 17207, 25, 13828, 67, 8159, 5097, 6570, 760, 59501, 7275, 760, 5264, 15469, 198, 41, 4740, 14171, 454, 1131, 10234, 1521, 279, 41020, 2484, 2518, 49634, 20380, 1056, 1131, 17949, 432, 5485, 279, 32466, 31523, 36180, 43, 41664, 15692, 198, 5097, 87073, 4211, 279, 22109, 3749, 11, 279, 6770, 4752, 2504, 315, 279, 22463, 18837, 16378, 11434, 320, 43, 41664, 568, 1096, 3363, 807, 1824, 19873, 11, 264, 22430, 11, 4269, 11, 264, 4027, 11, 7162, 11, 668, 754, 11, 264, 4027, 5224, 6738, 624, 5097, 87073, 4193, 264, 914, 476, 5351, 2052, 438, 1946, 323, 646, 470, 458, 24168, 943, 624, 9657, 27110, 11057, 340, 41, 4740, 14171, 454, 1131, 10234, 1521, 279, 41020, 2484, 2518, 49634, 20380, 1056, 1131, 17949, 432, 5485, 279, 32466, 31523, 36180, 30787, 315, 20083, 46028, 279, 6729, 11, 582, 1083, 1410, 3003, 1101, 3694, 432, 311, 1039, 22109, 8500, 510, 8819, 284, 9934, 760, 1614, 760, 6729, 8819, 27110, 16864, 1631, 788, 330, 40451, 752, 264, 21646, 1189, 3518, 41, 4740, 14171, 454, 1131, 10234, 1521, 279, 41020, 2484, 2518, 49634, 20380, 1056, 1131, 17949, 432, 5485, 279, 32466, 31523, 36180, 7983, 678, 87073, 1824, 279, 16842, 3749, 11, 1172, 3654, 87073, 646, 4269, 1526, 25244, 15676, 6171, 11, 2474, 419, 374, 7548, 17749, 389, 279, 2550, 943, 13, 393, 40488, 892, 4157, 9245, 7130, 6171, 686, 4936, 7540, 279, 7225, 15676, 2550, 624, 785, 8993, 5014, 5097, 6570, 369, 3110, 646, 4269, 1526, 7130, 16275, 510, 1499, 8688, 8819, 13413, 620, 40488, 4323, 1159, 8993, 5014, 5097, 6570, 2236, 61421, 284, 59501, 7275, 6387, 8693, 7, 262, 330, 5598, 264, 4718, 1633, 448, 458, 1565, 9217, 63, 1376, 429, 11253, 279, 2701, 3405, 25, 314, 7841, 55961, 2236, 18517, 284, 8993, 5014, 5097, 6570, 368, 2236, 30583, 284, 2951, 61421, 760, 1614, 760, 2951, 18517, 7082, 17207, 25, 16374, 5014, 5097, 6570, 198, 1607, 9304, 30583, 14958, 16864, 7841, 788, 330, 15191, 35492, 279, 72657, 7521, 44194, 58, 22655, 5360, 9217, 1210, 3355, 2137, 5360, 9217, 1210, 364, 17117, 24731, 5360, 9217, 1210, 364, 17117, 263, 24731, 5360, 9217, 1210, 364, 17117, 263, 645, 24731, 5360, 9217, 1210, 364, 17117, 263, 645, 5242, 24731, 5360, 9217, 1210, 364, 17117, 263, 645, 5242, 12066, 24731, 5360, 9217, 1210, 364, 17117, 263, 645, 5242, 12066, 84, 24731, 5360, 9217, 1210, 364, 17117, 263, 645, 5242, 12066, 84, 16948, 24731, 5360, 9217, 1210, 364, 17117, 263, 645, 5242, 12066, 84, 16948, 6161, 24731, 5360, 9217, 1210, 364, 17117, 263, 645, 5242, 12066, 84, 16948, 6161, 1225, 8275, 921, 67691, 11, 1958, 5355, 67, 8159, 5097, 6570, 510, 1607, 62591, 14958, 16864, 1631, 788, 330, 40451, 752, 264, 21646, 1189, 44194, 92863, 4740, 14171, 454, 1131, 10234, 1521, 279, 41020, 2484, 2518, 49634, 20380, 1056, 1131, 4567, 619, 4740, 14171, 454, 1131, 10234, 1521, 279, 41020, 2484, 2518, 49634, 20380, 1056, 1131, 17949, 4567, 619, 4740, 14171, 454, 1131, 10234, 1521, 279, 41020, 2484, 2518, 49634, 20380, 1056, 1131, 17949, 432, 4567, 619, 4740, 14171, 454, 1131, 10234, 1521, 279, 41020, 2484, 2518, 49634, 20380, 1056, 1131, 17949, 432, 5485, 4567, 619, 4740, 14171, 454, 1131, 10234, 1521, 279, 41020, 2484, 2518, 49634, 20380, 1056, 1131, 17949, 432, 5485, 279, 4567, 619, 4740, 14171, 454, 1131, 10234, 1521, 279, 41020, 2484, 2518, 49634, 20380, 1056, 1131, 17949, 432, 5485, 279, 32466, 4567, 619, 4740, 14171, 454, 1131, 10234, 1521, 279, 41020, 2484, 2518, 49634, 20380, 1056, 1131, 17949, 432, 5485, 279, 32466, 31523, 4567, 619, 4740, 14171, 454, 1131, 10234, 1521, 279, 41020, 2484, 2518, 49634, 20380, 1056, 1131, 17949, 432, 5485, 279, 32466, 31523, 0, 51028, 4036, 419, 2150, 26034, 419, 2150, 10950, 30, 21291, 4340, 311, 1598, 2526, 5746, 5847, 4340, 311, 3705, 5048, 1380, 902, 19556, 525, 7907, 1949, 3855, 43, 41664, 33768, 24862, 75615, 41574, 30280, 12545, 14, 9951, 7661, 96452, 26700, 53746, 13547, 7240, 220, 17, 15, 17, 20, 22463, 18837, 11, 4848, 74123, 12448, 220, 5959, 26223, 18837, 16378, 11434, 320, 43, 41664, 8, 760, 11162, 99, 250, 30543, 146450, 22463, 18837, 34583, 35134, 311, 1887, 2213, 12292, 601, 518, 220, 22145, 25, 576, 20713, 15235, 14872, 553, 22463, 18837, 389, 3217, 220, 16, 18, 609, 220, 16, 19, 304, 5836, 12879, 0, 1072, 14412, 804, 7082, 17207, 7661, 52984, 10607, 15919, 1454, 5785, 26223, 41885, 26223, 11212, 26223, 18837, 26538, 26223, 18837, 12162, 14, 9951, 85, 15, 13, 18, 85, 15, 13, 18, 85, 15, 13, 17, 85, 15, 13, 16, 145653, 5890, 37155, 51, 54927, 11066, 264, 15846, 21806, 287, 3766, 916, 264, 12165, 9994, 51, 54927, 11066, 264, 4285, 444, 10994, 3766, 448, 6236, 4119, 323, 9934, 19911, 11066, 264, 12853, 6331, 11066, 264, 19470, 831, 4928, 26980, 23470, 320, 49, 1890, 8, 1845, 25, 3660, 220, 17, 11066, 458, 94506, 28525, 11066, 458, 20713, 5668, 3173, 11066, 264, 19470, 831, 4928, 26980, 23470, 320, 49, 1890, 8, 1845, 25, 3660, 220, 16, 11066, 264, 41733, 2711, 4712, 11066, 264, 15846, 14, 16141, 287, 1849, 916, 7870, 821, 9190, 5612, 551, 2918, 4340, 4686, 27193, 4340, 4686, 27193, 4340, 311, 990, 7375, 304, 264, 8781, 4340, 311, 990, 264, 4621, 4314, 438, 264, 10759, 423, 4340, 311, 912, 4938, 311, 6236, 61905, 4340, 311, 990, 3110, 56037, 4340, 311, 912, 264, 41733, 6193, 916, 4771, 4625, 4340, 311, 19873, 1598, 77, 4788, 304, 15279, 4340, 311, 4269, 6236, 1614, 14507, 4340, 311, 912, 1638, 28696, 2827, 311, 264, 22109, 4340, 311, 912, 56370, 311, 6236, 61905, 4340, 311, 990, 2421, 6552, 10295, 304, 6236, 4119, 4340, 311, 653, 5392, 70643, 8098, 4340, 311, 4582, 22463, 18837, 14185, 4340, 311, 912, 10295, 311, 279, 9934, 369, 3239, 6358, 4340, 311, 990, 2421, 6552, 10295, 4340, 311, 1598, 2526, 5746, 4340, 311, 990, 2550, 87073, 311, 4715, 458, 444, 10994, 2033, 1119, 32930, 3561, 4340, 311, 3705, 5048, 1380, 902, 19556, 525, 7907, 4340, 311, 6021, 1948, 1186, 11582, 1735, 4340, 311, 470, 32930, 821, 504, 264, 1614, 4340, 311, 62079, 1467, 1526, 15279, 2022, 4340, 311, 62079, 1467, 1526, 86875, 72913, 4340, 311, 62079, 1467, 304, 264, 3175, 444, 10994, 1618, 4340, 311, 990, 5392, 89417, 4340, 311, 912, 993, 2832, 509, 5392, 8098, 22302, 311, 444, 10994, 82, 323, 12853, 26874, 11066, 458, 20713, 448, 20713, 25255, 320, 77415, 8, 4340, 311, 9245, 6540, 38999, 4340, 311, 25244, 3561, 9934, 19911, 4340, 311, 3705, 5248, 19556, 979, 3730, 3239, 6358, 4340, 311, 990, 5798, 3419, 7375, 323, 5392, 89417, 4340, 311, 1494, 1526, 5977, 504, 825, 3019, 311, 279, 1790, 4340, 311, 30335, 50932, 3786, 4340, 311, 3705, 5248, 10759, 3004, 979, 3730, 3239, 6358, 4340, 311, 912, 2750, 311, 264, 8781, 594, 1584, 4340, 311, 9245, 13406, 369, 3239, 6358, 4340, 311, 14411, 15592, 8781, 91025, 4340, 3484, 448, 1550, 55880, 487, 22049, 52603, 979, 3730, 3239, 6358, 10268, 11789, 27811, 4340, 311, 990, 279, 17439, 2859, 12020, 461, 2054, 4340, 311, 912, 12205, 311, 10759, 423, 3059, 34, 11829, 4340, 311, 990, 26679, 304, 3312, 21737, 4340, 311, 15498, 26679, 311, 264, 78679, 4340, 311, 57414, 26679, 220, 4692, 4340, 311, 6845, 2526, 4822, 4357, 4340, 311, 1494, 26679, 304, 518, 15592, 4340, 311, 6718, 553, 3668, 4340, 311, 6500, 6236, 1614, 14507, 4340, 311, 3705, 4379, 13388, 4340, 311, 2930, 894, 1614, 304, 825, 1555, 4340, 311, 3754, 3950, 10431, 304, 12853, 16969, 4340, 311, 912, 7375, 311, 6236, 61905, 4340, 311, 6718, 2038, 4340, 311, 653, 56370, 448, 65151, 25111, 4340, 311, 5508, 6452, 77, 4788, 311, 13852, 4340, 311, 1855, 2526, 4822, 24083, 4340, 311, 1855, 264, 2526, 6236, 1614, 536, 10268, 37068, 24602, 4340, 311, 1855, 264, 2526, 444, 10994, 536, 10268, 10392, 461, 2054, 4340, 311, 1855, 7375, 4340, 311, 7390, 697, 444, 10994, 10500, 4340, 311, 2795, 27445, 82, 4340, 311, 2795, 9293, 504, 264, 6220, 4340, 311, 2795, 9308, 4340, 311, 2795, 4718, 4340, 311, 2795, 73192, 4340, 311, 2795, 5100, 8246, 3542, 4340, 311, 2795, 11358, 82, 4340, 311, 2795, 3482, 6816, 4340, 311, 1855, 264, 8741, 320, 721, 12, 7596, 287, 8, 8781, 1178, 39088, 4119, 4340, 311, 15963, 3059, 504, 5248, 10759, 3004, 4340, 311, 3293, 10295, 504, 264, 22463, 41885, 10337, 4340, 311, 3293, 10295, 553, 3084, 4340, 311, 3293, 10295, 553, 53129, 31773, 40861, 320, 8035, 49, 8, 4340, 311, 3293, 10295, 553, 308, 12, 1520, 27248, 4340, 311, 3293, 10295, 553, 37623, 4340, 311, 990, 5785, 10295, 979, 3730, 32189, 4340, 311, 3705, 1293, 1467, 979, 3730, 32189, 4340, 311, 990, 49645, 7484, 320, 2152, 5392, 8098, 8, 311, 653, 32189, 4340, 311, 912, 32772, 82, 311, 264, 78679, 4340, 311, 4051, 6605, 30816, 16223, 7542, 4340, 311, 990, 279, 22463, 18837, 51980, 5333, 4340, 311, 24085, 1598, 77, 4788, 26223, 18837, 16378, 11434, 8436, 1862, 3674, 4340, 311, 6500, 444, 10994, 14507, 4340, 311, 3754, 3950, 10431, 369, 444, 10994, 82, 6727, 4119, 23490, 4340, 311, 633, 1487, 48216, 4340, 311, 83184, 30403, 3059, 311, 49360, 279, 330, 54337, 304, 279, 6149, 1, 2456, 4340, 311, 6718, 73192, 553, 21426, 4340, 311, 10880, 23921, 6605, 315, 279, 1852, 943, 4340, 311, 912, 1943, 3840, 4340, 311, 44566, 504, 19588, 22463, 18837, 13009, 311, 22463, 11212, 4340, 311, 17179, 1667, 5248, 22879, 817, 2197, 4340, 311, 1494, 79049, 57597, 821, 5961, 311, 4119, 4340, 311, 990, 79049, 57597, 50932, 4340, 311, 1855, 264, 2526, 9258, 21102, 4340, 311, 990, 279, 2550, 70913, 287, 6729, 4340, 311, 4715, 4718, 2550, 4340, 311, 22683, 979, 264, 22314, 1465, 13666, 4340, 311, 4715, 1467, 504, 1943, 6171, 4340, 311, 4715, 11874, 2550, 4340, 311, 4715, 53127, 2550, 4340, 311, 990, 279, 17022, 11789, 10392, 461, 2054, 4340, 311, 990, 22463, 18837, 448, 2155, 5355, 67, 8159, 10795, 4340, 311, 912, 6236, 3840, 4340, 311, 633, 264, 431, 1890, 3766, 311, 912, 51846, 4340, 311, 653, 817, 8694, 56370, 4340, 311, 633, 697, 431, 1890, 3766, 311, 470, 8173, 4340, 311, 4269, 3059, 504, 697, 431, 1890, 3766, 4340, 311, 6718, 4718, 821, 4340, 311, 52847, 6718, 1467, 553, 5766, 2582, 11160, 4340, 311, 1494, 15592, 23594, 311, 1598, 77, 4788, 4340, 311, 653, 330, 721, 65489, 287, 1, 56370, 4340, 311, 6718, 1467, 3118, 389, 41733, 37623, 4340, 311, 8781, 1598, 77, 4788, 4340, 311, 3581, 323, 2795, 22463, 18837, 6171, 4340, 311, 6718, 1467, 553, 11211, 4340, 311, 6718, 9308, 4340, 311, 653, 3405, 35764, 916, 27445, 82, 4340, 311, 3484, 448, 3460, 31806, 979, 3730, 7870, 3405, 12, 596, 86, 4671, 4340, 311, 2664, 9934, 979, 3730, 7870, 3405, 12, 596, 86, 4671, 4340, 311, 653, 3239, 10519, 438, 949, 315, 7870, 3405, 12, 596, 86, 4671, 4340, 311, 4269, 1598, 77, 4788, 4340, 311, 4269, 14507, 504, 458, 444, 10994, 4340, 311, 990, 264, 882, 12635, 291, 4621, 3553, 10759, 423, 4340, 311, 470, 35036, 504, 264, 5392, 4340, 311, 990, 6236, 4119, 311, 1618, 7375, 4340, 311, 11156, 15279, 5392, 8098, 4340, 311, 5344, 4119, 311, 1618, 264, 5392, 4340, 311, 2615, 279, 22109, 2648, 504, 264, 5392, 4340, 311, 1494, 5392, 16275, 311, 6236, 4119, 4340, 311, 1494, 1598, 882, 2750, 311, 7375, 4340, 311, 4269, 4357, 504, 264, 5392, 4340, 311, 4269, 5392, 6738, 4340, 311, 5508, 7375, 311, 5264, 15469, 23550, 4340, 311, 3705, 5392, 5975, 4340, 311, 990, 2421, 63530, 49645, 448, 5392, 8098, 4340, 311, 912, 264, 3738, 3419, 10603, 60666, 369, 7375, 4340, 311, 10719, 1614, 18906, 7375, 4340, 311, 11013, 6605, 4340, 311, 1855, 323, 3239, 4621, 10533, 44576, 928, 8474, 91804, 90951, 6525, 15473, 448, 8688, 8819, 44461, 15672, 3840, 15672, 4119, 7524, 68574, 25486, 6968, 4119, 82363, 13314, 56037, 71104, 63530, 49645, 44576, 928, 8474, 1592, 19083, 10533, 26223, 18837, 16378, 11434, 320, 43, 41664, 8, 15820, 40404, 318, 347, 2719, 5097, 87073, 54615, 19264, 12020, 7231, 831, 55988, 9471, 320, 49, 1890, 8, 12020, 7231, 831, 12020, 7231, 3004, 68836, 3749, 76509, 97457, 16275, 16451, 703, 3419, 11, 914, 9794, 9323, 1011, 1178, 12503, 28063, 29300, 7740, 8098, 16583, 1282, 4527, 3781, 10533, 10234, 22463, 18837, 30, 36, 23287, 123918, 250, 147615, 30543, 22463, 41885, 123918, 250, 147963, 30543, 22463, 11212, 69015, 85, 15, 13, 18, 85, 15, 13, 17, 13828, 67, 8159, 24748, 44, 5233, 1095, 504, 348, 15, 13, 15, 26179, 4340, 311, 44566, 504, 348, 15, 13, 15, 26179, 44, 5233, 1095, 504, 62185, 18837, 44, 5233, 1095, 504, 55396, 1663, 18837, 44, 5233, 1095, 504, 55396, 1663, 12020, 7231, 831, 18837, 44, 5233, 1095, 504, 444, 10994, 18837, 44, 5233, 1095, 504, 19504, 8035, 587, 18837, 44, 5233, 1095, 504, 444, 10994, 9523, 18837, 44, 5233, 1095, 504, 5027, 50325, 27143, 18837, 44, 5233, 1095, 504, 5027, 49, 261, 1180, 27143, 18837, 44, 5233, 1095, 504, 17439, 54615, 18837, 44, 5233, 1095, 504, 8550, 482, 27143, 18837, 44, 5233, 1095, 504, 19470, 831, 48, 1402, 5233, 1095, 504, 45486, 27143, 18837, 2324, 32259, 311, 22463, 11212, 4938, 4340, 311, 44566, 311, 22463, 11212, 4938, 4340, 311, 990, 5351, 15672, 2052, 13424, 448, 22463, 11212, 44, 5233, 1095, 1007, 50830, 4095, 10642, 476, 50830, 703, 4095, 10642, 44, 5233, 1095, 1007, 50830, 4095, 4267, 10642, 476, 55396, 17044, 4095, 10642, 44, 5233, 1095, 1007, 50830, 19237, 10642, 476, 50830, 19237, 4095, 10642, 32, 5724, 9285, 4195, 13850, 20713, 16077, 4842, 15352, 10974, 44576, 928, 8474, 26223, 18837, 16378, 11434, 320, 43, 41664, 8, 1925, 419, 2150, 26223, 18837, 16378, 11434, 320, 43, 41664, 340, 3533, 82301, 198, 68836, 20019, 271, 785, 22463, 18837, 16378, 11434, 320, 43, 41664, 8, 4990, 264, 9445, 1388, 5486, 311, 4752, 501, 6452, 77, 4788, 504, 6350, 6452, 77, 4788, 624, 1986, 3363, 429, 498, 7512, 1128, 1265, 3537, 11, 4751, 1091, 1246, 432, 1265, 3537, 11, 10693, 22463, 18837, 311, 29436, 279, 1598, 7246, 11320, 315, 279, 26179, 624, 1654, 3545, 8300, 311, 264, 22109, 3465, 1667, 444, 41664, 438, 264, 330, 8819, 3263, 1084, 594, 2989, 311, 6099, 429, 264, 330, 8819, 1, 374, 22109, 323, 432, 5169, 279, 2480, 22109, 20019, 624, 9974, 198, 785, 444, 41664, 75388, 3674, 4933, 4185, 12624, 429, 21058, 279, 22109, 3749, 323, 444, 41664, 23393, 624, 5501, 1490, 279, 2701, 1140, 315, 1246, 4686, 27193, 429, 3421, 4185, 9079, 448, 444, 41664, 624, 32, 1140, 315, 5798, 3419, 6452, 77, 4788, 646, 387, 1730, 304, 279, 22463, 18837, 9518, 5333, 17207, 13, 8999, 315, 1493, 6452, 77, 4788, 525, 5390, 979, 75878, 2526, 330, 58358, 1, 304, 22463, 18837, 1667, 444, 41664, 382, 84323, 315, 444, 41664, 15692, 198, 26223, 18837, 7553, 4756, 279, 1598, 7246, 11320, 315, 26179, 5798, 448, 444, 41664, 304, 264, 1372, 315, 5510, 1447, 21367, 45706, 15279, 11320, 25, 6452, 6452, 77, 4788, 304, 15279, 1667, 22109, 16547, 476, 1598, 5248, 11127, 1526, 264, 2661, 8781, 304, 15279, 1667, 279, 22109, 33904, 5333, 13, 49272, 11320, 646, 11941, 7949, 279, 39270, 438, 8692, 646, 387, 2814, 304, 15279, 4518, 315, 94559, 624, 16780, 277, 58250, 21433, 1824, 25, 5765, 8781, 5798, 448, 444, 41664, 646, 387, 1598, 67781, 1667, 279, 22109, 21433, 5333, 13, 1096, 646, 387, 5390, 979, 4303, 26179, 304, 264, 3538, 4573, 1380, 498, 1366, 311, 3705, 3460, 1372, 315, 7388, 78026, 624, 50, 70206, 16842, 25, 444, 41664, 26179, 646, 387, 73245, 11, 10693, 369, 52299, 2550, 438, 279, 8781, 374, 15695, 13, 22463, 18837, 646, 29436, 279, 16842, 315, 279, 2550, 311, 29337, 279, 882, 4686, 36943, 34841, 9730, 25333, 3080, 279, 1156, 11879, 315, 2550, 504, 264, 6236, 1614, 476, 9323, 76, 4041, 700, 3593, 11409, 7567, 2924, 1447, 1514, 309, 1717, 22463, 41885, 45415, 198, 2121, 697, 26179, 633, 803, 323, 803, 6351, 11, 432, 9044, 14756, 2989, 311, 3535, 1128, 6896, 374, 12482, 518, 1449, 3019, 624, 2354, 444, 41664, 11, 678, 7354, 525, 9463, 13726, 311, 22463, 41885, 369, 7192, 9282, 2897, 323, 7390, 70, 2897, 624, 19781, 5333, 25, 9211, 678, 26179, 525, 5798, 1667, 279, 22109, 3749, 11, 807, 646, 387, 1483, 304, 279, 1852, 1616, 438, 894, 1008, 22109, 624, 69464, 480, 448, 22463, 60421, 25, 81769, 5798, 448, 444, 41664, 646, 387, 26075, 1667, 369, 5670, 990, 382, 14996, 358, 990, 444, 41664, 30, 15692, 198, 43, 41664, 374, 458, 69884, 2165, 6291, 1177, 432, 6147, 22463, 18837, 311, 3705, 1598, 7246, 11320, 315, 26179, 304, 458, 33340, 1616, 624, 7983, 582, 614, 3884, 3847, 1598, 26179, 448, 11499, 315, 7354, 304, 5670, 11, 582, 8789, 6934, 1667, 444, 41664, 369, 34288, 69884, 2165, 9079, 13, 3197, 279, 3766, 7460, 6351, 1584, 6240, 11, 85467, 11, 24484, 476, 5248, 13009, 11, 582, 6934, 429, 3847, 1896, 9423, 315, 22463, 11212, 624, 641, 22463, 11212, 11, 3847, 6979, 38999, 429, 13837, 279, 3766, 594, 6396, 13, 1096, 6147, 3847, 311, 2506, 1667, 444, 41664, 2878, 3842, 7798, 979, 444, 41664, 374, 4362, 11, 1393, 3259, 432, 4135, 311, 6979, 6351, 69884, 2165, 12218, 429, 374, 803, 33798, 323, 10306, 480, 624, 8420, 525, 1045, 17501, 1447, 2679, 498, 525, 3259, 264, 3175, 444, 10994, 1618, 11, 498, 1513, 944, 1184, 444, 41664, 26, 4518, 1618, 279, 16533, 6236, 1614, 5961, 624, 2679, 498, 614, 264, 4285, 8781, 320, 68, 1302, 2572, 9934, 488, 9323, 76, 488, 6729, 11, 4285, 56370, 738, 705, 4992, 24389, 444, 41664, 374, 264, 13276, 4946, 11, 421, 498, 2299, 4633, 9423, 315, 279, 444, 41664, 7567, 624, 2679, 498, 2299, 4752, 264, 6351, 8781, 320, 68, 1302, 2572, 448, 85467, 11, 24484, 11, 5248, 13009, 11, 4992, 6138, 990, 22463, 11212, 4518, 13, 19881, 429, 498, 646, 2677, 990, 444, 41664, 2878, 3842, 7798, 304, 22463, 11212, 382, 75683, 2340, 35209, 15692, 198, 43, 41664, 26179, 525, 5798, 553, 75878, 6350, 6452, 77, 4788, 3786, 13, 576, 1378, 1887, 18037, 71194, 525, 22109, 14076, 323, 22109, 16547, 624, 8441, 1008, 18037, 71194, 320, 68, 1302, 2572, 22109, 28933, 8, 646, 387, 3381, 315, 438, 26244, 315, 1493, 1378, 71194, 624, 9974, 2610, 646, 1477, 264, 1140, 315, 678, 18037, 71194, 304, 279, 22463, 18837, 9518, 5333, 17207, 624, 68836, 14076, 15692, 198, 68836, 14076, 374, 264, 18037, 27594, 429, 6147, 498, 330, 8819, 1, 5248, 1598, 77, 4788, 94559, 11, 448, 279, 2550, 315, 825, 78679, 13480, 438, 279, 1946, 311, 279, 1790, 624, 1499, 8688, 8819, 15467, 7634, 77, 4788, 1159, 22109, 14076, 8819, 284, 22109, 14076, 2561, 6108, 16251, 16, 11, 78679, 17, 2467, 7082, 17207, 25, 68836, 14076, 198, 15174, 10746, 279, 8781, 448, 1045, 1946, 510, 11822, 7645, 284, 8781, 27110, 1141, 635, 5898, 340, 6005, 6818, 82, 311, 279, 2701, 510, 3006, 16, 284, 78679, 16, 27110, 1141, 635, 5898, 8, 11822, 7645, 284, 78679, 17, 27110, 11057, 16, 340, 1921, 261, 82211, 16, 323, 78679, 17, 525, 78428, 369, 894, 22109, 429, 498, 1366, 311, 8781, 3786, 624, 68836, 16547, 15692, 198, 68836, 16547, 374, 264, 18037, 27594, 429, 6147, 498, 311, 1598, 5248, 1598, 77, 4788, 78026, 11, 448, 279, 1852, 1946, 3897, 311, 1817, 624, 1499, 8688, 8819, 15467, 7634, 77, 4788, 1159, 22109, 16547, 8819, 284, 22109, 16547, 2306, 262, 330, 792, 16, 788, 78679, 16, 11, 262, 330, 792, 17, 788, 78679, 17, 11, 5410, 7082, 17207, 25, 68836, 16547, 198, 15174, 10746, 279, 8781, 448, 1045, 1946, 510, 11822, 7645, 284, 8781, 27110, 1141, 635, 5898, 340, 9945, 7540, 264, 1590, 7645, 10997, 448, 279, 1852, 6894, 438, 279, 1946, 10997, 11, 714, 448, 279, 2750, 12575, 553, 279, 2550, 315, 279, 12159, 78679, 624, 90, 262, 330, 792, 16, 788, 78679, 16, 27110, 1141, 635, 5898, 701, 262, 330, 792, 17, 788, 78679, 17, 27110, 1141, 635, 5898, 701, 532, 3820, 541, 11, 429, 279, 1598, 77, 4788, 525, 15695, 304, 15279, 11, 773, 1393, 279, 1102, 374, 279, 1852, 438, 198, 35671, 61094, 6839, 3403, 11, 279, 11320, 882, 374, 1753, 10596, 624, 9974, 68836, 4272, 5054, 4730, 2800, 82, 2176, 65949, 323, 39007, 11320, 320, 300, 678, 6452, 77, 4788, 653, 4292, 2461, 65949, 11320, 11, 22109, 16547, 5711, 264, 72716, 25255, 311, 1598, 279, 1598, 77, 4788, 78026, 624, 2461, 39007, 11320, 11, 22109, 16547, 5711, 44918, 1302, 1856, 311, 1598, 279, 1598, 77, 4788, 78026, 382, 75683, 32117, 15692, 198, 785, 10431, 315, 22109, 14076, 323, 22109, 16547, 374, 773, 4185, 429, 582, 3465, 264, 78339, 19482, 369, 1667, 1105, 13, 1096, 8609, 198, 983, 1281, 279, 2038, 803, 33798, 323, 63594, 624, 785, 760, 5675, 15692, 198, 1654, 614, 74591, 279, 760, 5675, 311, 1855, 264, 22109, 14076, 504, 1378, 6452, 77, 4788, 624, 8819, 284, 78679, 16, 760, 78679, 17, 198, 285, 91228, 311, 510, 8819, 284, 22109, 14076, 2561, 6108, 16251, 16, 11, 78679, 17, 2546, 785, 659, 13768, 1714, 15692, 198, 2679, 498, 614, 15659, 5841, 1011, 448, 5675, 916, 10628, 11, 498, 646, 990, 279, 659, 13768, 1714, 4518, 13, 1096, 374, 13578, 311, 279, 760, 5675, 624, 8819, 284, 78679, 16, 35924, 2601, 82211, 17, 340, 7339, 2962, 290, 15692, 198, 43, 41664, 16790, 16962, 943, 77142, 311, 1281, 432, 8661, 311, 30335, 26179, 624, 2679, 498, 653, 537, 3535, 279, 943, 77142, 11, 498, 646, 2677, 990, 279, 22109, 14076, 323, 22109, 16547, 6846, 5961, 624, 1986, 686, 1281, 279, 2038, 803, 13694, 11, 714, 432, 686, 1083, 1281, 432, 803, 11464, 624, 8517, 311, 22109, 16547, 15692, 198, 24480, 458, 444, 41664, 7493, 11, 264, 10997, 374, 9463, 16099, 311, 264, 22109, 16547, 624, 2461, 3110, 11, 279, 2701, 2038, 510, 40792, 284, 314, 262, 330, 792, 16, 788, 78679, 16, 11, 262, 330, 792, 17, 788, 78679, 17, 11, 92, 8819, 284, 12731, 760, 78679, 18, 198, 2132, 5221, 9463, 16099, 311, 279, 2701, 510, 8819, 284, 22109, 14076, 2561, 68836, 16547, 81057, 701, 78679, 18, 2546, 924, 1488, 2610, 614, 311, 387, 16585, 1576, 279, 12731, 10997, 374, 537, 264, 22109, 16547, 1633, 11, 432, 374, 1101, 264, 10997, 13, 1096, 3363, 429, 279, 2701, 2038, 686, 4828, 458, 53631, 30286, 3629, 27110, 1141, 635, 5898, 340, 5152, 311, 22109, 58266, 15692, 198, 24480, 458, 444, 41664, 7493, 11, 264, 729, 374, 9463, 16099, 311, 264, 22109, 58266, 624, 750, 1045, 9596, 2075, 1648, 262, 470, 856, 8819, 284, 1045, 9596, 760, 78679, 16, 198, 2132, 5221, 9463, 16099, 311, 279, 2701, 510, 8819, 284, 22109, 14076, 2561, 68836, 58266, 1141, 635, 9596, 701, 78679, 16, 2546, 924, 1488, 2610, 614, 311, 387, 16585, 1576, 279, 12459, 729, 374, 537, 264, 22109, 58266, 1633, 11, 432, 374, 1101, 264, 729, 13, 1096, 3363, 429, 279, 2701, 2038, 686, 4828, 458, 53631, 25, 12935, 856, 25, 856, 488, 220, 16, 27110, 1141, 635, 5898, 340, 77415, 26179, 15692, 198, 43, 41664, 21538, 311, 3410, 28137, 2163, 7709, 323, 48041, 916, 19588, 37190, 291, 26179, 1741, 438, 444, 10994, 18837, 323, 198, 1109, 3004, 1663, 12020, 7231, 831, 18837, 13, 8999, 315, 1493, 19588, 26179, 10265, 2989, 3565, 1075, 50932, 11, 323, 438, 264, 21864, 8045, 198, 1055, 30428, 4119, 32944, 11, 48041, 702, 3635, 803, 323, 803, 2989, 624, 2679, 498, 525, 5023, 1667, 825, 315, 1493, 19588, 26179, 11, 4486, 1490, 419, 8474, 369, 18821, 389, 1246, 311, 44566, 624, 2461, 27193, 389, 1246, 311, 653, 3151, 9079, 448, 444, 41664, 11, 1779, 700, 279, 9760, 1246, 4686, 27193, 35823, 419, 2150, 26034, 419, 2150, 10950, 30, 21291, 1592, 19083, 10533, 5847, 15820, 84323, 315, 444, 41664, 14996, 358, 990, 444, 41664, 30, 75683, 2340, 35209, 68836, 14076, 68836, 16547, 75683, 32117, 785, 760, 5675, 785, 659, 13768, 1714, 7339, 2962, 290, 77415, 26179, 33768, 24862, 75615, 41574, 30280, 12545, 14, 9951, 7661, 96452, 26700, 53746, 13547, 7240, 220, 17, 15, 17, 20, 22463, 18837, 11, 4848, 382]\n"
     ]
    }
   ],
   "source": [
    "# Doc texts concat\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "print(f\"Num tokens in all context: {num_tokens_from_string(concatenated_content)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc texts split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok = 2000\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size_tok, chunk_overlap=0\n",
    ")\n",
    "texts_split = text_splitter.split_text(concatenated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    "\n",
    "### --- Code from \n",
    "# https://github.com/parthsarthi03/raptor/blob/master/raptor/cluster_tree_builder.py\n",
    "# https://github.com/run-llama/llama_index/blob/main/llama-index-packs/llama-index-packs-raptor/llama_index/packs/raptor/clustering.py \n",
    "# (added comments and docstrings) --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
    "                   If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - num_neighbors: The number of neighbors to consider for each point.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - max_clusters: The maximum number of clusters to consider.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - An integer representing the optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the number of clusters determined.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
    "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for UMAP reduction.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
    "\n",
    "    Returns:\n",
    "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "\n",
    "### --- Our code below --- ###\n",
    "\n",
    "\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "\n",
    "    This function assumes the existence of an `embedding_model` object with a method `embed_documents`\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = embedding_model.embed_documents(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "\n",
    "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
    "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n",
    "    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes\n",
    "    the content within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that could define the depth or detail of processing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two DataFrames:\n",
    "      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n",
    "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n",
    "         and the cluster identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"Here is a sub-set of LangChain Expression Langauge doc.\n",
    "\n",
    "    LangChain Expression Langauge provides a way to compose chain in LangChain.\n",
    "\n",
    "    Give a detailed summary of the documentation provided.\n",
    "\n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideLangChain Expression Language (LCEL)On this pageLangChain Expression Language (LCEL)\\nPrerequisites\\nRunnable Interface\\n\\nThe LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables.\\nThis means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains.\\nWe often refer to a Runnable created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is Runnable and it implements the full Runnable Interface.\\nnote\\nThe LCEL cheatsheet shows common patterns that involve the Runnable interface and LCEL expressions.\\nPlease see the following list of how-to guides that cover common tasks with LCEL.\\nA list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n\\nBenefits of LCEL\\u200b\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\nOptimized parallel execution: Run Runnables in parallel using RunnableParallel or run multiple inputs through a given chain in parallel using the Runnable Batch API. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\nGuaranteed Async support: Any chain built with LCEL can be run asynchronously using the Runnable Async API. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\nSimplify streaming: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out).\\n\\nOther benefits include:\\n\\nSeamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\\nStandard API: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\nDeployable with LangServe: Chains built with LCEL can be deployed using for production use.\\n\\nShould I use LCEL?\\u200b\\nLCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way.\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of LangGraph.\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\nHere are some guidelines:\\n\\nIf you are making a single LLM call, you don\\'t need LCEL; instead call the underlying chat model directly.\\nIf you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\nIf you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\nComposition Primitives\\u200b\\nLCEL chains are built by composing existing Runnables together. The two main composition primitives are RunnableSequence and RunnableParallel.\\nMany other composition primitives (e.g., RunnableAssign) can be thought of as variations of these two primitives.\\nnoteYou can find a list of all composition primitives in the LangChain Core API Reference.\\nRunnableSequence\\u200b\\nRunnableSequence is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\nfrom langchain_core.runnables import RunnableSequencechain = RunnableSequence([runnable1, runnable2])API Reference:RunnableSequence\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\ncorresponds to the following:\\noutput1 = runnable1.invoke(some_input)final_output = runnable2.invoke(output1)\\nnoterunnable1 and runnable2 are placeholders for any Runnable that you want to chain together.\\nRunnableParallel\\u200b\\nRunnableParallel is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\nfrom langchain_core.runnables import RunnableParallelchain = RunnableParallel({    \"key1\": runnable1,    \"key2\": runnable2,})API Reference:RunnableParallel\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\nWill yield a final_output dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n{    \"key1\": runnable1.invoke(some_input),    \"key2\": runnable2.invoke(some_input),}\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\nnoteRunnableParallelsupports both synchronous and asynchronous execution (as all Runnables do).\\nFor synchronous execution, RunnableParallel uses a ThreadPoolExecutor to run the runnables concurrently.\\nFor asynchronous execution, RunnableParallel uses asyncio.gather to run the runnables concurrently.\\n\\nComposition Syntax\\u200b\\nThe usage of RunnableSequence and RunnableParallel is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\nThe | operator\\u200b\\nWe have overloaded the | operator to create a RunnableSequence from two Runnables.\\nchain = runnable1 | runnable2\\nis Equivalent to:\\nchain = RunnableSequence([runnable1, runnable2])\\nThe .pipe method\\u200b\\nIf you have moral qualms with operator overloading, you can use the .pipe method instead. This is equivalent to the | operator.\\nchain = runnable1.pipe(runnable2)\\nCoercion\\u200b\\nLCEL applies automatic type coercion to make it easier to compose chains.\\nIf you do not understand the type coercion, you can always use the RunnableSequence and RunnableParallel classes directly.\\nThis will make the code more verbose, but it will also make it more explicit.\\nDictionary to RunnableParallel\\u200b\\nInside an LCEL expression, a dictionary is automatically converted to a RunnableParallel.\\nFor example, the following code:\\nmapping = {    \"key1\": runnable1,    \"key2\": runnable2,}chain = mapping | runnable3\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\ncautionYou have to be careful because the mapping dictionary is not a RunnableParallel object, it is just a dictionary. This means that the following code will raise an AttributeError:mapping.invoke(some_input)\\nFunction to RunnableLambda\\u200b\\nInside an LCEL expression, a function is automatically converted to a RunnableLambda.\\ndef some_func(x):    return xchain = some_func | runnable1\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\ncautionYou have to be careful because the lambda function is not a RunnableLambda object, it is just a function. This means that the following code will raise an AttributeError:lambda x: x + 1.invoke(some_input)\\nLegacy chains\\u200b\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\\nConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\nIf you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.\\nFor guides on how to do specific tasks with LCEL, check out the relevant how-to guides.Edit this pageWas this page helpful?PreviousKey-value storesNextMessagesBenefits of LCELShould I use LCEL?Composition PrimitivesRunnableSequenceRunnableParallelComposition SyntaxThe | operatorThe .pipe methodCoercionLegacy chainsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n',\n",
       " '\\n\\n\\n\\n\\nHow to use output parsers to parse an LLM response into structured format | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to use output parsers to parse an LLM response into structured formatOn this pageHow to use output parsers to parse an LLM response into structured format\\nLanguage models output text. But there are times where you want to get more structured information than just text back. While some model providers support built-in ways to return structured output, not all do.\\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\\n\\n\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\\n\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\\n\\nAnd then one optional one:\\n\\n\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\\n\\nGet started\\u200b\\nBelow we go over the main type of output parser, the PydanticOutputParser.\\nfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import OpenAIfrom pydantic import BaseModel, Field, model_validatormodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @model_validator(mode=\"before\")    @classmethod    def question_ends_with_question_mark(cls, values: dict) -> dict:        setup = values.get(\"setup\")        if setup and setup[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return values# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)API Reference:PydanticOutputParser | PromptTemplate | OpenAI\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nLCEL\\u200b\\nOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\\nOutput parsers accept a string or BaseMessage as input and can return an arbitrary type.\\nparser.invoke(output)\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nInstead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:\\nchain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nWhile all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.\\nThe SimpleJsonOutputParser for example can stream through partial outputs:\\nfrom langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parserAPI Reference:SimpleJsonOutputParser\\nlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))\\n[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]\\nSimilarly,for PydanticOutputParser:\\nlist(chain.stream({\"query\": \"Tell me a joke.\"}))\\n[Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')]Edit this pageWas this page helpful?PreviousHow to run custom functionsNextHow to handle cases where no queries are generatedGet startedLCELCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n',\n",
       " '\\n\\n\\n\\n\\nHow to do \"self-querying\" retrieval | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to do \"self-querying\" retrievalOn this pageHow to do \"self-querying\" retrieval\\ninfoHead to Integrations for documentation on vector stores with built-in support for self-querying.\\nA self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying vector store. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\\n\\nGet started\\u200b\\nFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.\\nNote: The self-query retriever requires you to have lark package installed.\\n%pip install --upgrade --quiet  lark langchain-chroma\\nfrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())API Reference:Document | OpenAIEmbeddings\\nCreating our self-querying retriever\\u200b\\nNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\\nfrom langchain.chains.query_constructor.schema import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)API Reference:AttributeInfo | SelfQueryRetriever | ChatOpenAI\\nTesting it out\\u200b\\nAnd now we can actually try using our retriever!\\n# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")\\n[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]\\n# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\\n[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]\\n# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")\\n[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]\\n# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")\\n[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]\\nFilter k\\u200b\\nWe can also use the self query retriever to specify k: the number of documents to fetch.\\nWe can do this by passing enable_limit=True to the constructor.\\nretriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")\\n[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]\\nConstructing from scratch with LCEL\\u200b\\nTo see what\\'s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.\\nFirst, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.\\nfrom langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserAPI Reference:StructuredQueryOutputParser | get_query_constructor_prompt\\nLet\\'s look at our prompt:\\nprint(prompt.format(query=\"dummy question\"))\\nYour goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:\\\\`\\\\`\\\\`json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}\\\\`\\\\`\\\\`The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: `comp(attr, val)`:- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator- `attr` (string):  name of attribute to apply the comparison to- `val` (string): is the comparison valueA logical operation statement takes the form `op(statement1, statement2, ...)`:- `op` (and | or | not): logical operator- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.Make sure that filters only refer to attributes that exist in the data source.Make sure that filters only use the attributed names with its function names if there are functions applied on them.Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.Make sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}\\\\`\\\\`\\\\`User Query:What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:\\\\`\\\\`\\\\`json{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}\\\\`\\\\`\\\\`<< Example 2. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}\\\\`\\\\`\\\\`User Query:What are songs that were not published on SpotifyStructured Request:\\\\`\\\\`\\\\`json{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}\\\\`\\\\`\\\\`<< Example 3. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}\\\\`\\\\`\\\\`User Query:dummy questionStructured Request:\\nAnd what our full chain produces:\\nquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })\\nStructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)\\nThe query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.\\nThe next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.\\nfrom langchain_community.query_constructors.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)API Reference:ChromaTranslator\\nretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")\\n[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Edit this pageWas this page helpful?PreviousHow to pass runtime secrets to runnablesNextHow to split text based on semantic similarityGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1 clusters--\n"
     ]
    }
   ],
   "source": [
    "# Build tree\n",
    "leaf_texts = docs_texts\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"raptor\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"raptor\",\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "# Initialize all_texts with leaf_texts\n",
    "all_texts = leaf_texts.copy()\n",
    "\n",
    "# Iterate through the results to extract summaries from each level and add them to all_texts\n",
    "for level in sorted(results.keys()):\n",
    "    # Extract summaries from the current level's DataFrame\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    # Extend all_texts with the summaries from the current level\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "vector_store.add_texts(texts=all_texts)\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['QdrantVectorStore', 'OllamaEmbeddings'], vectorstore=<langchain_qdrant.qdrant.QdrantVectorStore object at 0x31dd25fd0>, search_kwargs={})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/easonyin/miniconda3/lib/python3.12/site-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'_id': '2e180894119b4f3c8968358617155dff', '_collection_name': 'raptor'}, page_content='This guide provides a comprehensive overview of creating and using a self-querying retriever in LangChain. Let\\'s break down the key steps and concepts involved:\\n\\n### Key Concepts\\n\\n1. **Query Construction**:\\n   - The query is transformed into a structured form that includes both text content to match against document contents and metadata filters.\\n   - This involves generating a `StructuredQuery` object which contains the textual search term (`query`) and the logical conditions on metadata fields (`filter`).\\n\\n2. **Prompt and Output Parser**:\\n   - A prompt is provided for constructing the structured query, which includes examples of how to format the query.\\n   - An output parser translates the structured query into a form that can be understood by the retrieval system.\\n\\n3. **StructuredQueryTranslator**:\\n   - This component converts the structured query into a filter syntax compatible with the underlying vector store (in this case, Chroma).\\n\\n4. **Self-QueryRetriever**:\\n   - Combines the query constructor and translator to provide an end-to-end solution for constructing and executing self-queries on document collections.\\n\\n### Step-by-Step Implementation\\n\\n1. **Define Metadata Fields and Document Content Description**:\\n\\n```python\\nfrom langchain.chains.query_constructor.base import get_query_constructor_prompt, StructuredQueryOutputParser\\n\\n# Define metadata fields and document content description\\nmetadata_field_info = [\\n    {\"name\": \"genre\", \"type\": \"string\", \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\"},\\n    {\"name\": \"year\", \"type\": \"integer\", \"description\": \"The year the movie was released\"},\\n    {\"name\": \"director\", \"type\": \"string\", \"description\": \"The name of the movie director\"},\\n    {\"name\": \"rating\", \"type\": \"float\", \"description\": \"A 1-10 rating for the movie\"}\\n]\\n\\ndocument_content_description = \"Brief summary of a movie\"\\n```\\n\\n2. **Generate Query Constructor Prompt and Output Parser**:\\n\\n```python\\nprompt = get_query_constructor_prompt(document_content_description, metadata_field_info)\\noutput_parser = StructuredQueryOutputParser.from_components()\\nquery_constructor = prompt | llm | output_parser\\n```\\n\\n3. **Define the Self-QueryRetriever**:\\n\\n```python\\nfrom langchain_community.query_constructors.chroma import ChromaTranslator\\n\\n# Create a self-query retriever\\nretriever = SelfQueryRetriever(\\n    query_constructor=query_constructor,\\n    vectorstore=vectorstore,  # Assume `vectorstore` is already defined and initialized.\\n    structured_query_translator=ChromaTranslator()\\n)\\n```\\n\\n4. **Invoke the Query**:\\n\\n```python\\n# Example queries\\nresponse = retriever.invoke(\"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\")\\nprint(response)\\n\\nresponse = retriever.invoke(\"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")\\nprint(response)\\n```\\n\\n### Advanced Customization\\n\\n- **Adjusting the Prompt**:\\n  - You can customize the prompt by adding or modifying examples to better fit your specific use case.\\n  \\n- **Enabling Limitation**:\\n  - By setting `enable_limit=True` when creating the retriever, you can limit the number of results returned.\\n\\n### Example Output\\n\\n```python\\n# Example output for a query like: \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"\\nStructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)\\n\\n# Example output for a query like: \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\"\\n[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]\\n```\\n\\nThis setup allows you to construct complex queries with both text content and metadata filters, ensuring that the retrieval system can return relevant documents based on user input.'), Document(metadata={'_id': 'f3bd87d374e34000a6fec12649755f0c', '_collection_name': 'raptor'}, page_content='\\n\\n\\n\\n\\nHow to use output parsers to parse an LLM response into structured format | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to use output parsers to parse an LLM response into structured formatOn this pageHow to use output parsers to parse an LLM response into structured format\\nLanguage models output text. But there are times where you want to get more structured information than just text back. While some model providers support built-in ways to return structured output, not all do.\\nOutput parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\\n\\n\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\\n\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.\\n\\nAnd then one optional one:\\n\\n\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.\\n\\nGet started\\u200b\\nBelow we go over the main type of output parser, the PydanticOutputParser.\\nfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_openai import OpenAIfrom pydantic import BaseModel, Field, model_validatormodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @model_validator(mode=\"before\")    @classmethod    def question_ends_with_question_mark(cls, values: dict) -> dict:        setup = values.get(\"setup\")        if setup and setup[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return values# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)API Reference:PydanticOutputParser | PromptTemplate | OpenAI\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nLCEL\\u200b\\nOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.\\nOutput parsers accept a string or BaseMessage as input and can return an arbitrary type.\\nparser.invoke(output)\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nInstead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:\\nchain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})\\nJoke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')\\nWhile all parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.\\nThe SimpleJsonOutputParser for example can stream through partial outputs:\\nfrom langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parserAPI Reference:SimpleJsonOutputParser\\nlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))\\n[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]\\nSimilarly,for PydanticOutputParser:\\nlist(chain.stream({\"query\": \"Tell me a joke.\"}))\\n[Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing\\'), Joke(setup=\\'Why did the tomato turn red?\\', punchline=\\'Because it saw the salad dressing!\\')]Edit this pageWas this page helpful?PreviousHow to run custom functionsNextHow to handle cases where no queries are generatedGet startedLCELCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'), Document(metadata={'_id': 'c2e57ff8d51745c7b450a58197685bdf', '_collection_name': 'raptor'}, page_content='\\n\\n\\n\\n\\nHow to do \"self-querying\" retrieval | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyHow-to guidesHow to do \"self-querying\" retrievalOn this pageHow to do \"self-querying\" retrieval\\ninfoHead to Integrations for documentation on vector stores with built-in support for self-querying.\\nA self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying vector store. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.\\n\\nGet started\\u200b\\nFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.\\nNote: The self-query retriever requires you to have lark package installed.\\n%pip install --upgrade --quiet  lark langchain-chroma\\nfrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())API Reference:Document | OpenAIEmbeddings\\nCreating our self-querying retriever\\u200b\\nNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.\\nfrom langchain.chains.query_constructor.schema import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)API Reference:AttributeInfo | SelfQueryRetriever | ChatOpenAI\\nTesting it out\\u200b\\nAnd now we can actually try using our retriever!\\n# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")\\n[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]\\n# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")\\n[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]\\n# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")\\n[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]\\n# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")\\n[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]\\nFilter k\\u200b\\nWe can also use the self query retriever to specify k: the number of documents to fetch.\\nWe can do this by passing enable_limit=True to the constructor.\\nretriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")\\n[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]\\nConstructing from scratch with LCEL\\u200b\\nTo see what\\'s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.\\nFirst, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.\\nfrom langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserAPI Reference:StructuredQueryOutputParser | get_query_constructor_prompt\\nLet\\'s look at our prompt:\\nprint(prompt.format(query=\"dummy question\"))\\nYour goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:\\\\`\\\\`\\\\`json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}\\\\`\\\\`\\\\`The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: `comp(attr, val)`:- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator- `attr` (string):  name of attribute to apply the comparison to- `val` (string): is the comparison valueA logical operation statement takes the form `op(statement1, statement2, ...)`:- `op` (and | or | not): logical operator- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.Make sure that filters only refer to attributes that exist in the data source.Make sure that filters only use the attributed names with its function names if there are functions applied on them.Make sure that filters only use format `YYYY-MM-DD` when handling date data typed values.Make sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.Make sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}\\\\`\\\\`\\\\`User Query:What are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:\\\\`\\\\`\\\\`json{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}\\\\`\\\\`\\\\`<< Example 2. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}\\\\`\\\\`\\\\`User Query:What are songs that were not published on SpotifyStructured Request:\\\\`\\\\`\\\\`json{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}\\\\`\\\\`\\\\`<< Example 3. >>Data Source:\\\\`\\\\`\\\\`json{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}\\\\`\\\\`\\\\`User Query:dummy questionStructured Request:\\nAnd what our full chain produces:\\nquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })\\nStructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)\\nThe query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.\\nThe next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.\\nfrom langchain_community.query_constructors.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)API Reference:ChromaTranslator\\nretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")\\n[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Edit this pageWas this page helpful?PreviousHow to pass runtime secrets to runnablesNextHow to split text based on semantic similarityGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'), Document(metadata={'_id': 'c7a38fb947034fb385dc4d2d50e799f6', '_collection_name': 'raptor'}, page_content='\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideLangChain Expression Language (LCEL)On this pageLangChain Expression Language (LCEL)\\nPrerequisites\\nRunnable Interface\\n\\nThe LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables.\\nThis means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains.\\nWe often refer to a Runnable created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is Runnable and it implements the full Runnable Interface.\\nnote\\nThe LCEL cheatsheet shows common patterns that involve the Runnable interface and LCEL expressions.\\nPlease see the following list of how-to guides that cover common tasks with LCEL.\\nA list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n\\nBenefits of LCEL\\u200b\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\nOptimized parallel execution: Run Runnables in parallel using RunnableParallel or run multiple inputs through a given chain in parallel using the Runnable Batch API. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\nGuaranteed Async support: Any chain built with LCEL can be run asynchronously using the Runnable Async API. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\nSimplify streaming: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out).\\n\\nOther benefits include:\\n\\nSeamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\\nStandard API: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\nDeployable with LangServe: Chains built with LCEL can be deployed using for production use.\\n\\nShould I use LCEL?\\u200b\\nLCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way.\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of LangGraph.\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\nHere are some guidelines:\\n\\nIf you are making a single LLM call, you don\\'t need LCEL; instead call the underlying chat model directly.\\nIf you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\nIf you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\nComposition Primitives\\u200b\\nLCEL chains are built by composing existing Runnables together. The two main composition primitives are RunnableSequence and RunnableParallel.\\nMany other composition primitives (e.g., RunnableAssign) can be thought of as variations of these two primitives.\\nnoteYou can find a list of all composition primitives in the LangChain Core API Reference.\\nRunnableSequence\\u200b\\nRunnableSequence is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\nfrom langchain_core.runnables import RunnableSequencechain = RunnableSequence([runnable1, runnable2])API Reference:RunnableSequence\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\ncorresponds to the following:\\noutput1 = runnable1.invoke(some_input)final_output = runnable2.invoke(output1)\\nnoterunnable1 and runnable2 are placeholders for any Runnable that you want to chain together.\\nRunnableParallel\\u200b\\nRunnableParallel is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\nfrom langchain_core.runnables import RunnableParallelchain = RunnableParallel({    \"key1\": runnable1,    \"key2\": runnable2,})API Reference:RunnableParallel\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\nWill yield a final_output dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n{    \"key1\": runnable1.invoke(some_input),    \"key2\": runnable2.invoke(some_input),}\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\nnoteRunnableParallelsupports both synchronous and asynchronous execution (as all Runnables do).\\nFor synchronous execution, RunnableParallel uses a ThreadPoolExecutor to run the runnables concurrently.\\nFor asynchronous execution, RunnableParallel uses asyncio.gather to run the runnables concurrently.\\n\\nComposition Syntax\\u200b\\nThe usage of RunnableSequence and RunnableParallel is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\nThe | operator\\u200b\\nWe have overloaded the | operator to create a RunnableSequence from two Runnables.\\nchain = runnable1 | runnable2\\nis Equivalent to:\\nchain = RunnableSequence([runnable1, runnable2])\\nThe .pipe method\\u200b\\nIf you have moral qualms with operator overloading, you can use the .pipe method instead. This is equivalent to the | operator.\\nchain = runnable1.pipe(runnable2)\\nCoercion\\u200b\\nLCEL applies automatic type coercion to make it easier to compose chains.\\nIf you do not understand the type coercion, you can always use the RunnableSequence and RunnableParallel classes directly.\\nThis will make the code more verbose, but it will also make it more explicit.\\nDictionary to RunnableParallel\\u200b\\nInside an LCEL expression, a dictionary is automatically converted to a RunnableParallel.\\nFor example, the following code:\\nmapping = {    \"key1\": runnable1,    \"key2\": runnable2,}chain = mapping | runnable3\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\ncautionYou have to be careful because the mapping dictionary is not a RunnableParallel object, it is just a dictionary. This means that the following code will raise an AttributeError:mapping.invoke(some_input)\\nFunction to RunnableLambda\\u200b\\nInside an LCEL expression, a function is automatically converted to a RunnableLambda.\\ndef some_func(x):    return xchain = some_func | runnable1\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\ncautionYou have to be careful because the lambda function is not a RunnableLambda object, it is just a function. This means that the following code will raise an AttributeError:lambda x: x + 1.invoke(some_input)\\nLegacy chains\\u200b\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\\nConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\nIf you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.\\nFor guides on how to do specific tasks with LCEL, check out the relevant how-to guides.Edit this pageWas this page helpful?PreviousKey-value storesNextMessagesBenefits of LCELShould I use LCEL?Composition PrimitivesRunnableSequenceRunnableParallelComposition SyntaxThe | operatorThe .pipe methodCoercionLegacy chainsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the information provided in the prompt and the context of your query, it seems you are looking to understand how to use LCEL (LangChain Expression Language) for building chains or runnables. Here\\'s a summary of key points and examples that can help you get started:\\n\\n### Benefits of LCEL\\n\\n1. **Optimized Parallel Execution**: Run multiple Runnables in parallel using `RunnableParallel`.\\n2. **Guaranteed Async Support**: Asynchronous execution with the `RunnableAsync` API.\\n3. **Simplified Streaming**: Incremental output as chains are executed.\\n\\n### Composition Primitives\\n\\n1. **RunnableSequence**:\\n   - **Usage**: Sequentially chain multiple runnables.\\n   ```python\\n   from langchain_core.runnables import RunnableSequence\\n\\n   chain = RunnableSequence([runnable1, runnable2])\\n   final_output = chain.invoke(some_input)\\n   ```\\n\\n2. **RunnableParallel**:\\n   - **Usage**: Run multiple runnables concurrently with the same input.\\n   ```python\\n   from langchain_core.runnables import RunnableParallel\\n\\n   chain = RunnableParallel({\\n       \"key1\": runnable1,\\n       \"key2\": runnable2\\n   })\\n   final_output = chain.invoke(some_input)\\n   ```\\n\\n### Shorthand Syntax\\n\\n- **| Operator**: Creates a `RunnableSequence`.\\n  ```python\\n  chain = runnable1 | runnable2\\n  ```\\n  \\n- **.pipe Method**: Equivalent to the `|` operator.\\n  ```python\\n  chain = runnable1.pipe(runnable2)\\n  ```\\n\\n### Coercion and Dictionary Conversion\\n\\n- **Dictionary to RunnableParallel**:\\n  ```python\\n  mapping = {\\n      \"key1\": runnable1,\\n      \"key2\": runnable2\\n  }\\n  chain = mapping | runnable3\\n  # Equivalent to:\\n  chain = RunnableSequence([RunnableParallel(mapping), runnable3])\\n  ```\\n\\n- **Function to RunnableLambda**:\\n  ```python\\n  def some_func(x):\\n      return x\\n\\n  chain = some_func | runnable1\\n  # Equivalent to:\\n  chain = RunnableSequence([RunnableLambda(some_func), runnable1])\\n  ```\\n\\n### Legacy Chains and Migrations\\n\\nFor users currently using legacy chains like `LLMChain` or `ConversationalRetrievalChain`, LCEL offers a more consistent behavior and customization. The guide suggests migrating these to LCEL for better type coercion, explicitness, and ease of use.\\n\\n### Example Use Case\\n\\nSuppose you want to create a chain that processes some input through three steps: transformation, retrieval, and parsing:\\n\\n```python\\nfrom langchain_core.runnables import RunnableSequence\\n\\n# Define individual runnables\\ntransformation = RunnableLambda(lambda x: f\"Transformed {x}\")\\nretrieval = RunnableParallel({\"key1\": lambda x: f\"Retrieved 1 for {x}\", \"key2\": lambda x: f\"Retrieved 2 for {x}\"})\\nparsing = RunnableSequence([RunnableLambda(lambda x: f\"Parsed {x}\")])\\n\\n# Create the chain\\nchain = transformation | retrieval | parsing\\n\\ninput_data = \"example input\"\\n\\noutput = chain.invoke(input_data)\\nprint(output)  # Output will be a dictionary with transformed and parsed data from each step.\\n```\\n\\nThis example demonstrates how to use LCEL to build complex chains from simpler runnables, ensuring optimized execution and maintainability.\\n\\nIf you need more specific help or examples related to your exact scenario, feel free to provide additional details!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    print(docs)\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"HWhat is LangChain Expression Language? Give me a specific code example.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
